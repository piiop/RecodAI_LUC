{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_sizes(directory):\n",
    "    size_counts = defaultdict(int)\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                try:\n",
    "                    with Image.open(os.path.join(root, file)) as img:\n",
    "                        size = img.size\n",
    "                        size_counts[size] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error {file}: {e}\")\n",
    "    return size_counts\n",
    "\n",
    "base_path = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
    "\n",
    "splits = {\n",
    "    \"train\": [\n",
    "        os.path.join(base_path, \"train_images/authentic\"),\n",
    "        os.path.join(base_path, \"train_images/forged\"),\n",
    "    ],\n",
    "    \"supplemental\": [\n",
    "        os.path.join(base_path, \"supplemental_images\"),\n",
    "    ],\n",
    "    \"test\": [\n",
    "        os.path.join(base_path, \"test_images\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "total_summary = {}\n",
    "train_auth_count = 0\n",
    "train_forged_count = 0\n",
    "\n",
    "for split_name, dirs in splits.items():\n",
    "    agg_sizes = defaultdict(int)\n",
    "\n",
    "    for d in dirs:\n",
    "        sizes = get_unique_sizes(d)\n",
    "\n",
    "        # train authentic/forged counts\n",
    "        if split_name == \"train\":\n",
    "            if d.endswith(\"authentic\"):\n",
    "                train_auth_count += sum(sizes.values())\n",
    "            elif d.endswith(\"forged\"):\n",
    "                train_forged_count += sum(sizes.values())\n",
    "\n",
    "        for size, count in sizes.items():\n",
    "            agg_sizes[size] += count\n",
    "\n",
    "    total_images = sum(agg_sizes.values())\n",
    "    unique_sizes = len(agg_sizes)\n",
    "    total_summary[split_name] = total_images\n",
    "\n",
    "    print(f\"\\n=== {split_name.upper()} ===\")\n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Unique sizes: {unique_sizes}\")\n",
    "\n",
    "    top5 = sorted(agg_sizes.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(\"Top 5 sizes (width, height) -> count:\")\n",
    "    for (w, h), count in top5:\n",
    "        print(f\"  ({w}, {h}) -> {count}\")\n",
    "\n",
    "# explicit train breakdown\n",
    "print(\"\\n=== TRAIN BREAKDOWN ===\")\n",
    "print(f\"Authentic: {train_auth_count}\")\n",
    "print(f\"Forged:    {train_forged_count}\")\n",
    "\n",
    "print(\"\\n=== SUMMARY TOTALS ===\")\n",
    "for split_name, total_images in total_summary.items():\n",
    "    print(f\"{split_name.capitalize()}: {total_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d67faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_npy(directory):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".npy\"):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "base_path = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
    "\n",
    "train_masks_dir = os.path.join(base_path, \"train_masks\")\n",
    "supp_masks_dir  = os.path.join(base_path, \"supplemental_masks\")\n",
    "\n",
    "train_masks_count = count_npy(train_masks_dir)\n",
    "supp_masks_count  = count_npy(supp_masks_dir)\n",
    "\n",
    "print(\"\\n=== MASK COUNTS ===\")\n",
    "print(f\"Train masks:        {train_masks_count}\")\n",
    "print(f\"Supplemental masks: {supp_masks_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask_shapes(directory, k=5):\n",
    "    # collect .npy files\n",
    "    mask_files = [\n",
    "        os.path.join(directory, f)\n",
    "        for f in os.listdir(directory)\n",
    "        if f.lower().endswith(\".npy\")\n",
    "    ]\n",
    "    if not mask_files:\n",
    "        print(f\"No masks found in {directory}\")\n",
    "        return\n",
    "\n",
    "    # pick up to k random masks\n",
    "    sample = random.sample(mask_files, min(k, len(mask_files)))\n",
    "\n",
    "    print(f\"\\nShapes from {directory}:\")\n",
    "    for path in sample:\n",
    "        arr = np.load(path)\n",
    "        print(f\"{os.path.basename(path)} -> {arr.shape}\")\n",
    "\n",
    "random_mask_shapes(train_masks_dir)\n",
    "random_mask_shapes(supp_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff71a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_masks_channel_first(directory, max_channels=10):\n",
    "    mask_files = [\n",
    "        os.path.join(directory, f)\n",
    "        for f in os.listdir(directory)\n",
    "        if f.lower().endswith(\".npy\")\n",
    "    ]\n",
    "\n",
    "    non_channel_first = []\n",
    "\n",
    "    for path in mask_files:\n",
    "        arr = np.load(path, mmap_mode=\"r\")\n",
    "        shape = arr.shape\n",
    "\n",
    "        is_channel_first = (len(shape) > 0 and shape[0] <= max_channels)\n",
    "\n",
    "        if not is_channel_first:\n",
    "            non_channel_first.append((path, shape))\n",
    "\n",
    "    if non_channel_first:\n",
    "        print(f\"[WARNING] {len(non_channel_first)} masks in {directory} are NOT channel-first:\")\n",
    "        for p, s in non_channel_first[:10]:\n",
    "            print(f\"  {os.path.basename(p)} -> {s}\")\n",
    "        return True\n",
    "\n",
    "    print(f\"All masks in {directory} appear channel-first.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "train_bad = check_masks_channel_first(train_masks_dir)\n",
    "supp_bad  = check_masks_channel_first(supp_masks_dir)\n",
    "\n",
    "if train_bad or supp_bad:\n",
    "    print(\"\\n>> Some masks are NOT channel-first.\")\n",
    "else:\n",
    "    print(\"\\n>> All masks appear channel-first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804de48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mask_regions(*mask_dirs):\n",
    "    total_files = 0\n",
    "    total_regions = 0\n",
    "    images_with_multiple = 0\n",
    "    max_regions = 0\n",
    "\n",
    "    for mask_dir in mask_dirs:\n",
    "        for fname in os.listdir(mask_dir):\n",
    "            if not fname.lower().endswith(\".npy\"):\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(mask_dir, fname)\n",
    "            arr = np.load(path)\n",
    "\n",
    "            if arr.ndim == 2:\n",
    "                n_regions = 1\n",
    "            elif arr.ndim == 3:\n",
    "                n_regions = arr.shape[0]\n",
    "            else:\n",
    "                print(f\"Skipping {fname} (in {mask_dir}): unexpected shape {arr.shape}\")\n",
    "                continue\n",
    "\n",
    "            total_files += 1\n",
    "            total_regions += n_regions\n",
    "\n",
    "            if n_regions > 1:\n",
    "                images_with_multiple += 1\n",
    "\n",
    "            if n_regions > max_regions:\n",
    "                max_regions = n_regions\n",
    "\n",
    "    print(\"\\n=== MASK REGION SUMMARY (train + supplemental) ===\")\n",
    "    print(f\"Mask files (images with masks): {total_files}\")\n",
    "    print(f\"Total individual regions:       {total_regions}\")\n",
    "    print(f\"Images with ≥2 regions:         {images_with_multiple}\")\n",
    "    print(f\"Max regions in one image:       {max_regions}\")\n",
    "\n",
    "\n",
    "summarize_mask_regions(train_masks_dir, supp_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect region counts for each mask file (train + supplemental)\n",
    "region_counts = []\n",
    "\n",
    "for mask_dir in (train_masks_dir, supp_masks_dir):\n",
    "    for fname in os.listdir(mask_dir):\n",
    "        if fname.lower().endswith(\".npy\"):\n",
    "            arr = np.load(os.path.join(mask_dir, fname))\n",
    "            region_counts.append(1 if arr.ndim == 2 else arr.shape[0])\n",
    "\n",
    "plt.hist(region_counts, bins=range(1, max(region_counts)+2), align='left', rwidth=0.8)\n",
    "plt.xlabel(\"Number of forgery regions\")\n",
    "plt.ylabel(\"Count of images\")\n",
    "plt.title(\"Histogram of Forgery Regions per Mask (Train + Supplemental)\")\n",
    "plt.xticks(range(1, max(region_counts)+1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forgery_sizes = []          # pixel count per region\n",
    "forgery_percentages = []    # percentage of image area per region\n",
    "\n",
    "for mask_dir in (train_masks_dir, supp_masks_dir):\n",
    "    for fname in os.listdir(mask_dir):\n",
    "        if fname.lower().endswith(\".npy\"):\n",
    "            arr = np.load(os.path.join(mask_dir, fname))\n",
    "\n",
    "            if arr.ndim == 2:\n",
    "                arr = arr[np.newaxis, :, :]\n",
    "\n",
    "            C, H, W = arr.shape\n",
    "            img_area = H * W\n",
    "\n",
    "            for c in range(C):\n",
    "                region_pixels = arr[c].sum()\n",
    "                forgery_sizes.append(region_pixels)\n",
    "                forgery_percentages.append(region_pixels / img_area * 100.0)\n",
    "\n",
    "forgery_sizes = np.array(forgery_sizes)\n",
    "forgery_percentages = np.array(forgery_percentages)\n",
    "\n",
    "print(\"\\n=== FORGERY SIZE SUMMARY (pixels per region) ===\")\n",
    "print(f\"Total regions:      {len(forgery_sizes)}\")\n",
    "print(f\"Mean size:          {forgery_sizes.mean():.2f}\")\n",
    "print(f\"Median size:        {np.median(forgery_sizes):.2f}\")\n",
    "print(f\"Min size:           {forgery_sizes.min()}\")\n",
    "print(f\"Max size:           {forgery_sizes.max()}\")\n",
    "print(f\"Std deviation:      {forgery_sizes.std():.2f}\")\n",
    "\n",
    "print(\"\\n=== FORGERY SIZE AS % OF IMAGE AREA ===\")\n",
    "print(f\"Mean %:             {forgery_percentages.mean():.4f}\")\n",
    "print(f\"Median %:           {np.median(forgery_percentages):.4f}\")\n",
    "print(f\"Min %:              {forgery_percentages.min():.6f}\")\n",
    "print(f\"Max %:              {forgery_percentages.max():.4f}\")\n",
    "print(f\"Std deviation %:    {forgery_percentages.std():.4f}\")\n",
    "\n",
    "plt.hist(forgery_percentages, bins=50)\n",
    "plt.xlabel(\"Forgery region size (% of image area)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Forgery Region Sizes (Percent of Image Area)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_region_filters(max_width=5, max_height=5, area_thresholds=(20, 25, 40, 50)):\n",
    "    total_regions = 0\n",
    "    filtered_width = 0\n",
    "    filtered_height = 0\n",
    "    filtered_either = 0\n",
    "    pixel_areas = []\n",
    "\n",
    "    for mask_dir in (train_masks_dir, supp_masks_dir):\n",
    "        for fname in os.listdir(mask_dir):\n",
    "            if not fname.lower().endswith(\".npy\"):\n",
    "                continue\n",
    "\n",
    "            arr = np.load(os.path.join(mask_dir, fname))\n",
    "\n",
    "            if arr.ndim == 2:\n",
    "                arr = arr[np.newaxis, :, :]\n",
    "\n",
    "            C, H, W = arr.shape\n",
    "\n",
    "            for c in range(C):\n",
    "                region = arr[c]\n",
    "\n",
    "                if not np.any(region):\n",
    "                    continue\n",
    "\n",
    "                ys, xs = np.where(region > 0)\n",
    "                h = ys.max() - ys.min() + 1\n",
    "                w = xs.max() - xs.min() + 1\n",
    "\n",
    "                total_regions += 1\n",
    "\n",
    "                flag_w = w <= max_width\n",
    "                flag_h = h <= max_height\n",
    "\n",
    "                if flag_w:\n",
    "                    filtered_width += 1\n",
    "                if flag_h:\n",
    "                    filtered_height += 1\n",
    "                if flag_w or flag_h:\n",
    "                    filtered_either += 1\n",
    "\n",
    "                region_pixels = region.sum()\n",
    "                pixel_areas.append(region_pixels)\n",
    "\n",
    "    print(\"\\n=== SMALL REGION FILTER (by width / height) ===\")\n",
    "    print(f\"Total regions considered:            {total_regions}\")\n",
    "    print(f\"Filtered due to width <= {max_width}:   {filtered_width}\")\n",
    "    print(f\"Filtered due to height <= {max_height}:  {filtered_height}\")\n",
    "    print(f\"Filtered due to width OR height:     {filtered_either}\")\n",
    "\n",
    "    pixel_areas = np.array(pixel_areas)\n",
    "\n",
    "    print(\"\\n=== SMALL REGION FILTER (by pixel area) ===\")\n",
    "    if len(pixel_areas) == 0:\n",
    "        print(\"No regions found.\")\n",
    "        return\n",
    "\n",
    "    for thr in area_thresholds:\n",
    "        n_filtered = (pixel_areas <= thr).sum()\n",
    "        pct = n_filtered / len(pixel_areas) * 100.0\n",
    "        print(f\"Area <= {thr:4d}: filtered regions = {n_filtered} ({pct:.2f}% of regions)\")\n",
    "\n",
    "\n",
    "analyze_region_filters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label, sobel, binary_dilation\n",
    "from skimage.morphology import convex_hull_image\n",
    "\n",
    "region_areas = []\n",
    "region_components = []\n",
    "region_aspect = []\n",
    "region_elongation = []\n",
    "region_convexity = []\n",
    "region_entropy = []\n",
    "region_edge_density = []\n",
    "region_centroid_y = []\n",
    "region_centroid_x = []\n",
    "region_border_dist = []\n",
    "regions_per_img = []\n",
    "area_per_img = []\n",
    "\n",
    "forged_dir = os.path.join(base_path, \"train_images\", \"forged\")\n",
    "img_index = {}\n",
    "\n",
    "for img_name in os.listdir(forged_dir):\n",
    "    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        img_stem = os.path.splitext(img_name)[0]\n",
    "        img_index[img_stem] = os.path.join(forged_dir, img_name)\n",
    "\n",
    "for mask_name in os.listdir(train_masks_dir):\n",
    "    if not mask_name.lower().endswith(\".npy\"):\n",
    "        continue\n",
    "\n",
    "    mask_id = os.path.splitext(mask_name)[0]\n",
    "    img_path = img_index.get(mask_id)\n",
    "    if img_path is None:\n",
    "        continue\n",
    "\n",
    "    mask_arr = np.load(os.path.join(train_masks_dir, mask_name))\n",
    "    if mask_arr.ndim == 2:\n",
    "        mask_arr = mask_arr[np.newaxis, :, :]\n",
    "    ch, ht, wd = mask_arr.shape\n",
    "\n",
    "    img_gray = np.array(Image.open(img_path).convert(\"L\"), dtype=np.float32)\n",
    "\n",
    "    img_region_count = 0\n",
    "    img_region_area = 0\n",
    "\n",
    "    gx = sobel(img_gray, axis=1)\n",
    "    gy = sobel(img_gray, axis=0)\n",
    "    edge_mag = np.hypot(gx, gy)\n",
    "\n",
    "    for ci in range(ch):\n",
    "        region = mask_arr[ci] > 0\n",
    "        pix = region.sum()\n",
    "        if pix == 0:\n",
    "            continue\n",
    "\n",
    "        img_region_count += 1\n",
    "        img_region_area += pix\n",
    "        region_areas.append(pix)\n",
    "\n",
    "        # connected components\n",
    "        _, n_comp = label(region)\n",
    "        region_components.append(n_comp)\n",
    "\n",
    "        ys, xs = np.where(region)\n",
    "        y0, y1 = ys.min(), ys.max()\n",
    "        x0, x1 = xs.min(), xs.max()\n",
    "        h_box = y1 - y0 + 1\n",
    "        w_box = x1 - x0 + 1\n",
    "        region_aspect.append(h_box / w_box if w_box > 0 else 0.0)\n",
    "\n",
    "        coords = np.stack([xs, ys], axis=1).astype(np.float32)\n",
    "        coords -= coords.mean(axis=0, keepdims=True)\n",
    "        cov = coords.T @ coords / coords.shape[0]\n",
    "        eigvals, _ = np.linalg.eig(cov)\n",
    "        eigvals = np.sort(np.real(eigvals))\n",
    "        if eigvals[0] > 1e-6:\n",
    "            region_elongation.append(float(np.sqrt(eigvals[-1] / eigvals[0])))\n",
    "        else:\n",
    "            region_elongation.append(0.0)\n",
    "\n",
    "        hull = convex_hull_image(region)\n",
    "        hull_area = hull.sum()\n",
    "        region_convexity.append(pix / hull_area if hull_area > 0 else 0.0)\n",
    "\n",
    "        cy = ys.mean() / ht\n",
    "        cx = xs.mean() / wd\n",
    "        region_centroid_y.append(cy)\n",
    "        region_centroid_x.append(cx)\n",
    "        region_border_dist.append(min(cy, 1 - cy, cx, 1 - cx))\n",
    "\n",
    "        vals = img_gray[region]\n",
    "        hist, _ = np.histogram(vals, bins=32, range=(0, 255), density=True)\n",
    "        hist = hist[hist > 0]\n",
    "        region_entropy.append(float(-(hist * np.log2(hist)).sum()))\n",
    "\n",
    "        region_edge_density.append(float(edge_mag[region].mean()))\n",
    "\n",
    "    if img_region_count > 0:\n",
    "        regions_per_img.append(img_region_count)\n",
    "        area_per_img.append(img_region_area)\n",
    "\n",
    "region_areas = np.array(region_areas)\n",
    "region_components = np.array(region_components)\n",
    "region_aspect = np.array(region_aspect)\n",
    "region_elongation = np.array(region_elongation)\n",
    "region_convexity = np.array(region_convexity)\n",
    "region_entropy = np.array(region_entropy)\n",
    "region_edge_density = np.array(region_edge_density)\n",
    "region_centroid_y = np.array(region_centroid_y)\n",
    "region_centroid_x = np.array(region_centroid_x)\n",
    "region_border_dist = np.array(region_border_dist)\n",
    "regions_per_img = np.array(regions_per_img)\n",
    "area_per_img = np.array(area_per_img)\n",
    "\n",
    "print(\"\\n=== REGION COMPLEXITY ===\")\n",
    "print(\"Regions:\", len(region_areas))\n",
    "print(\"Connected components: mean\", region_components.mean(), \"max\", region_components.max())\n",
    "print(\"Aspect ratio: mean\", region_aspect.mean())\n",
    "print(\"Elongation: mean\", region_elongation.mean())\n",
    "print(\"Convexity: mean\", region_convexity.mean())\n",
    "print(\"Entropy: mean\", region_entropy.mean())\n",
    "print(\"Edge density: mean\", region_edge_density.mean())\n",
    "\n",
    "print(\"\\n=== SPATIAL DISTRIBUTION ===\")\n",
    "print(\"Centroid Y (0=top,1=bottom): mean\", region_centroid_y.mean())\n",
    "print(\"Centroid X (0=left,1=right): mean\", region_centroid_x.mean())\n",
    "print(\"Border distance (normalized): mean\", region_border_dist.mean(), \"min\", region_border_dist.min())\n",
    "\n",
    "print(\"\\n=== REGION SCALE PATTERNS ===\")\n",
    "log_area = np.log10(region_areas + 1)\n",
    "print(\"Log10(area): mean\", log_area.mean(), \"std\", log_area.std())\n",
    "print(\"Regions per image: mean\", regions_per_img.mean(), \"max\", regions_per_img.max())\n",
    "corr = np.corrcoef(regions_per_img, area_per_img)[0, 1]\n",
    "print(\"Corr(regions per image, total forged area):\", corr)\n",
    "\n",
    "# This helps decide whether to emphasize:\n",
    "\n",
    "# fine-grained detail (small forgeries)\n",
    "\n",
    "# global context (large forgeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb993891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.ndimage import binary_dilation, binary_erosion\n",
    "\n",
    "def load_forged_gray(mask_fname):\n",
    "    stem = os.path.splitext(mask_fname)[0]\n",
    "    for ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "        p = os.path.join(forged_dir, stem + ext)\n",
    "        if os.path.exists(p):\n",
    "            return np.array(Image.open(p).convert(\"L\"), dtype=np.float32) / 255.0\n",
    "    return None\n",
    "\n",
    "# Texture consistency: border inside vs border outside (SSIM)\n",
    "def analyze_texture_consistency(n_samples=100):\n",
    "    mask_files = [f for f in os.listdir(train_masks_dir) if f.lower().endswith(\".npy\")]\n",
    "    if not mask_files:\n",
    "        print(\"No mask files found.\")\n",
    "        return\n",
    "\n",
    "    chosen = random.sample(mask_files, min(n_samples, len(mask_files)))\n",
    "    ssim_scores = []\n",
    "\n",
    "    for fname in chosen:\n",
    "        m_arr = np.load(os.path.join(train_masks_dir, fname))\n",
    "        if m_arr.ndim == 2:\n",
    "            m_arr = m_arr[np.newaxis, :, :]\n",
    "\n",
    "        img = load_forged_gray(fname)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        for c in range(m_arr.shape[0]):\n",
    "            m = m_arr[c] > 0.5\n",
    "            if m.sum() < 50:\n",
    "                continue\n",
    "\n",
    "            dil = binary_dilation(m, iterations=2)\n",
    "            ero = binary_erosion(m, iterations=2)\n",
    "\n",
    "            border_in = m & ~ero\n",
    "            border_out = dil & ~m\n",
    "\n",
    "            if border_in.sum() < 20 or border_out.sum() < 20:\n",
    "                continue\n",
    "\n",
    "            ys, xs = np.where(dil)\n",
    "            y0, y1 = ys.min(), ys.max() + 1\n",
    "            x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "            img_roi = img[y0:y1, x0:x1]\n",
    "            bi_roi = border_in[y0:y1, x0:x1]\n",
    "            bo_roi = border_out[y0:y1, x0:x1]\n",
    "\n",
    "            inside_img = img_roi * bi_roi\n",
    "            outside_img = img_roi * bo_roi\n",
    "\n",
    "            try:\n",
    "                score = ssim(inside_img, outside_img, data_range=1.0)\n",
    "                ssim_scores.append(score)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if not ssim_scores:\n",
    "        print(\"No valid border pairs for SSIM.\")\n",
    "        return\n",
    "\n",
    "    ssim_scores = np.array(ssim_scores)\n",
    "    print(\"\\n=== TEXTURE CONSISTENCY (border inside vs outside) ===\")\n",
    "    print(f\"Samples used: {len(ssim_scores)}\")\n",
    "    print(f\"Mean SSIM:    {ssim_scores.mean():.4f}\")\n",
    "    print(f\"Median SSIM:  {np.median(ssim_scores):.4f}\")\n",
    "    print(f\"Min / Max:    {ssim_scores.min():.4f} / {ssim_scores.max():.4f}\")\n",
    "\n",
    "# Copy–move characteristics via global autocorrelation (dominant displacement)\n",
    "def analyze_copy_move_displacements(n_samples=10):\n",
    "    mask_files = [f for f in os.listdir(train_masks_dir) if f.lower().endswith(\".npy\")]\n",
    "    if not mask_files:\n",
    "        print(\"No mask files found.\")\n",
    "        return\n",
    "\n",
    "    chosen = random.sample(mask_files, min(n_samples, len(mask_files)))\n",
    "    displacements = []\n",
    "\n",
    "    print(\"\\n=== COPY–MOVE DISPLACEMENT ESTIMATES (autocorrelation peaks) ===\")\n",
    "    for fname in chosen:\n",
    "        img = load_forged_gray(fname)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        # zero-mean for autocorrelation\n",
    "        im = img - img.mean()\n",
    "        fft = np.fft.fft2(im)\n",
    "        ac = np.fft.ifft2(np.abs(fft) ** 2).real\n",
    "        ac_shift = np.fft.fftshift(ac)\n",
    "\n",
    "        # center is zero displacement\n",
    "        h, w = ac_shift.shape\n",
    "        cy, cx = h // 2, w // 2\n",
    "\n",
    "        ac_flat = ac_shift.copy()\n",
    "        ac_flat[cy, cx] = -np.inf  # ignore zero-lag peak\n",
    "\n",
    "        idx = np.argmax(ac_flat)\n",
    "        dy, dx = np.unravel_index(idx, ac_shift.shape)\n",
    "        dy -= cy\n",
    "        dx -= cx\n",
    "\n",
    "        displacements.append((dy, dx))\n",
    "        print(f\"{fname}: strongest non-zero displacement ~ (dy={dy}, dx={dx})\")\n",
    "\n",
    "    if displacements:\n",
    "        dy_vals = np.array([d[0] for d in displacements])\n",
    "        dx_vals = np.array([d[1] for d in displacements])\n",
    "        print(\"\\nSummary of dominant displacements (pixels):\")\n",
    "        print(f\"Mean dy, dx: ({dy_vals.mean():.2f}, {dx_vals.mean():.2f})\")\n",
    "        print(f\"Median |dy|, |dx|: ({np.median(np.abs(dy_vals)):.2f}, {np.median(np.abs(dx_vals)):.2f})\")\n",
    "\n",
    "# run both analyses\n",
    "analyze_texture_consistency()\n",
    "analyze_copy_move_displacements()\n",
    "\n",
    "# A feature that searches for duplicated features (ORB/SIFT keypoints)\n",
    "\n",
    "# A model architecture focusing on patch-matching (like PatchCore or Swin with windowed attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78297be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_copy_move_displacements_orb_local(\n",
    "    n_samples=100,\n",
    "    max_kp=5000,\n",
    "    min_disp=3,\n",
    "    min_matches=10\n",
    "):\n",
    "    mask_files = [f for f in os.listdir(train_masks_dir) if f.lower().endswith(\".npy\")]\n",
    "    if not mask_files:\n",
    "        print(\"No mask files found.\")\n",
    "        return\n",
    "\n",
    "    chosen = random.sample(mask_files, min(n_samples, len(mask_files)))\n",
    "    all_dy, all_dx = [], []\n",
    "\n",
    "    print(\"\\n=== COPY–MOVE DISPLACEMENT ESTIMATES (ORB keypoint matches, local) ===\")\n",
    "    orb = cv2.ORB_create(max_kp)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "\n",
    "    for fname in chosen:\n",
    "        # load mask\n",
    "        m_arr = np.load(os.path.join(train_masks_dir, fname))\n",
    "        if m_arr.ndim == 2:\n",
    "            m = m_arr > 0.5\n",
    "        else:\n",
    "            m = (m_arr > 0.5).any(axis=0)  # union over channels\n",
    "\n",
    "        # load forged image\n",
    "        img_f = load_forged_gray(fname)\n",
    "        if img_f is None:\n",
    "            continue\n",
    "\n",
    "        img_u8 = (img_f * 255).astype(np.uint8)\n",
    "\n",
    "        # detect ORB keypoints\n",
    "        kps, desc = orb.detectAndCompute(img_u8, None)\n",
    "        if desc is None or len(kps) < 2:\n",
    "            continue\n",
    "\n",
    "        # split keypoints into inside-mask (forged) and outside-mask (background)\n",
    "        inside_idx, outside_idx = [], []\n",
    "        h, w = m.shape\n",
    "        for i, kp in enumerate(kps):\n",
    "            x, y = int(round(kp.pt[0])), int(round(kp.pt[1]))\n",
    "            if 0 <= x < w and 0 <= y < h:\n",
    "                if m[y, x]:\n",
    "                    inside_idx.append(i)\n",
    "                else:\n",
    "                    outside_idx.append(i)\n",
    "\n",
    "        if len(inside_idx) == 0 or len(outside_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        desc_in  = desc[inside_idx]\n",
    "        desc_out = desc[outside_idx]\n",
    "        kps_in   = [kps[i] for i in inside_idx]\n",
    "        kps_out  = [kps[i] for i in outside_idx]\n",
    "\n",
    "        if len(desc_in) == 0 or len(desc_out) == 0:\n",
    "            continue\n",
    "\n",
    "        # knn matching: forged region -> background\n",
    "        knn_matches = bf.knnMatch(desc_in, desc_out, k=2)\n",
    "        disps = []\n",
    "\n",
    "        for mlist in knn_matches:\n",
    "            if len(mlist) < 2:\n",
    "                continue\n",
    "            m1, m2 = mlist\n",
    "            # Lowe ratio test\n",
    "            if m1.distance >= 0.75 * m2.distance:\n",
    "                continue\n",
    "\n",
    "            q_idx = m1.queryIdx\n",
    "            t_idx = m1.trainIdx\n",
    "            \n",
    "            p1 = kps_in[q_idx].pt   # inside (forged)\n",
    "            p2 = kps_out[t_idx].pt  # outside (source/background)\n",
    "\n",
    "            dx = p2[0] - p1[0]\n",
    "            dy = p2[1] - p1[1]\n",
    "            if abs(dx) + abs(dy) < min_disp:\n",
    "                continue  # ignore tiny shifts\n",
    "\n",
    "            disps.append((dy, dx))\n",
    "\n",
    "        if len(disps) < min_matches:\n",
    "            continue\n",
    "\n",
    "        dy_arr = np.array([d[0] for d in disps])\n",
    "        dx_arr = np.array([d[1] for d in disps])\n",
    "\n",
    "        dom_dy = np.median(dy_arr)\n",
    "        dom_dx = np.median(dx_arr)\n",
    "\n",
    "        print(f\"{fname}: ~ (dy={dom_dy:.1f}, dx={dom_dx:.1f}) \"\n",
    "              f\"from {len(disps)} matches\")\n",
    "\n",
    "        all_dy.extend(dy_arr)\n",
    "        all_dx.extend(dx_arr)\n",
    "\n",
    "    if all_dy:\n",
    "        all_dy = np.array(all_dy)\n",
    "        all_dx = np.array(all_dx)\n",
    "        print(\"\\nSummary over all images (pixels):\")\n",
    "        print(f\"Mean dy, dx: ({all_dy.mean():.2f}, {all_dx.mean():.2f})\")\n",
    "        print(f\"Median dy, dx: ({np.median(all_dy):.2f}, {np.median(all_dx):.2f})\")\n",
    "        print(f\"Median |dy|, |dx|: ({np.median(np.abs(all_dy)):.2f}, \"\n",
    "              f\"{np.median(np.abs(all_dx)):.2f})\")\n",
    "    else:\n",
    "        print(\"No non-trivial displacements found (after mask-aware matching).\")\n",
    "\n",
    "# run it\n",
    "analyze_copy_move_displacements_orb_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask_perimeter(mask2d):\n",
    "    m = mask2d.astype(bool)\n",
    "    if m.sum() == 0:\n",
    "        return 0\n",
    "    p = np.pad(m, 1, constant_values=False)\n",
    "    core = p[1:-1, 1:-1]\n",
    "    perim = 0\n",
    "    perim += np.logical_and(core, ~p[1:-1, 2:]).sum()   # right\n",
    "    perim += np.logical_and(core, ~p[1:-1, 0:-2]).sum() # left\n",
    "    perim += np.logical_and(core, ~p[2:, 1:-1]).sum()   # down\n",
    "    perim += np.logical_and(core, ~p[0:-2, 1:-1]).sum() # up\n",
    "    return perim\n",
    "\n",
    "def _fractal_dim(mask2d):\n",
    "    m = mask2d.astype(bool)\n",
    "    h, w = m.shape\n",
    "    sizes = [1, 2, 4, 8, 16, 32]\n",
    "    eps = []\n",
    "    counts = []\n",
    "    for s in sizes:\n",
    "        if s > min(h, w):\n",
    "            continue\n",
    "        hh = (h // s) * s\n",
    "        ww = (w // s) * s\n",
    "        if hh == 0 or ww == 0:\n",
    "            continue\n",
    "        mm = m[:hh, :ww]\n",
    "        mm = mm.reshape(hh // s, s, ww // s, s)\n",
    "        red = mm.max(axis=(1, 3))\n",
    "        n = red.sum()\n",
    "        if n > 0:\n",
    "            eps.append(1.0 / s)\n",
    "            counts.append(n)\n",
    "    if len(counts) < 2:\n",
    "        return np.nan\n",
    "    eps = np.array(eps, dtype=float)\n",
    "    counts = np.array(counts, dtype=float)\n",
    "    coef = np.polyfit(np.log(1.0 / eps), np.log(counts), 1)\n",
    "    return coef[0]\n",
    "\n",
    "def analyze_forgery_nature(mask_dir, image_dir, max_images=None):\n",
    "    perim_area_ratios = []\n",
    "    jaggedness_vals = []\n",
    "    fractal_vals = []\n",
    "    bg_vars = []\n",
    "    bg_edges = []\n",
    "\n",
    "    processed_images = 0\n",
    "\n",
    "    for fname in os.listdir(mask_dir):\n",
    "        if not fname.lower().endswith(\".npy\"):\n",
    "            continue\n",
    "\n",
    "        base = os.path.splitext(fname)[0]\n",
    "        img_path = None\n",
    "        for ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "            candidate = os.path.join(image_dir, base + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                img_path = candidate\n",
    "                break\n",
    "        if img_path is None:\n",
    "            continue\n",
    "\n",
    "        arr = np.load(os.path.join(mask_dir, fname))\n",
    "        if arr.ndim == 2:\n",
    "            arr = arr[np.newaxis, :, :]\n",
    "        C, H, W = arr.shape\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img_arr = np.array(img, dtype=np.float32) / 255.0\n",
    "\n",
    "        for c in range(C):\n",
    "            m = arr[c].astype(bool)\n",
    "            area = m.sum()\n",
    "            if area == 0:\n",
    "                continue\n",
    "\n",
    "            # A. Boundary complexity\n",
    "            perim = _mask_perimeter(m)\n",
    "            perim_area_ratios.append(perim / area)\n",
    "            circ_perim = 2.0 * np.sqrt(np.pi * area)\n",
    "            jaggedness_vals.append(perim / (circ_perim + 1e-8))\n",
    "            fractal_vals.append(_fractal_dim(m))\n",
    "\n",
    "            # B. Region interaction (simple background complexity proxy)\n",
    "            ys, xs = np.where(m)\n",
    "            y0, y1 = ys.min(), ys.max() + 1\n",
    "            x0, x1 = xs.min(), xs.max() + 1\n",
    "            patch = img_arr[y0:y1, x0:x1]\n",
    "            if patch.size == 0:\n",
    "                continue\n",
    "\n",
    "            bg_vars.append(patch.var())\n",
    "            gx = np.abs(patch[:, 1:] - patch[:, :-1])\n",
    "            gy = np.abs(patch[1:, :] - patch[:-1, :])\n",
    "            \n",
    "            edge_vals = []\n",
    "            if gx.size > 0:\n",
    "                edge_vals.append(gx.mean())\n",
    "            if gy.size > 0:\n",
    "                edge_vals.append(gy.mean())\n",
    "            \n",
    "            if edge_vals:\n",
    "                bg_edges.append(float(np.mean(edge_vals)))\n",
    "            else:\n",
    "                # degenerate 1x1 patch: no edges; treat as 0 complexity\n",
    "                bg_edges.append(0.0)\n",
    "\n",
    "        processed_images += 1\n",
    "        if max_images is not None and processed_images >= max_images:\n",
    "            break\n",
    "\n",
    "    perim_area_ratios = np.array(perim_area_ratios, dtype=float)\n",
    "    jaggedness_vals = np.array(jaggedness_vals, dtype=float)\n",
    "    fractal_vals = np.array(fractal_vals, dtype=float)\n",
    "    bg_vars = np.array(bg_vars, dtype=float)\n",
    "    bg_edges = np.array(bg_edges, dtype=float)\n",
    "\n",
    "    print(\"\\n=== BOUNDARY COMPLEXITY (per region) ===\")\n",
    "    print(f\"Regions analyzed:             {len(perim_area_ratios)}\")\n",
    "    print(f\"Perimeter/area mean:          {perim_area_ratios.mean():.4f}\")\n",
    "    print(f\"Perimeter/area median:        {np.median(perim_area_ratios):.4f}\")\n",
    "    print(f\"Jaggedness mean (circle=1):   {jaggedness_vals.mean():.4f}\")\n",
    "    print(f\"Jaggedness median:            {np.median(jaggedness_vals):.4f}\")\n",
    "    print(f\"Fractal dimension mean:       {np.nanmean(fractal_vals):.4f}\")\n",
    "    print(f\"Fractal dimension median:     {np.nanmedian(fractal_vals):.4f}\")\n",
    "\n",
    "    print(\"\\n=== REGION INTERACTION PROXIES (image patch under mask) ===\")\n",
    "    print(f\"Patch variance mean:          {bg_vars.mean():.6f}\")\n",
    "    print(f\"Patch variance median:        {np.median(bg_vars):.6f}\")\n",
    "    print(f\"Edge magnitude mean:          {bg_edges.mean():.6f}\")\n",
    "    print(f\"Edge magnitude median:        {np.median(bg_edges):.6f}\")\n",
    "\n",
    "# forged images dir from your earlier splits\n",
    "forged_images_dir = splits[\"train\"][1]\n",
    "\n",
    "analyze_forgery_nature(train_masks_dir, forged_images_dir, max_images=None)\n",
    "\n",
    "# Certain architectures excel at detecting:\n",
    "\n",
    "# Smooth blending\n",
    "\n",
    "# Hard cut-and-paste edges\n",
    "\n",
    "# Irregular cloning artifacts\n",
    "\n",
    "# This helps decide whether to incorporate:\n",
    "\n",
    "# OCR signals\n",
    "\n",
    "# Structural representations of scientific figures\n",
    "\n",
    "# Local high-frequency filters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
