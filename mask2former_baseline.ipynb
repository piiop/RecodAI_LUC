{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import sympy\n",
    "import sympy.printing  # ensure submodule is imported\n",
    "sympy.printing = sympy.printing  # attach as attribute explicitly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights\n",
    "from torchvision.ops import FeaturePyramidNetwork\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa60692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_structure():\n",
    "    base_path = '/kaggle/input/recodai-luc-scientific-image-forgery-detection'\n",
    "    \n",
    "    # Checking train images\n",
    "    train_authentic_path = os.path.join(base_path, 'train_images/authentic')\n",
    "    train_forged_path = os.path.join(base_path, 'train_images/forged')\n",
    "    train_masks_path = os.path.join(base_path, 'train_masks')\n",
    "    test_images_path = os.path.join(base_path, 'test_images')\n",
    "    \n",
    "    print(f\"Authentic images: {len(os.listdir(train_authentic_path))}\")\n",
    "    print(f\"Forged images: {len(os.listdir(train_forged_path))}\")\n",
    "    print(f\"Masks: {len(os.listdir(train_masks_path))}\")\n",
    "    print(f\"Test images: {len(os.listdir(test_images_path))}\")\n",
    "    \n",
    "    # Let's analyze some examples of masks\n",
    "    mask_files = os.listdir(train_masks_path)[:5]\n",
    "    print(f\"Examples of mask files: {mask_files}\")\n",
    "    \n",
    "    # Checking the mask format\n",
    "    sample_mask = np.load(os.path.join(train_masks_path, mask_files[0]))\n",
    "    print(f\"Mask format: {sample_mask.shape}, dtype: {sample_mask.dtype}\")\n",
    "    \n",
    "    test_files = os.listdir(test_images_path)\n",
    "    print(f\"Test images: {test_files}\")\n",
    "    \n",
    "    return {\n",
    "        'train_authentic': train_authentic_path,\n",
    "        'train_forged': train_forged_path,\n",
    "        'train_masks': train_masks_path,\n",
    "        'test_images': test_images_path\n",
    "    }\n",
    "\n",
    "paths = analyze_data_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForgeryDataset(Dataset):\n",
    "    def __init__(self, authentic_path, forged_path, masks_path, transform=None, is_train=True):\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Collect all data samples\n",
    "        self.samples = []\n",
    "        \n",
    "        # Authentic images\n",
    "        for file in os.listdir(authentic_path):\n",
    "            img_path = os.path.join(authentic_path, file)\n",
    "            base_name = file.split('.')[0]\n",
    "            mask_path = os.path.join(masks_path, f\"{base_name}.npy\")\n",
    "            \n",
    "            self.samples.append({\n",
    "                'image_path': img_path,\n",
    "                'mask_path': mask_path,\n",
    "                'is_forged': False,\n",
    "                'image_id': base_name\n",
    "            })\n",
    "        \n",
    "        # Forged images\n",
    "        for file in os.listdir(forged_path):\n",
    "            img_path = os.path.join(forged_path, file)\n",
    "            base_name = file.split('.')[0]\n",
    "            mask_path = os.path.join(masks_path, f\"{base_name}.npy\")\n",
    "            \n",
    "            self.samples.append({\n",
    "                'image_path': img_path,\n",
    "                'mask_path': mask_path,\n",
    "                'is_forged': True,\n",
    "                'image_id': base_name\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(sample['image_path']).convert('RGB')\n",
    "        image = np.array(image)  # (H, W, 3)\n",
    "        \n",
    "        # Load and process mask\n",
    "        if os.path.exists(sample['mask_path']):\n",
    "            mask = np.load(sample['mask_path'])\n",
    "            \n",
    "            # Handle multi-channel masks\n",
    "            if mask.ndim == 3:\n",
    "                if mask.shape[0] <= 10:  # channels first (C, H, W)\n",
    "                    mask = np.any(mask, axis=0)\n",
    "                elif mask.shape[-1] <= 10:  # channels last (H, W, C)\n",
    "                    mask = np.any(mask, axis=-1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Ambiguous 3D mask shape: {mask.shape}\")\n",
    "            \n",
    "            mask = (mask > 0).astype(np.uint8)\n",
    "        else:\n",
    "            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "    \n",
    "        # Shape validation\n",
    "        assert image.shape[:2] == mask.shape, f\"Shape mismatch: img {image.shape}, mask {mask.shape}\"\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        else:\n",
    "            image = F_transforms.to_tensor(image)\n",
    "            mask = torch.tensor(mask, dtype=torch.uint8)\n",
    "        \n",
    "        # Prepare targets for Mask2Former\n",
    "        if sample['is_forged'] and mask.sum() > 0:\n",
    "            boxes, labels, masks = self.mask_to_boxes(mask)\n",
    "            \n",
    "            target = {\n",
    "                'boxes': boxes,\n",
    "                'labels': labels,\n",
    "                'masks': masks,\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "                'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "            }\n",
    "            target['image_label'] = torch.tensor(1.0)   # forged\n",
    "        else:\n",
    "            # For authentic images or images without masks\n",
    "            target = {\n",
    "                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'masks': torch.zeros((0, image.shape[1], image.shape[2]), dtype=torch.uint8),\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': torch.zeros(0, dtype=torch.float32),\n",
    "                'iscrowd': torch.zeros((0,), dtype=torch.int64)\n",
    "            }\n",
    "            target['image_label'] = torch.tensor(0.0)   # authentic\n",
    "        return image, target\n",
    "    \n",
    "    def mask_to_boxes(self, mask):\n",
    "        \"\"\"Convert segmentation mask to bounding boxes for Mask R-CNN\"\"\"\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask_np = mask.numpy()\n",
    "        else:\n",
    "            mask_np = mask\n",
    "        \n",
    "        # Find contours in the mask\n",
    "        contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        boxes = []\n",
    "        masks = []\n",
    "        \n",
    "        for contour in contours:\n",
    "            if len(contour) > 0:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                # Filter out very small regions\n",
    "                if w > 5 and h > 5:\n",
    "                    boxes.append([x, y, x + w, y + h])\n",
    "                    # Create binary mask for this contour\n",
    "                    contour_mask = np.zeros_like(mask_np)\n",
    "                    cv2.fillPoly(contour_mask, [contour], 1)\n",
    "                    masks.append(contour_mask)\n",
    "        \n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "            masks = torch.tensor(np.array(masks), dtype=torch.uint8)\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "            masks = torch.zeros((0, mask_np.shape[0], mask_np.shape[1]), dtype=torch.uint8)\n",
    "        \n",
    "        return boxes, labels, masks\n",
    "\n",
    "# Transformations for learning\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba758ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ForgeryDataset(\n",
    "    paths['train_authentic'], \n",
    "    paths['train_forged'], \n",
    "    paths['train_masks'],\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Changing transformations for the val dataset\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "# Creating dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62984b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtFPNBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNeXt-T backbone with FPN neck.\n",
    "    Uses outputs from indices [1, 3, 5, 7], which have channels\n",
    "    [96, 192, 384, 768] according to your debug print.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels=256, train_backbone=True):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            backbone = convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "        except Exception:\n",
    "            backbone = convnext_tiny(weights=None)\n",
    "\n",
    "        self.body = backbone.features  # modules 0..7\n",
    "\n",
    "        # Use the last block of each resolution stage:\n",
    "        #  1: 96 @ 64×64\n",
    "        #  3: 192 @ 32×32\n",
    "        #  5: 384 @ 16×16\n",
    "        #  7: 768 @  8×8\n",
    "        self.out_indices = (1, 3, 5, 7)\n",
    "\n",
    "        in_channels_list = [96, 192, 384, 768]\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=in_channels_list,\n",
    "            out_channels=out_channels,\n",
    "        )\n",
    "\n",
    "        if not train_backbone:\n",
    "            for p in self.body.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        out = x\n",
    "        for i, layer in enumerate(self.body):\n",
    "            out = layer(out)\n",
    "            if i in self.out_indices:\n",
    "                feats.append(out)\n",
    "\n",
    "        # feats is a list of 4 tensors with channels [96, 192, 384, 768]\n",
    "        feat_dict = {str(i): f for i, f in enumerate(feats)}  # keys \"0\",\"1\",\"2\",\"3\"\n",
    "        fpn_out = self.fpn(feat_dict)  # dict of {level_name: [B, C, H, W]}\n",
    "\n",
    "        # Sort by level name to keep consistent order\n",
    "        levels = [fpn_out[k] for k in sorted(fpn_out.keys(), key=int)]\n",
    "        return levels  # [P2, P3, P4, P5], all with C=out_channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_fpn = ConvNeXtFPNBackbone(out_channels=256, train_backbone=False).eval()\n",
    "\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    levels = backbone_fpn(x)\n",
    "    print(\"=== FPN levels ===\")\n",
    "    for i, f in enumerate(levels):\n",
    "        print(f\"Level {i}: {tuple(f.shape)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    DETR-style transformer decoder operating on query embeddings\n",
    "    attending over concatenated multi-scale FPN features.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, nhead=8, num_layers=6, dim_feedforward=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,  # [S, B, C]\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(layer, num_layers=num_layers)\n",
    "\n",
    "        # Simple sine positional encoding\n",
    "        self.pos_embed = PositionEmbeddingSine(num_pos_feats=d_model // 2, normalize=True)\n",
    "\n",
    "    def forward(self, query_embed, multi_scale_feats):\n",
    "        \"\"\"\n",
    "        query_embed: [num_queries, C]\n",
    "        multi_scale_feats: list of [B, C, H, W]\n",
    "        \"\"\"\n",
    "        B = multi_scale_feats[0].shape[0]\n",
    "        C = multi_scale_feats[0].shape[1]\n",
    "\n",
    "        # Build memory by concatenating flattened multi-scale feature maps\n",
    "        srcs = []\n",
    "        pos_embs = []\n",
    "        for feat in multi_scale_feats:\n",
    "            pos = self.pos_embed(feat)  # [B, C, H, W]\n",
    "            B_, C_, H, W = feat.shape\n",
    "            srcs.append(feat.flatten(2).permute(2, 0, 1))     # [HW, B, C]\n",
    "            pos_embs.append(pos.flatten(2).permute(2, 0, 1))  # [HW, B, C]\n",
    "\n",
    "        memory = torch.cat(srcs, dim=0)        # [S, B, C]\n",
    "        pos_all = torch.cat(pos_embs, dim=0)   # [S, B, C]\n",
    "\n",
    "        # Prepare queries: [num_queries, B, C]\n",
    "        num_queries = query_embed.shape[0]\n",
    "        query = query_embed.unsqueeze(1).repeat(1, B, 1)  # [Q, B, C]\n",
    "\n",
    "        # Decoder with cross-attention to memory\n",
    "        hs = self.decoder(tgt=query, memory=memory, tgt_pos=None, memory_key_padding_mask=None, memory_pos=pos_all)\n",
    "        # hs: [Q, B, C]\n",
    "        hs = hs.permute(1, 0, 2)  # [B, Q, C]\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e2cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard sine-cosine positional encoding as in DETR.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=128, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        self.scale = 2 * torch.pi if scale is None else scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        B, C, H, W = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        y_embed = torch.arange(H, device=device).unsqueeze(1).repeat(1, W)\n",
    "        x_embed = torch.arange(W, device=device).unsqueeze(0).repeat(H, 1)\n",
    "\n",
    "        y_embed = y_embed.float()\n",
    "        x_embed = x_embed.float()\n",
    "\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (H - 1 + eps) * self.scale\n",
    "            x_embed = x_embed / (W - 1 + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, device=device).float()\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[..., None] / dim_t\n",
    "        pos_y = y_embed[..., None] / dim_t\n",
    "        pos_x = torch.stack([pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()], dim=-1).flatten(-2)\n",
    "        pos_y = torch.stack([pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()], dim=-1).flatten(-2)\n",
    "\n",
    "        pos = torch.cat([pos_y, pos_x], dim=-1)  # [H, W, 2*num_pos_feats]\n",
    "        pos = pos.permute(2, 0, 1).unsqueeze(0).expand(B, -1, -1, -1)  # [B, C, H, W]\n",
    "        return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0fdf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ConvNeXt-Tiny exactly as in the Mask2Former model\n",
    "try:\n",
    "    backbone = convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "except Exception:\n",
    "    backbone = convnext_tiny(weights=None)\n",
    "\n",
    "backbone.eval()\n",
    "\n",
    "# Dummy input\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = x\n",
    "    print(\"Input:\", x.shape)\n",
    "    for i, layer in enumerate(backbone.features):\n",
    "        out = layer(out)\n",
    "        print(f\"features[{i}] output:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure ConvNeXtFPNBackbone is already defined above this cell\n",
    "backbone_fpn = ConvNeXtFPNBackbone(out_channels=256, train_backbone=False).eval()\n",
    "\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats = []\n",
    "    out = x\n",
    "    print(\"=== Debug ConvNeXtFPNBackbone ===\")\n",
    "    for i, layer in enumerate(backbone_fpn.body):\n",
    "        out = layer(out)\n",
    "        print(f\"body[{i}] output:\", out.shape)\n",
    "    print(\"---- FPN inputs (as dict keys / shapes) ----\")\n",
    "    # Re-run forward like in the class, but instrumented\n",
    "    out = x\n",
    "    feat_list = []\n",
    "    for i, layer in enumerate(backbone_fpn.body):\n",
    "        out = layer(out)\n",
    "        if i in backbone_fpn.out_indices:\n",
    "            feat_list.append((i, out))\n",
    "    for idx, feat in feat_list:\n",
    "        print(f\"Selected feature index {idx} -> {feat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13349081",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    feat_dict = {str(i): f for i, (_, f) in enumerate(feat_list)}\n",
    "    fpn_out = backbone_fpn.fpn(feat_dict)\n",
    "    print(\"---- FPN outputs ----\")\n",
    "    for k in sorted(fpn_out.keys(), key=int):\n",
    "        print(f\"FPN level {k} -> {fpn_out[k].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask2FormerForgeryModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Instance-Seg Transformer (Mask2Former-style) + Authenticity Gate baseline.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_queries=15,\n",
    "        d_model=256,\n",
    "        nhead=8,\n",
    "        num_decoder_layers=6,\n",
    "        mask_dim=256,\n",
    "        backbone_trainable=True,\n",
    "        authenticity_penalty_weight=5.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "        self.d_model = d_model\n",
    "        self.mask_dim = mask_dim\n",
    "        self.authenticity_penalty_weight = authenticity_penalty_weight\n",
    "\n",
    "        # Backbone + FPN\n",
    "        self.backbone = ConvNeXtFPNBackbone(out_channels=d_model, train_backbone=backbone_trainable)\n",
    "\n",
    "        # Transformer decoder\n",
    "        self.query_embed = nn.Embedding(num_queries, d_model)\n",
    "        self.transformer_decoder = SimpleTransformerDecoder(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_decoder_layers,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "\n",
    "        # Pixel decoder: project highest-res FPN level to mask feature space\n",
    "        self.mask_feature_proj = nn.Conv2d(d_model, mask_dim, kernel_size=1)\n",
    "\n",
    "        # Instance heads\n",
    "        self.class_head = nn.Linear(d_model, 1)  # forgery vs ignore, per query\n",
    "        self.mask_embed_head = nn.Linear(d_model, mask_dim)\n",
    "\n",
    "        # Image-level authenticity head (global pooled high-level feat)\n",
    "        self.img_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        \"\"\"\n",
    "        images: Tensor [B, 3, H, W]\n",
    "        targets: list[dict], each with:\n",
    "          - 'masks': [N_gt, H, W] binary mask tensor\n",
    "          - 'image_label': scalar tensor 0 (authentic) or 1 (forged)\n",
    "        Returns:\n",
    "          - if training: dict of losses\n",
    "          - if inference: list[dict] with masks, mask_scores, mask_forgery_scores, image_authenticity\n",
    "        \"\"\"\n",
    "        if isinstance(images, list):\n",
    "            images = torch.stack(images, dim=0)\n",
    "\n",
    "        B = images.shape[0]\n",
    "\n",
    "        # Backbone + FPN\n",
    "        fpn_feats = self.backbone(images)  # [P2, P3, P4, P5]\n",
    "        # Use highest-res level (P2) for mask features\n",
    "        mask_feats = self.mask_feature_proj(fpn_feats[0])  # [B, mask_dim, Hm, Wm]\n",
    "\n",
    "        # Transformer on multi-scale features\n",
    "        hs = self.transformer_decoder(self.query_embed.weight, fpn_feats)  # [B, Q, C]\n",
    "\n",
    "        # Heads\n",
    "        class_logits = self.class_head(hs).squeeze(-1)        # [B, Q]\n",
    "        mask_embeddings = self.mask_embed_head(hs)            # [B, Q, mask_dim]\n",
    "\n",
    "        # Produce mask logits via dot-product\n",
    "        # mask_feats: [B, mask_dim, Hm, Wm]; mask_emb: [B, Q, mask_dim]\n",
    "        mask_logits = torch.einsum(\"bqc, bchw -> bqhw\", mask_embeddings, mask_feats)\n",
    "\n",
    "        # Image-level authenticity from highest-level FPN feature (P5)\n",
    "        high_level_feat = fpn_feats[-1]  # [B, C, Hh, Wh]\n",
    "        img_logits = self.img_head(high_level_feat)  # [B, 1]\n",
    "        img_logits = img_logits.squeeze(-1)          # [B]\n",
    "\n",
    "        if targets is not None:\n",
    "            return self.compute_losses(mask_logits, class_logits, img_logits, targets)\n",
    "        else:\n",
    "            return self.inference(mask_logits, class_logits, img_logits)\n",
    "\n",
    "    # ------------------- Losses & matching -------------------\n",
    "\n",
    "    def compute_losses(self, mask_logits, class_logits, img_logits, targets):\n",
    "        \"\"\"\n",
    "        mask_logits: [B, Q, Hm, Wm]\n",
    "        class_logits: [B, Q]  (forgery vs ignore)\n",
    "        img_logits: [B]\n",
    "        targets: list[dict]\n",
    "        \"\"\"\n",
    "        B, Q, Hm, Wm = mask_logits.shape\n",
    "\n",
    "        # Hungarian matching per image\n",
    "        indices = []\n",
    "        for b in range(B):\n",
    "            tgt_masks = targets[b][\"masks\"]  # [N_gt, H, W] or [0, ...] if authentic\n",
    "            if tgt_masks.numel() == 0:\n",
    "                indices.append((torch.empty(0, dtype=torch.long, device=mask_logits.device),\n",
    "                                torch.empty(0, dtype=torch.long, device=mask_logits.device)))\n",
    "                continue\n",
    "\n",
    "            # Downsample GT masks to mask resolution\n",
    "            tgt_masks_resized = F.interpolate(\n",
    "                tgt_masks.unsqueeze(1).float(),\n",
    "                size=(Hm, Wm),\n",
    "                mode=\"nearest\"\n",
    "            ).squeeze(1)  # [N_gt, Hm, Wm]\n",
    "\n",
    "            pred = mask_logits[b]  # [Q, Hm, Wm]\n",
    "            cost = self.match_cost(pred, tgt_masks_resized)  # [N_gt, Q]\n",
    "\n",
    "            tgt_ind, pred_ind = linear_sum_assignment(cost.detach().cpu().numpy())\n",
    "            tgt_ind = torch.as_tensor(tgt_ind, dtype=torch.long, device=mask_logits.device)\n",
    "            pred_ind = torch.as_tensor(pred_ind, dtype=torch.long, device=mask_logits.device)\n",
    "            indices.append((pred_ind, tgt_ind))\n",
    "\n",
    "        # Mask losses (Dice + BCE) on matched pairs\n",
    "        loss_mask = 0.0\n",
    "        loss_dice = 0.0\n",
    "        num_instances = 0\n",
    "\n",
    "        for b in range(B):\n",
    "            pred_ind, tgt_ind = indices[b]\n",
    "            if len(pred_ind) == 0:\n",
    "                continue\n",
    "\n",
    "            tgt_masks = targets[b][\"masks\"]\n",
    "            tgt_masks_resized = F.interpolate(\n",
    "                tgt_masks.unsqueeze(1).float(),\n",
    "                size=(Hm, Wm),\n",
    "                mode=\"nearest\"\n",
    "            ).squeeze(1)  # [N_gt, Hm, Wm]\n",
    "\n",
    "            pred_masks = mask_logits[b, pred_ind]        # [M, Hm, Wm]\n",
    "            gt_masks = tgt_masks_resized[tgt_ind]        # [M, Hm, Wm]\n",
    "\n",
    "            loss_mask += self.sigmoid_ce_loss(pred_masks, gt_masks).sum()\n",
    "            loss_dice += self.dice_loss(pred_masks, gt_masks).sum()\n",
    "            num_instances += pred_ind.numel()\n",
    "\n",
    "        if num_instances > 0:\n",
    "            loss_mask = loss_mask / num_instances\n",
    "            loss_dice = loss_dice / num_instances\n",
    "        else:\n",
    "            loss_mask = mask_logits.sum() * 0.0\n",
    "            loss_dice = mask_logits.sum() * 0.0\n",
    "\n",
    "        # Mask-level classification BCE\n",
    "        # All GT instances are \"forgery\" (1). Unmatched predictions are ignore (0).\n",
    "        class_targets = torch.zeros_like(class_logits)  # [B, Q]\n",
    "        for b in range(B):\n",
    "            pred_ind, tgt_ind = indices[b]\n",
    "            if len(pred_ind) > 0:\n",
    "                class_targets[b, pred_ind] = 1.0\n",
    "\n",
    "        loss_cls = F.binary_cross_entropy_with_logits(\n",
    "            class_logits,\n",
    "            class_targets,\n",
    "        )\n",
    "\n",
    "        # Image-level authenticity loss\n",
    "        img_targets = torch.stack([t[\"image_label\"].float() for t in targets]).to(img_logits.device)  # [B]\n",
    "        loss_img = F.binary_cross_entropy_with_logits(img_logits, img_targets)\n",
    "\n",
    "        # Authenticity penalty: if authentic image (y=0) has non-empty predicted forgery mask\n",
    "        with torch.no_grad():\n",
    "            mask_probs = torch.sigmoid(mask_logits)        # [B, Q, Hm, Wm]\n",
    "            cls_probs = torch.sigmoid(class_logits)        # [B, Q]\n",
    "            # Take only masks with high forgery prob\n",
    "            forgery_mask = cls_probs > 0.5                 # [B, Q]\n",
    "\n",
    "        penalty = 0.0\n",
    "        for b in range(B):\n",
    "            if img_targets[b] < 0.5:  # authentic\n",
    "                if mask_logits.shape[1] == 0:\n",
    "                    continue\n",
    "                # Select predicted forgery masks\n",
    "                if forgery_mask[b].any():\n",
    "                    m = mask_probs[b, forgery_mask[b]]  # [K, Hm, Wm]\n",
    "                    # L1 norm as \"area\"\n",
    "                    penalty += m.mean()\n",
    "\n",
    "        if isinstance(penalty, float):\n",
    "            penalty = mask_logits.sum() * 0.0\n",
    "        loss_auth_penalty = self.authenticity_penalty_weight * penalty / max(B, 1)\n",
    "\n",
    "        losses = {\n",
    "            \"loss_mask_bce\": loss_mask,\n",
    "            \"loss_mask_dice\": loss_dice,\n",
    "            \"loss_mask_cls\": loss_cls,\n",
    "            \"loss_img_auth\": loss_img,\n",
    "            \"loss_auth_penalty\": loss_auth_penalty,\n",
    "        }\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def match_cost(self, pred_masks, tgt_masks):\n",
    "        \"\"\"\n",
    "        pred_masks: [Q, H, W]\n",
    "        tgt_masks: [N_gt, H, W]\n",
    "        Returns cost matrix [N_gt, Q]\n",
    "        \"\"\"\n",
    "        Q, H, W = pred_masks.shape\n",
    "        N = tgt_masks.shape[0]\n",
    "\n",
    "        pred_flat = pred_masks.flatten(1)  # [Q, HW]\n",
    "        tgt_flat = tgt_masks.flatten(1)    # [N, HW]\n",
    "\n",
    "        # BCE cost\n",
    "        pred_logits = pred_flat.unsqueeze(0)             # [1, Q, HW]\n",
    "        tgt = tgt_flat.unsqueeze(1)                      # [N, 1, HW]\n",
    "        bce = F.binary_cross_entropy_with_logits(\n",
    "            pred_logits.expand(N, -1, -1),\n",
    "            tgt.expand(-1, Q, -1),\n",
    "            reduction=\"none\",\n",
    "        ).mean(-1)  # [N, Q]\n",
    "\n",
    "        # Dice cost\n",
    "        pred_prob = pred_flat.sigmoid()\n",
    "        numerator = 2 * (pred_prob.unsqueeze(0) * tgt_flat.unsqueeze(1)).sum(-1)\n",
    "        denominator = pred_prob.unsqueeze(0).sum(-1) + tgt_flat.unsqueeze(1).sum(-1) + 1e-6\n",
    "        dice = 1.0 - (numerator + 1e-6) / (denominator)\n",
    "\n",
    "        cost = bce + dice\n",
    "        return cost\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_ce_loss(inputs, targets):\n",
    "        \"\"\"\n",
    "        BCE on logits, per-instance mean.\n",
    "        inputs: [M, H, W], targets: [M, H, W]\n",
    "        \"\"\"\n",
    "        return F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\").mean(dim=(1, 2))\n",
    "\n",
    "    @staticmethod\n",
    "    def dice_loss(inputs, targets, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Soft dice loss on logits.\n",
    "        inputs: [M, H, W], targets: [M, H, W]\n",
    "        \"\"\"\n",
    "        inputs = inputs.sigmoid()\n",
    "        inputs = inputs.flatten(1)\n",
    "        targets = targets.flatten(1)\n",
    "\n",
    "        numerator = 2 * (inputs * targets).sum(1)\n",
    "        denominator = inputs.sum(1) + targets.sum(1) + eps\n",
    "        loss = 1 - (numerator + eps) / (denominator)\n",
    "        return loss\n",
    "\n",
    "    # ------------------- Inference -------------------\n",
    "\n",
    "    def inference(self, mask_logits, class_logits, img_logits, mask_threshold=0.5, cls_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Returns list of dicts per image:\n",
    "          - 'masks': [K, Hm, Wm] uint8\n",
    "          - 'mask_scores': [K]\n",
    "          - 'mask_forgery_scores': [K]\n",
    "          - 'image_authenticity': float in [0,1], prob of \"forged\"\n",
    "        Authenticity gate: if prob(authentic) > 0.5, masks list is empty.\n",
    "        \"\"\"\n",
    "        B, Q, Hm, Wm = mask_logits.shape\n",
    "        mask_probs = torch.sigmoid(mask_logits)\n",
    "        cls_probs = torch.sigmoid(class_logits)\n",
    "        img_probs = torch.sigmoid(img_logits)  # prob \"forged\"\n",
    "\n",
    "        outputs = []\n",
    "        for b in range(B):\n",
    "            forged_prob = img_probs[b].item()\n",
    "            authentic_prob = 1.0 - forged_prob\n",
    "\n",
    "            if authentic_prob > 0.5:\n",
    "                # Authenticity gate: suppress all masks\n",
    "                outputs.append({\n",
    "                    \"masks\": torch.zeros((0, Hm, Wm), dtype=torch.uint8, device=mask_logits.device),\n",
    "                    \"mask_scores\": torch.empty(0, device=mask_logits.device),\n",
    "                    \"mask_forgery_scores\": torch.empty(0, device=mask_logits.device),\n",
    "                    \"image_authenticity\": forged_prob,\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Keep masks with high forgery prob\n",
    "            keep = cls_probs[b] > cls_threshold\n",
    "            if keep.sum() == 0:\n",
    "                outputs.append({\n",
    "                    \"masks\": torch.zeros((0, Hm, Wm), dtype=torch.uint8, device=mask_logits.device),\n",
    "                    \"mask_scores\": torch.empty(0, device=mask_logits.device),\n",
    "                    \"mask_forgery_scores\": torch.empty(0, device=mask_logits.device),\n",
    "                    \"image_authenticity\": forged_prob,\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            masks_b = (mask_probs[b, keep] > mask_threshold).to(torch.uint8)  # [K, Hm, Wm]\n",
    "            scores_b = mask_probs[b, keep].flatten(1).mean(-1)                # avg mask prob\n",
    "            cls_b = cls_probs[b, keep]\n",
    "\n",
    "            outputs.append({\n",
    "                \"masks\": masks_b,\n",
    "                \"mask_scores\": scores_b,\n",
    "                \"mask_forgery_scores\": cls_b,\n",
    "                \"image_authenticity\": forged_prob,\n",
    "            })\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# ------------------- Usage with your existing pipeline -------------------\n",
    "# Assuming:\n",
    "#   - You already have `device` from the Mask R-CNN notebook.\n",
    "#   - Your DataLoader still returns (images, targets) where:\n",
    "#       targets[i]['masks'] is [N_i, H, W] binary masks\n",
    "#       You ADD targets[i]['image_label'] = 0 (authentic) or 1 (forged)\n",
    "#\n",
    "# Example:\n",
    "#\n",
    "model = Mask2FormerForgeryModel(num_queries=15, d_model=256).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "for images, targets in train_loader:\n",
    "    images = [img.to(device) for img in images]\n",
    "    for t in targets:\n",
    "        t['masks'] = t['masks'].to(device)\n",
    "        t['image_label'] = t['image_label'].to(device)\n",
    "\n",
    "    loss_dict = model(images, targets)\n",
    "    loss = sum(loss_dict.values())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    \"\"\"\n",
    "    Encode binary mask to RLE in the format required by the competition.\n",
    "    Returns a JSON string like \"[123,4,567,8]\"\n",
    "    \"\"\"\n",
    "    # Ensure mask is 2D and binary\n",
    "    mask = mask.astype(bool)\n",
    "    \n",
    "    # Flatten in Fortran order\n",
    "    flat = mask.T.flatten()\n",
    "    \n",
    "    # Find indices where value is True\n",
    "    dots = np.where(flat)[0]\n",
    "    \n",
    "    if len(dots) == 0:\n",
    "        return json.dumps([])  # or just return 'authentic' upstream\n",
    "    \n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend([b + 1, 0])  # 1-based index\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    \n",
    "    # Convert numpy ints to Python ints for JSON compatibility\n",
    "    run_lengths = [int(x) for x in run_lengths]\n",
    "    return json.dumps(run_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482685c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_images(model, test_path, device):\n",
    "    model.eval()\n",
    "    predictions = {}\n",
    "    \n",
    "    test_files = sorted(os.listdir(test_path))\n",
    "    \n",
    "    transform = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    for file in tqdm(test_files, desc=\"Processing test images\"):\n",
    "        case_id = file.split('.')[0]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img_path = os.path.join(test_path, file)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "        original_h, original_w = image_np.shape[:2]\n",
    "        \n",
    "        transformed = transform(image=image_np)\n",
    "        image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Model prediction (our Mask2Former-style model)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_tensor)   # list of length 1\n",
    "        out = outputs[0]\n",
    "        \n",
    "        masks = out[\"masks\"]              # [K, Hm, Wm], uint8 0/1\n",
    "        # Authenticity gate is already applied inside the model:\n",
    "        #   - if image predicted authentic -> masks.shape[0] == 0\n",
    "        \n",
    "        if masks.shape[0] == 0:\n",
    "            # Authentic image ⇒ \"authentic\" string for submission\n",
    "            predictions[case_id] = \"authentic\"\n",
    "            continue\n",
    "        \n",
    "        # Combine instance masks into a single binary mask\n",
    "        combined_mask = (masks.sum(dim=0) > 0).to(torch.uint8)  # [Hm, Wm]\n",
    "        \n",
    "        # Resize back to original size\n",
    "        combined_mask_np = combined_mask.cpu().numpy().astype(np.uint8)\n",
    "        combined_mask_resized = cv2.resize(\n",
    "            combined_mask_np,\n",
    "            (original_w, original_h),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "        \n",
    "        # RLE encoding for competition\n",
    "        if combined_mask_resized.sum() == 0:\n",
    "            predictions[case_id] = \"authentic\"\n",
    "        else:\n",
    "            rle_json = rle_encode_competition(combined_mask_resized)\n",
    "            predictions[case_id] = rle_json\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# This line stays the same:\n",
    "predictions = predict_test_images(model, paths['test_images'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc003c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the sample submission for the correct order\n",
    "sample_submission = pd.read_csv('/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv')\n",
    "    \n",
    "# Create DataFrame with predictions\n",
    "submission_data = []\n",
    "for case_id in sample_submission['case_id']:\n",
    "    case_id_str = str(case_id)\n",
    "    if case_id_str in predictions:\n",
    "        submission_data.append({'case_id': case_id, 'annotation': predictions[case_id_str]})\n",
    "    else:\n",
    "        # If case_id not in predictions, use authentic as default\n",
    "        submission_data.append({'case_id': case_id, 'annotation': 'authentic'})\n",
    "    \n",
    "submission = pd.DataFrame(submission_data)\n",
    "    \n",
    "# Save submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "# Prediction statistics\n",
    "authentic_count = (submission['annotation'] == 'authentic').sum()\n",
    "forged_count = len(submission) - authentic_count\n",
    "\n",
    "print(f\"Prediction Statistics:\")\n",
    "print(f\"Authentic: {authentic_count}\")\n",
    "print(f\"Forged: {forged_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# We take the first file from the test folder\n",
    "test_files = sorted(os.listdir(paths['test_images']))\n",
    "file = test_files[0]\n",
    "img_path = os.path.join(paths['test_images'], file)\n",
    "\n",
    "# Uploading an image\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_np = np.array(image)\n",
    "\n",
    "# Transformations\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Apply transformations and make predictions\n",
    "transformed = transform(image=image_np)\n",
    "image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(image_tensor)\n",
    "\n",
    "masks = prediction[0]['masks']\n",
    "scores = prediction[0]['scores']\n",
    "confidence_threshold = 0.5\n",
    "valid_detections = scores > confidence_threshold\n",
    "\n",
    "# Creating a shape: original on the left, mask on the right\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(image_np)\n",
    "axes[0].set_title(f'Original: {file}', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Mask\n",
    "if valid_detections.sum() == 0:\n",
    "    combined_mask = np.zeros((256, 256))\n",
    "    title = 'Predicted: Authentic'\n",
    "    cmap = 'gray'\n",
    "else:\n",
    "    combined_mask = np.zeros((256, 256), dtype=np.float32)\n",
    "    for idx in range(len(masks)):\n",
    "        if valid_detections[idx]:\n",
    "            mask = masks[idx, 0] > 0.5\n",
    "            combined_mask = np.maximum(combined_mask, mask.cpu().numpy().astype(np.float32))\n",
    "    \n",
    "    title = f'Predicted: Forged ({valid_detections.sum()} regions)'\n",
    "    cmap = 'hot'\n",
    "\n",
    "im = axes[1].imshow(combined_mask, cmap=cmap, vmin=0, vmax=1)\n",
    "axes[1].set_title(title, fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "if valid_detections.sum() > 0:\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/kaggle/working/submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final_mask2former_model.pth')\n",
    "print(\"The final model is saved as: 'final_mask2former_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
