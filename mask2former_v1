{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113558,"databundleVersionId":14456136,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport json\nimport sympy\nimport sympy.printing  # ensure submodule is imported\nsympy.printing = sympy.printing  # attach as attribute explicitly\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights\nfrom torchvision.ops import FeaturePyramidNetwork\n\nfrom scipy.optimize import linear_sum_assignment\n\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection import MaskRCNN\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.transforms import functional as F_transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:37:42.316819Z","iopub.execute_input":"2025-12-09T23:37:42.316998Z","iopub.status.idle":"2025-12-09T23:38:26.066326Z","shell.execute_reply.started":"2025-12-09T23:37:42.316978Z","shell.execute_reply":"2025-12-09T23:38:26.065446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking GPU availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:26.068084Z","iopub.execute_input":"2025-12-09T23:38:26.068463Z","iopub.status.idle":"2025-12-09T23:38:26.167904Z","shell.execute_reply.started":"2025-12-09T23:38:26.068444Z","shell.execute_reply":"2025-12-09T23:38:26.167268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_data_structure():\n    base_path = '/kaggle/input/recodai-luc-scientific-image-forgery-detection'\n    \n    # Checking train images\n    train_authentic_path = os.path.join(base_path, 'train_images/authentic')\n    train_forged_path = os.path.join(base_path, 'train_images/forged')\n    train_masks_path = os.path.join(base_path, 'train_masks')\n    test_images_path = os.path.join(base_path, 'test_images')\n    supp_forged_path = os.path.join(base_path, 'supplemental_images')\n    supp_masks_path = os.path.join(base_path, 'supplemental_masks')\n    \n    print(f\"Authentic images: {len(os.listdir(train_authentic_path))}\")\n    print(f\"Forged images: {len(os.listdir(train_forged_path))}\")\n    print(f\"Masks: {len(os.listdir(train_masks_path))}\")\n    print(f\"Test images: {len(os.listdir(test_images_path))}\")\n\n    print(f\"Supp forged images: {len(os.listdir(supp_forged_path))}\")\n    print(f\"Supp masks: {len(os.listdir(supp_masks_path))}\")\n    \n    # Let's analyze some examples of masks\n    mask_files = os.listdir(train_masks_path)[:5]\n    print(f\"Examples of mask files: {mask_files}\")\n    \n    # Checking the mask format\n    sample_mask = np.load(os.path.join(train_masks_path, mask_files[0]))\n    print(f\"Mask format: {sample_mask.shape}, dtype: {sample_mask.dtype}\")\n    \n    test_files = os.listdir(test_images_path)\n    print(f\"Test images: {test_files}\")\n    \n    return {\n        'train_authentic': train_authentic_path,\n        'train_forged': train_forged_path,\n        'train_masks': train_masks_path,\n        'test_images': test_images_path,\n        'supp_forged': supp_forged_path,\n        'supp_masks' : supp_masks_path\n    }\n\npaths = analyze_data_structure()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:26.168599Z","iopub.execute_input":"2025-12-09T23:38:26.16888Z","iopub.status.idle":"2025-12-09T23:38:26.263014Z","shell.execute_reply.started":"2025-12-09T23:38:26.168862Z","shell.execute_reply":"2025-12-09T23:38:26.262339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ForgeryDataset(Dataset):\n    def __init__(\n        self,\n        authentic_path,\n        forged_path,\n        masks_path,\n        supp_forged_path=None,\n        supp_masks_path=None,\n        transform=None,\n        is_train=True,\n    ):\n        self.transform = transform\n        self.is_train = is_train\n        \n        # Collect all data samples\n        self.samples = []\n        \n        # Authentic images\n        for file in os.listdir(authentic_path):\n            img_path = os.path.join(authentic_path, file)\n            base_name = file.split('.')[0]\n            mask_path = os.path.join(masks_path, f\"{base_name}.npy\")\n            \n            self.samples.append({\n                'image_path': img_path,\n                'mask_path': mask_path,\n                'is_forged': False,\n                'image_id': base_name\n            })\n        \n        # Forged images (original)\n        for file in os.listdir(forged_path):\n            img_path = os.path.join(forged_path, file)\n            base_name = file.split('.')[0]\n            mask_path = os.path.join(masks_path, f\"{base_name}.npy\")\n            \n            self.samples.append({\n                'image_path': img_path,\n                'mask_path': mask_path,\n                'is_forged': True,\n                'image_id': base_name\n            })\n\n        # Supplemental forged images (all forged)\n        if supp_forged_path is not None and supp_masks_path is not None:\n            for file in os.listdir(supp_forged_path):\n                img_path = os.path.join(supp_forged_path, file)\n                base_name = file.split('.')[0]\n                mask_path = os.path.join(supp_masks_path, f\"{base_name}.npy\")\n                \n                self.samples.append({\n                    'image_path': img_path,\n                    'mask_path': mask_path,\n                    'is_forged': True,\n                    'image_id': base_name\n                })\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        \n        # Load image\n        image = Image.open(sample['image_path']).convert('RGB')\n        image = np.array(image)  # (H, W, 3)\n        \n        # Load and process mask\n        if os.path.exists(sample['mask_path']):\n            mask = np.load(sample['mask_path'])\n            \n            # Handle multi-channel masks: MUST be channel-first (C, H, W)\n            if mask.ndim == 3:\n                # Expect small channel dimension as first dim\n                if mask.shape[0] <= 10 and mask.shape[1:] == image.shape[:2]:\n                    mask = np.any(mask, axis=0)\n                else:\n                    raise ValueError(\n                        f\"Expected channel-first mask (C, H, W) with small C. Got {mask.shape}\"\n                    )\n            \n            mask = (mask > 0).astype(np.uint8)\n        else:\n            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n    \n        # Shape validation\n        assert image.shape[:2] == mask.shape, f\"Shape mismatch: img {image.shape}, mask {mask.shape}\"\n        \n        # Apply transformations\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed['image']\n            mask = transformed['mask']\n        else:\n            image = F_transforms.to_tensor(image)\n            mask = torch.tensor(mask, dtype=torch.uint8)\n        \n        # Prepare targets for Mask2Former\n        if sample['is_forged'] and mask.sum() > 0:\n            boxes, labels, masks = self.mask_to_boxes(mask)\n            \n            target = {\n                'boxes': boxes,\n                'labels': labels,\n                'masks': masks,\n                'image_id': torch.tensor([idx]),\n                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n                'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n            }\n            target['image_label'] = torch.tensor(1.0)   # forged\n        else:\n            # For authentic images or images without masks\n            target = {\n                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n                'labels': torch.zeros(0, dtype=torch.int64),\n                'masks': torch.zeros((0, image.shape[1], image.shape[2]), dtype=torch.uint8),\n                'image_id': torch.tensor([idx]),\n                'area': torch.zeros(0, dtype=torch.float32),\n                'iscrowd': torch.zeros((0,), dtype=torch.int64)\n            }\n            target['image_label'] = torch.tensor(0.0)   # authentic\n        return image, target\n    \n    def mask_to_boxes(self, mask):\n        \"\"\"Convert segmentation mask to bounding boxes for Mask2Former\"\"\"\n        if isinstance(mask, torch.Tensor):\n            mask_np = mask.numpy()\n        else:\n            mask_np = mask\n        \n        # Find contours in the mask\n        contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        boxes = []\n        masks = []\n        \n        for contour in contours:\n            if len(contour) > 0:\n                x, y, w, h = cv2.boundingRect(contour)\n                # Filter out very small regions\n                if w > 5 and h > 5:\n                    boxes.append([x, y, x + w, y + h])\n                    # Create binary mask for this contour\n                    contour_mask = np.zeros_like(mask_np)\n                    cv2.fillPoly(contour_mask, [contour], 1)\n                    masks.append(contour_mask)\n        \n        if boxes:\n            boxes = torch.tensor(boxes, dtype=torch.float32)\n            labels = torch.ones((len(boxes),), dtype=torch.int64)\n            masks = torch.tensor(np.array(masks), dtype=torch.uint8)\n        else:\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels = torch.zeros(0, dtype=torch.int64)\n            masks = torch.zeros((0, mask_np.shape[0], mask_np.shape[1]), dtype=torch.uint8)\n        \n        return boxes, labels, masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:26.263689Z","iopub.execute_input":"2025-12-09T23:38:26.263857Z","iopub.status.idle":"2025-12-09T23:38:26.283939Z","shell.execute_reply.started":"2025-12-09T23:38:26.263843Z","shell.execute_reply":"2025-12-09T23:38:26.283411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transformations for learning, ImageNet standards\ntrain_transform = A.Compose([\n    A.Resize(256, 256),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Resize(256, 256),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\nfull_dataset = ForgeryDataset(\n    paths['train_authentic'],\n    paths['train_forged'],\n    paths['train_masks'],\n    supp_forged_path=paths['supp_forged'],\n    supp_masks_path=paths['supp_masks'],\n    transform=train_transform\n)\n\n# Split into train/val\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n\n# Use val transforms for validation subset\nval_dataset.dataset.transform = val_transform\n\n# Dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples: {len(val_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:26.284668Z","iopub.execute_input":"2025-12-09T23:38:26.28522Z","iopub.status.idle":"2025-12-09T23:38:26.315879Z","shell.execute_reply.started":"2025-12-09T23:38:26.2852Z","shell.execute_reply":"2025-12-09T23:38:26.315119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConvNeXtFPNBackbone(nn.Module):\n    \"\"\"\n    ConvNeXt-T backbone with FPN neck.\n    Uses outputs from indices [1, 3, 5, 7], which have channels\n    [96, 192, 384, 768] according to your debug print.\n    \"\"\"\n    def __init__(self, out_channels=256, train_backbone=True):\n        super().__init__()\n        try:\n            backbone = convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n        except Exception:\n            backbone = convnext_tiny(weights=None)\n\n        self.body = backbone.features  # modules 0..7\n\n        # Use the last block of each resolution stage:\n        #  1: 96 @ 64×64\n        #  3: 192 @ 32×32\n        #  5: 384 @ 16×16\n        #  7: 768 @  8×8\n        self.out_indices = (1, 3, 5, 7)\n\n        in_channels_list = [96, 192, 384, 768]\n        self.fpn = FeaturePyramidNetwork(\n            in_channels_list=in_channels_list,\n            out_channels=out_channels,\n        )\n\n        if not train_backbone:\n            for p in self.body.parameters():\n                p.requires_grad = False\n\n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.body):\n            out = layer(out)\n            if i in self.out_indices:\n                feats.append(out)\n\n        # feats is a list of 4 tensors with channels [96, 192, 384, 768]\n        feat_dict = {str(i): f for i, f in enumerate(feats)}  # keys \"0\",\"1\",\"2\",\"3\"\n        fpn_out = self.fpn(feat_dict)  # dict of {level_name: [B, C, H, W]}\n\n        # Sort by level name to keep consistent order\n        levels = [fpn_out[k] for k in sorted(fpn_out.keys(), key=int)]\n        return levels  # [P2, P3, P4, P5], all with C=out_channels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:26.317471Z","iopub.execute_input":"2025-12-09T23:38:26.318049Z","iopub.status.idle":"2025-12-09T23:38:26.324245Z","shell.execute_reply.started":"2025-12-09T23:38:26.318023Z","shell.execute_reply":"2025-12-09T23:38:26.323569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"backbone_fpn = ConvNeXtFPNBackbone(out_channels=256, train_backbone=False).eval()\n\nx = torch.randn(1, 3, 256, 256)\n\nwith torch.no_grad():\n    levels = backbone_fpn(x)\n    print(\"=== FPN levels ===\")\n    for i, f in enumerate(levels):\n        print(f\"Level {i}: {tuple(f.shape)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:26.326773Z","iopub.execute_input":"2025-12-09T23:38:26.32705Z","iopub.status.idle":"2025-12-09T23:38:59.579293Z","shell.execute_reply.started":"2025-12-09T23:38:26.327033Z","shell.execute_reply":"2025-12-09T23:38:59.578427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DetrTransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n\n        # FFN\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n        self.dropout_ffn = nn.Dropout(dropout)\n\n        if activation == \"relu\":\n            self.activation = F.relu\n        elif activation == \"gelu\":\n            self.activation = F.gelu\n        else:\n            raise ValueError(f\"Unsupported activation: {activation}\")\n\n    def with_pos(self, x, pos):\n        if pos is None:\n            return x\n        return x + pos\n\n    def forward(\n        self,\n        tgt,                      # [Q, B, C]\n        memory,                   # [S, B, C]\n        tgt_pos=None,             # [Q, B, C] or None\n        memory_pos=None,          # [S, B, C] or None\n        tgt_mask=None,\n        memory_mask=None,\n        tgt_key_padding_mask=None,\n        memory_key_padding_mask=None,\n    ):\n        # ---- Self-attention (queries attend to themselves) ----\n        q = k = self.with_pos(tgt, tgt_pos)\n        tgt2, _ = self.self_attn(\n            q, k, value=tgt,\n            attn_mask=tgt_mask,\n            key_padding_mask=tgt_key_padding_mask,\n        )\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n\n        # ---- Cross-attention (queries attend to encoder memory) ----\n        q = self.with_pos(tgt, tgt_pos)\n        k = self.with_pos(memory, memory_pos)\n        tgt2, _ = self.multihead_attn(\n            q, k, value=memory,\n            attn_mask=memory_mask,\n            key_padding_mask=memory_key_padding_mask,\n        )\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n\n        # ---- Feed-forward ----\n        tgt2 = self.linear2(self.dropout_ffn(self.activation(self.linear1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n\n        return tgt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:59.580173Z","iopub.execute_input":"2025-12-09T23:38:59.580497Z","iopub.status.idle":"2025-12-09T23:38:59.588829Z","shell.execute_reply.started":"2025-12-09T23:38:59.58047Z","shell.execute_reply":"2025-12-09T23:38:59.588158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DetrTransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()\n        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n        self.num_layers = num_layers\n        self.norm = norm\n        self.return_intermediate = return_intermediate\n\n    def forward(\n        self,\n        tgt,                      # [Q, B, C]\n        memory,                   # [S, B, C]\n        tgt_pos=None,             # [Q, B, C] or None\n        memory_pos=None,          # [S, B, C] or None\n        tgt_mask=None,\n        memory_mask=None,\n        tgt_key_padding_mask=None,\n        memory_key_padding_mask=None,\n    ):\n        output = tgt\n        intermediate = []\n\n        for layer in self.layers:\n            output = layer(\n                output,\n                memory,\n                tgt_pos=tgt_pos,\n                memory_pos=memory_pos,\n                tgt_mask=tgt_mask,\n                memory_mask=memory_mask,\n                tgt_key_padding_mask=tgt_key_padding_mask,\n                memory_key_padding_mask=memory_key_padding_mask,\n            )\n            if self.return_intermediate:\n                intermediate.append(output)\n\n        if self.norm is not None:\n            output = self.norm(output)\n            if self.return_intermediate:\n                intermediate[-1] = output\n\n        if self.return_intermediate:\n            # [num_layers, Q, B, C]\n            return torch.stack(intermediate)\n\n        # [1, Q, B, C] to match DETR’s interface\n        return output.unsqueeze(0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:59.589779Z","iopub.execute_input":"2025-12-09T23:38:59.590163Z","iopub.status.idle":"2025-12-09T23:38:59.608837Z","shell.execute_reply.started":"2025-12-09T23:38:59.590137Z","shell.execute_reply":"2025-12-09T23:38:59.60807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimpleTransformerDecoder(nn.Module):\n    def __init__(\n        self,\n        d_model=256,\n        nhead=8,\n        num_layers=6,\n        dim_feedforward=2048,\n        dropout=0.1,\n        num_queries=100,\n    ):\n        super().__init__()\n        self.num_queries = num_queries\n\n        # learned object queries (like DETR)\n        self.query_embed = nn.Embedding(num_queries, d_model)\n\n        layer = DetrTransformerDecoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n        )\n        self.decoder = DetrTransformerDecoder(\n            decoder_layer=layer,\n            num_layers=num_layers,\n            norm=nn.LayerNorm(d_model),\n            return_intermediate=True,\n        )\n\n    def forward(self, feats, pos_list):\n        \"\"\"\n        feats:    list of [B, C, H, W]\n        pos_list: list of [B, C, H, W] (positional encodings for each feat)\n        \"\"\"\n        srcs = []\n        pos_embs = []\n        for feat, pos in zip(feats, pos_list):\n            # [B, C, H, W] -> [HW, B, C]\n            srcs.append(feat.flatten(2).permute(2, 0, 1))\n            pos_embs.append(pos.flatten(2).permute(2, 0, 1))\n\n        memory = torch.cat(srcs, dim=0)        # [S, B, C]\n        memory_pos = torch.cat(pos_embs, dim=0)  # [S, B, C]\n\n        B = memory.size(1)\n        # [Q, B, C] query positions\n        query_pos = self.query_embed.weight.unsqueeze(1).repeat(1, B, 1)\n        tgt = torch.zeros_like(query_pos)  # initial target is zeros; queries in query_pos\n\n        hs = self.decoder(\n            tgt=tgt,\n            memory=memory,\n            tgt_pos=query_pos,\n            memory_pos=memory_pos,\n        )\n        # [num_layers, Q, B, C] -> [num_layers, B, Q, C]\n        hs = hs.permute(0, 2, 1, 3)\n        return hs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:59.609754Z","iopub.execute_input":"2025-12-09T23:38:59.609969Z","iopub.status.idle":"2025-12-09T23:38:59.624026Z","shell.execute_reply.started":"2025-12-09T23:38:59.609947Z","shell.execute_reply":"2025-12-09T23:38:59.623392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionEmbeddingSine(nn.Module):\n    \"\"\"\n    Standard sine-cosine positional encoding as in DETR.\n    \"\"\"\n    def __init__(self, num_pos_feats=128, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        self.scale = 2 * torch.pi if scale is None else scale\n\n    def forward(self, x):\n        # x: [B, C, H, W]\n        B, C, H, W = x.shape\n        device = x.device\n\n        y_embed = torch.arange(H, device=device).unsqueeze(1).repeat(1, W)\n        x_embed = torch.arange(W, device=device).unsqueeze(0).repeat(H, 1)\n\n        y_embed = y_embed.float()\n        x_embed = x_embed.float()\n\n        if self.normalize:\n            eps = 1e-6\n            y_embed = y_embed / (H - 1 + eps) * self.scale\n            x_embed = x_embed / (W - 1 + eps) * self.scale\n\n        dim_t = torch.arange(self.num_pos_feats, device=device).float()\n        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n\n        pos_x = x_embed[..., None] / dim_t\n        pos_y = y_embed[..., None] / dim_t\n        pos_x = torch.stack([pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()], dim=-1).flatten(-2)\n        pos_y = torch.stack([pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()], dim=-1).flatten(-2)\n\n        pos = torch.cat([pos_y, pos_x], dim=-1)  # [H, W, 2*num_pos_feats]\n        pos = pos.permute(2, 0, 1).unsqueeze(0).expand(B, -1, -1, -1)  # [B, C, H, W]\n        return pos\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:59.624647Z","iopub.execute_input":"2025-12-09T23:38:59.624885Z","iopub.status.idle":"2025-12-09T23:38:59.643614Z","shell.execute_reply.started":"2025-12-09T23:38:59.624852Z","shell.execute_reply":"2025-12-09T23:38:59.642927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate ConvNeXt-Tiny exactly as in the Mask2Former model\ntry:\n    backbone = convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\nexcept Exception:\n    backbone = convnext_tiny(weights=None)\n\nbackbone.eval()\n\n# Dummy input\nx = torch.randn(1, 3, 256, 256)\n\nwith torch.no_grad():\n    out = x\n    print(\"Input:\", x.shape)\n    for i, layer in enumerate(backbone.features):\n        out = layer(out)\n        print(f\"features[{i}] output:\", out.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:38:59.644406Z","iopub.execute_input":"2025-12-09T23:38:59.644673Z","iopub.status.idle":"2025-12-09T23:39:32.653044Z","shell.execute_reply.started":"2025-12-09T23:38:59.64465Z","shell.execute_reply":"2025-12-09T23:39:32.652161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"backbone_fpn = ConvNeXtFPNBackbone(out_channels=256, train_backbone=False).eval()\n\nx = torch.randn(1, 3, 256, 256)\n\nwith torch.no_grad():\n    feats = []\n    out = x\n    print(\"=== Debug ConvNeXtFPNBackbone ===\")\n    for i, layer in enumerate(backbone_fpn.body):\n        out = layer(out)\n        print(f\"body[{i}] output:\", out.shape)\n    print(\"---- FPN inputs (as dict keys / shapes) ----\")\n    # Re-run forward like in the class, but instrumented\n    out = x\n    feat_list = []\n    for i, layer in enumerate(backbone_fpn.body):\n        out = layer(out)\n        if i in backbone_fpn.out_indices:\n            feat_list.append((i, out))\n    for idx, feat in feat_list:\n        print(f\"Selected feature index {idx} -> {feat.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:39:32.653925Z","iopub.execute_input":"2025-12-09T23:39:32.654463Z","iopub.status.idle":"2025-12-09T23:40:05.747335Z","shell.execute_reply.started":"2025-12-09T23:39:32.654437Z","shell.execute_reply":"2025-12-09T23:40:05.746518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    feat_dict = {str(i): f for i, (_, f) in enumerate(feat_list)}\n    fpn_out = backbone_fpn.fpn(feat_dict)\n    print(\"---- FPN outputs ----\")\n    for k in sorted(fpn_out.keys(), key=int):\n        print(f\"FPN level {k} -> {fpn_out[k].shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:40:05.748144Z","iopub.execute_input":"2025-12-09T23:40:05.748643Z","iopub.status.idle":"2025-12-09T23:40:05.787953Z","shell.execute_reply.started":"2025-12-09T23:40:05.748616Z","shell.execute_reply":"2025-12-09T23:40:05.787263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mask2FormerForgeryModel(nn.Module):\n    \"\"\"\n    Instance-Seg Transformer (Mask2Former-style) + Authenticity Gate baseline.\n    \"\"\"\n    def __init__(\n        self,\n        num_queries=15,\n        d_model=256,\n        nhead=8,\n        num_decoder_layers=6,\n        mask_dim=256,\n        backbone_trainable=True,\n        authenticity_penalty_weight=5.0,\n    ):\n        super().__init__()\n        self.num_queries = num_queries\n        self.d_model = d_model\n        self.mask_dim = mask_dim\n        self.authenticity_penalty_weight = authenticity_penalty_weight\n\n        # Backbone + FPN\n        self.backbone = ConvNeXtFPNBackbone(out_channels=d_model, train_backbone=backbone_trainable)\n\n        # Transformer decoder\n        self.position_encoding = PositionEmbeddingSine(d_model // 2, normalize=True)\n        self.transformer_decoder = SimpleTransformerDecoder(\n            d_model=256,\n            nhead=8,\n            num_layers=6,\n            dim_feedforward=2048,\n            dropout=0.1,\n            num_queries=num_queries,\n        )\n\n        # Pixel decoder: project highest-res FPN level to mask feature space\n        self.mask_feature_proj = nn.Conv2d(d_model, mask_dim, kernel_size=1)\n\n        # Instance heads\n        self.class_head = nn.Linear(d_model, 1)  # forgery vs ignore, per query\n        self.mask_embed_head = nn.Linear(d_model, mask_dim)\n\n        # Image-level authenticity head (global pooled high-level feat)\n        self.img_head = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(d_model, d_model),\n            nn.ReLU(inplace=True),\n            nn.Linear(d_model, 1),\n        )\n\n    def forward(self, images, targets=None):\n        \"\"\"\n        images: Tensor [B, 3, H, W]\n        targets: list[dict], each with:\n          - 'masks': [N_gt, H, W] binary mask tensor\n          - 'image_label': scalar tensor 0 (authentic) or 1 (forged)\n        Returns:\n          - if training: dict of losses\n          - if inference: list[dict] with masks, mask_scores, mask_forgery_scores, image_authenticity\n        \"\"\"\n        if isinstance(images, list):\n            images = torch.stack(images, dim=0)\n\n        B = images.shape[0]\n\n        # Backbone + FPN\n        fpn_feats = self.backbone(images)  # [P2, P3, P4, P5]\n        # Use highest-res level (P2) for mask features\n        mask_feats = self.mask_feature_proj(fpn_feats[0])  # [B, mask_dim, Hm, Wm]\n        pos_list = [self.position_encoding(x) for x in fpn_feats]\n\n        # Transformer on multi-scale features\n        hs_all = self.transformer_decoder(fpn_feats, pos_list)  # [num_layers, B, Q, C]\n        hs = hs_all[-1]                                         # last layer: [B, Q, C]\n\n        # Heads\n        class_logits    = self.class_head(hs).squeeze(-1)       # [B, Q]\n        mask_embeddings = self.mask_embed_head(hs)              # [B, Q, mask_dim]\n\n\n        # Produce mask logits via dot-product\n        # mask_feats: [B, mask_dim, Hm, Wm]; mask_emb: [B, Q, mask_dim]\n        mask_logits = torch.einsum(\"bqc, bchw -> bqhw\", mask_embeddings, mask_feats)\n\n        # Image-level authenticity from highest-level FPN feature (P5)\n        high_level_feat = fpn_feats[-1]  # [B, C, Hh, Wh]\n        img_logits = self.img_head(high_level_feat)  # [B, 1]\n        img_logits = img_logits.squeeze(-1)          # [B]\n\n        if targets is not None:\n            return self.compute_losses(mask_logits, class_logits, img_logits, targets)\n        else:\n            return self.inference(mask_logits, class_logits, img_logits)\n\n    # ------------------- Losses & matching -------------------\n\n    def compute_losses(self, mask_logits, class_logits, img_logits, targets):\n        \"\"\"\n        mask_logits: [B, Q, Hm, Wm]\n        class_logits: [B, Q]  (forgery vs ignore)\n        img_logits: [B]\n        targets: list[dict]\n        \"\"\"\n        B, Q, Hm, Wm = mask_logits.shape\n\n        # Hungarian matching per image\n        indices = []\n        for b in range(B):\n            tgt_masks = targets[b][\"masks\"]  # [N_gt, H, W] or [0, ...] if authentic\n            if tgt_masks.numel() == 0:\n                indices.append((torch.empty(0, dtype=torch.long, device=mask_logits.device),\n                                torch.empty(0, dtype=torch.long, device=mask_logits.device)))\n                continue\n\n            # Downsample GT masks to mask resolution\n            tgt_masks_resized = F.interpolate(\n                tgt_masks.unsqueeze(1).float(),\n                size=(Hm, Wm),\n                mode=\"nearest\"\n            ).squeeze(1)  # [N_gt, Hm, Wm]\n\n            pred = mask_logits[b]  # [Q, Hm, Wm]\n            cost = self.match_cost(pred, tgt_masks_resized)  # [N_gt, Q]\n\n            tgt_ind, pred_ind = linear_sum_assignment(cost.detach().cpu().numpy())\n            tgt_ind = torch.as_tensor(tgt_ind, dtype=torch.long, device=mask_logits.device)\n            pred_ind = torch.as_tensor(pred_ind, dtype=torch.long, device=mask_logits.device)\n            indices.append((pred_ind, tgt_ind))\n\n        # Mask losses (Dice + BCE) on matched pairs\n        loss_mask = 0.0\n        loss_dice = 0.0\n        num_instances = 0\n\n        for b in range(B):\n            pred_ind, tgt_ind = indices[b]\n            if len(pred_ind) == 0:\n                continue\n\n            tgt_masks = targets[b][\"masks\"]\n            tgt_masks_resized = F.interpolate(\n                tgt_masks.unsqueeze(1).float(),\n                size=(Hm, Wm),\n                mode=\"nearest\"\n            ).squeeze(1)  # [N_gt, Hm, Wm]\n\n            pred_masks = mask_logits[b, pred_ind]        # [M, Hm, Wm]\n            gt_masks = tgt_masks_resized[tgt_ind]        # [M, Hm, Wm]\n\n            loss_mask += self.sigmoid_ce_loss(pred_masks, gt_masks).sum()\n            loss_dice += self.dice_loss(pred_masks, gt_masks).sum()\n            num_instances += pred_ind.numel()\n\n        if num_instances > 0:\n            loss_mask = loss_mask / num_instances\n            loss_dice = loss_dice / num_instances\n        else:\n            loss_mask = mask_logits.sum() * 0.0\n            loss_dice = mask_logits.sum() * 0.0\n\n        # Mask-level classification BCE\n        # All GT instances are \"forgery\" (1). Unmatched predictions are ignore (0).\n        class_targets = torch.zeros_like(class_logits)  # [B, Q]\n        for b in range(B):\n            pred_ind, tgt_ind = indices[b]\n            if len(pred_ind) > 0:\n                class_targets[b, pred_ind] = 1.0\n\n        loss_cls = F.binary_cross_entropy_with_logits(\n            class_logits,\n            class_targets,\n        )\n\n        # Image-level authenticity loss\n        img_targets = torch.stack([t[\"image_label\"].float() for t in targets]).to(img_logits.device)  # [B]\n        loss_img = F.binary_cross_entropy_with_logits(img_logits, img_targets)\n\n        # Authenticity penalty: if authentic image (y=0) has non-empty predicted forgery mask\n        with torch.no_grad():\n            mask_probs = torch.sigmoid(mask_logits)        # [B, Q, Hm, Wm]\n            cls_probs = torch.sigmoid(class_logits)        # [B, Q]\n            # Take only masks with high forgery prob\n            forgery_mask = cls_probs > 0.5                 # [B, Q]\n\n        penalty = 0.0\n        for b in range(B):\n            if img_targets[b] < 0.5:  # authentic\n                if mask_logits.shape[1] == 0:\n                    continue\n                # Select predicted forgery masks\n                if forgery_mask[b].any():\n                    m = mask_probs[b, forgery_mask[b]]  # [K, Hm, Wm]\n                    # L1 norm as \"area\"\n                    penalty += m.mean()\n\n        if isinstance(penalty, float):\n            penalty = mask_logits.sum() * 0.0\n        loss_auth_penalty = self.authenticity_penalty_weight * penalty / max(B, 1)\n\n        losses = {\n            \"loss_mask_bce\": loss_mask,\n            \"loss_mask_dice\": loss_dice,\n            \"loss_mask_cls\": loss_cls,\n            \"loss_img_auth\": loss_img,\n            \"loss_auth_penalty\": loss_auth_penalty,\n        }\n\n        return losses\n\n    def match_cost(self, pred_masks, tgt_masks):\n        \"\"\"\n        pred_masks: [Q, H, W]\n        tgt_masks: [N_gt, H, W]\n        Returns cost matrix [N_gt, Q]\n        \"\"\"\n        Q, H, W = pred_masks.shape\n        N = tgt_masks.shape[0]\n\n        pred_flat = pred_masks.flatten(1)  # [Q, HW]\n        tgt_flat = tgt_masks.flatten(1)    # [N, HW]\n\n        # BCE cost\n        pred_logits = pred_flat.unsqueeze(0)             # [1, Q, HW]\n        tgt = tgt_flat.unsqueeze(1)                      # [N, 1, HW]\n        bce = F.binary_cross_entropy_with_logits(\n            pred_logits.expand(N, -1, -1),\n            tgt.expand(-1, Q, -1),\n            reduction=\"none\",\n        ).mean(-1)  # [N, Q]\n\n        # Dice cost\n        pred_prob = pred_flat.sigmoid()\n        numerator = 2 * (pred_prob.unsqueeze(0) * tgt_flat.unsqueeze(1)).sum(-1)\n        denominator = pred_prob.unsqueeze(0).sum(-1) + tgt_flat.unsqueeze(1).sum(-1) + 1e-6\n        dice = 1.0 - (numerator + 1e-6) / (denominator)\n\n        cost = bce + dice\n        return cost\n\n    @staticmethod\n    def sigmoid_ce_loss(inputs, targets):\n        \"\"\"\n        BCE on logits, per-instance mean.\n        inputs: [M, H, W], targets: [M, H, W]\n        \"\"\"\n        return F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\").mean(dim=(1, 2))\n\n    @staticmethod\n    def dice_loss(inputs, targets, eps=1e-6):\n        \"\"\"\n        Soft dice loss on logits.\n        inputs: [M, H, W], targets: [M, H, W]\n        \"\"\"\n        inputs = inputs.sigmoid()\n        inputs = inputs.flatten(1)\n        targets = targets.flatten(1)\n\n        numerator = 2 * (inputs * targets).sum(1)\n        denominator = inputs.sum(1) + targets.sum(1) + eps\n        loss = 1 - (numerator + eps) / (denominator)\n        return loss\n\n    # ------------------- Inference -------------------\n\n    def inference(self, mask_logits, class_logits, img_logits, mask_threshold=0.5, cls_threshold=0.5):\n        \"\"\"\n        Returns list of dicts per image:\n          - 'masks': [K, Hm, Wm] uint8\n          - 'mask_scores': [K]\n          - 'mask_forgery_scores': [K]\n          - 'image_authenticity': float in [0,1], prob of \"forged\"\n        Authenticity gate: if prob(authentic) > 0.5, masks list is empty.\n        \"\"\"\n        B, Q, Hm, Wm = mask_logits.shape\n        mask_probs = torch.sigmoid(mask_logits)\n        cls_probs = torch.sigmoid(class_logits)\n        img_probs = torch.sigmoid(img_logits)  # prob \"forged\"\n\n        outputs = []\n        for b in range(B):\n            forged_prob = img_probs[b].item()\n            authentic_prob = 1.0 - forged_prob\n\n            if authentic_prob > 0.5:\n                # Authenticity gate: suppress all masks\n                outputs.append({\n                    \"masks\": torch.zeros((0, Hm, Wm), dtype=torch.uint8, device=mask_logits.device),\n                    \"mask_scores\": torch.empty(0, device=mask_logits.device),\n                    \"mask_forgery_scores\": torch.empty(0, device=mask_logits.device),\n                    \"image_authenticity\": forged_prob,\n                })\n                continue\n\n            # Keep masks with high forgery prob\n            keep = cls_probs[b] > cls_threshold\n            if keep.sum() == 0:\n                outputs.append({\n                    \"masks\": torch.zeros((0, Hm, Wm), dtype=torch.uint8, device=mask_logits.device),\n                    \"mask_scores\": torch.empty(0, device=mask_logits.device),\n                    \"mask_forgery_scores\": torch.empty(0, device=mask_logits.device),\n                    \"image_authenticity\": forged_prob,\n                })\n                continue\n\n            masks_b = (mask_probs[b, keep] > mask_threshold).to(torch.uint8)  # [K, Hm, Wm]\n            scores_b = mask_probs[b, keep].flatten(1).mean(-1)                # avg mask prob\n            cls_b = cls_probs[b, keep]\n\n            outputs.append({\n                \"masks\": masks_b,\n                \"mask_scores\": scores_b,\n                \"mask_forgery_scores\": cls_b,\n                \"image_authenticity\": forged_prob,\n            })\n\n        return outputs\n\nmodel = Mask2FormerForgeryModel(num_queries=15, d_model=256).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\nfor images, targets in train_loader:\n    images = [img.to(device) for img in images]\n    for t in targets:\n        t['masks'] = t['masks'].to(device)\n        t['image_label'] = t['image_label'].to(device)\n\n    loss_dict = model(images, targets)\n    loss = sum(loss_dict.values())\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:40:05.788815Z","iopub.execute_input":"2025-12-09T23:40:05.789157Z","iopub.status.idle":"2025-12-09T23:46:19.281214Z","shell.execute_reply.started":"2025-12-09T23:40:05.789132Z","shell.execute_reply":"2025-12-09T23:46:19.280371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Sanity / shape checks for encoder/decoder/model/loss/train/val ====\nprint(f\"Using device: {device}\")\n\n# Small dummy batch\nB, C, H, W = 2, 3, 128, 128\ndummy_imgs = torch.randn(B, C, H, W, device=device)\n\n# -------------------------------------------------------------------------\n# 1) Instantiate model\n# -------------------------------------------------------------------------\nmodel = Mask2FormerForgeryModel(num_queries=15, d_model=256).to(device)\nmodel.eval()\nprint(\"\\n[Model] num_queries:\", model.num_queries)\n\n# -------------------------------------------------------------------------\n# 2) Encoder: ConvNeXt + FPN features\n# -------------------------------------------------------------------------\nwith torch.no_grad():\n    fpn_feats = model.backbone(dummy_imgs)  # list of feature maps\n\nprint(\"\\n[Encoder / FPN]\")\nfor i, f in enumerate(fpn_feats):\n    print(f\"  P{i}: {tuple(f.shape)}\")\nassert len(fpn_feats) >= 1, \"Backbone/FPN returned no feature maps\"\n\n# -------------------------------------------------------------------------\n# 3) Positional encodings + Transformer decoder\n# -------------------------------------------------------------------------\nwith torch.no_grad():\n    pos_list = [model.position_encoding(f) for f in fpn_feats]\n    hs_all = model.transformer_decoder(fpn_feats, pos_list)  # [num_layers, B, Q, C]\n    hs = hs_all[-1]                                          # [B, Q, C]\n\nprint(\"\\n[Decoder]\")\nprint(\"  hs_all shape (num_layers, B, Q, C):\", tuple(hs_all.shape))\nprint(\"  hs last layer shape (B, Q, C):\", tuple(hs.shape))\n\n# -------------------------------------------------------------------------\n# 4) Build dummy targets and run full forward (training mode -> losses)\n# -------------------------------------------------------------------------\ndummy_targets = []\n\n# sample 0: forged image with 2 GT instances\nmasks0 = torch.zeros(2, H, W, device=device)\nmasks0[0, H // 4 : H // 2, W // 4 : W // 2] = 1\nmasks0[1, H // 2 : 3 * H // 4, W // 2 : 3 * W // 4] = 1\ndummy_targets.append({\n    \"masks\": masks0,\n    \"image_label\": torch.tensor(1.0, device=device),\n})\n\n# sample 1: authentic image with no GT masks\nmasks1 = torch.zeros(0, H, W, device=device)\ndummy_targets.append({\n    \"masks\": masks1,\n    \"image_label\": torch.tensor(0.0, device=device),\n})\n\nmodel.train()\nloss_dict = model(dummy_imgs, dummy_targets)\nprint(\"\\n[Forward + losses (train mode)]\")\nfor k, v in loss_dict.items():\n    print(f\"  {k}: {float(v):.4f}\")\ntotal_loss = sum(loss_dict.values())\nprint(\"  total loss:\", float(total_loss))\n\n# -------------------------------------------------------------------------\n# 5) Direct loss helper: match_cost sanity\n# -------------------------------------------------------------------------\nwith torch.no_grad():\n    pred_masks = torch.randn(4, H, W, device=device)                # Q=4\n    tgt_masks = (torch.rand(3, H, W, device=device) > 0.5).float()  # N_gt=3\n    cost = model.match_cost(pred_masks, tgt_masks)\n\nprint(\"\\n[match_cost]\")\nprint(\"  cost shape (N_gt, Q):\", tuple(cost.shape))\n\n# -------------------------------------------------------------------------\n# 6) Inference forward (no targets) to check outputs\n# -------------------------------------------------------------------------\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(dummy_imgs, targets=None)\n\nprint(\"\\n[Forward (eval mode / inference)]\")\nprint(\"  batch size:\", len(outputs))\nfor b, out in enumerate(outputs):\n    print(f\"  sample {b}:\")\n    print(\"    masks:\", tuple(out[\"masks\"].shape))\n    print(\"    mask_scores:\", tuple(out[\"mask_scores\"].shape))\n    print(\"    mask_forgery_scores:\", tuple(out[\"mask_forgery_scores\"].shape))\n    print(\"    image_authenticity:\", float(out[\"image_authenticity\"]))\n\n# -------------------------------------------------------------------------\n# 7) One dummy train step\n# -------------------------------------------------------------------------\nmodel.train()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\noptimizer.zero_grad()\nloss_dict = model(dummy_imgs, dummy_targets)\ntrain_loss = sum(loss_dict.values())\ntrain_loss.backward()\noptimizer.step()\nprint(\"\\n[Train step]\")\nprint(\"  train_loss:\", float(train_loss))\n\n# -------------------------------------------------------------------------\n# 8) One dummy val step (no grad)\n# -------------------------------------------------------------------------\nmodel.eval()\nwith torch.no_grad():\n    val_loss_dict = model(dummy_imgs, dummy_targets)\nval_loss = sum(val_loss_dict.values())\nprint(\"\\n[Val step]\")\nprint(\"  val_loss:\", float(val_loss))\n\nprint(\"\\n[Done] All main components ran without shape errors (if you see this, shapes are consistent).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:46:19.28234Z","iopub.execute_input":"2025-12-09T23:46:19.28282Z","iopub.status.idle":"2025-12-09T23:46:52.507132Z","shell.execute_reply.started":"2025-12-09T23:46:19.2828Z","shell.execute_reply":"2025-12-09T23:46:52.506318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rle_encode(mask):\n    \"\"\"\n    Encode binary mask to RLE in the format required by the competition.\n    Returns a JSON string like \"[123,4,567,8]\"\n    \"\"\"\n    # Ensure mask is 2D and binary\n    mask = mask.astype(bool)\n    \n    # Flatten in Fortran order\n    flat = mask.T.flatten()\n    \n    # Find indices where value is True\n    dots = np.where(flat)[0]\n    \n    if len(dots) == 0:\n        return json.dumps([])  # or just return 'authentic' upstream\n    \n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if b > prev + 1:\n            run_lengths.extend([b + 1, 0])  # 1-based index\n        run_lengths[-1] += 1\n        prev = b\n    \n    # Convert numpy ints to Python ints for JSON compatibility\n    run_lengths = [int(x) for x in run_lengths]\n    return json.dumps(run_lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:46:52.507974Z","iopub.execute_input":"2025-12-09T23:46:52.508272Z","iopub.status.idle":"2025-12-09T23:46:52.513595Z","shell.execute_reply.started":"2025-12-09T23:46:52.508236Z","shell.execute_reply":"2025-12-09T23:46:52.512885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_test_images(model, test_path, device):\n    model.eval()\n    predictions = {}\n    \n    test_files = sorted(os.listdir(test_path))\n    \n    transform = A.Compose([\n        A.Resize(256, 256),\n        A.Normalize(mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    \n    for file in tqdm(test_files, desc=\"Processing test images\"):\n        case_id = file.split('.')[0]\n        \n        # Load and preprocess image\n        img_path = os.path.join(test_path, file)\n        image = Image.open(img_path).convert('RGB')\n        image_np = np.array(image)\n        original_h, original_w = image_np.shape[:2]\n        \n        transformed = transform(image=image_np)\n        image_tensor = transformed['image'].unsqueeze(0).to(device)\n        \n        # Model prediction (our Mask2Former-style model)\n        with torch.no_grad():\n            outputs = model(image_tensor)   # list of length 1\n        out = outputs[0]\n        \n        masks = out[\"masks\"]              # [K, Hm, Wm], uint8 0/1\n        # Authenticity gate is already applied inside the model:\n        #   - if image predicted authentic -> masks.shape[0] == 0\n        \n        if masks.shape[0] == 0:\n            # Authentic image ⇒ \"authentic\" string for submission\n            predictions[case_id] = \"authentic\"\n            continue\n        \n        # Combine instance masks into a single binary mask\n        combined_mask = (masks.sum(dim=0) > 0).to(torch.uint8)  # [Hm, Wm]\n        \n        # Resize back to original size\n        combined_mask_np = combined_mask.cpu().numpy().astype(np.uint8)\n        combined_mask_resized = cv2.resize(\n            combined_mask_np,\n            (original_w, original_h),\n            interpolation=cv2.INTER_NEAREST\n        )\n        \n        # RLE encoding for competition\n        if combined_mask_resized.sum() == 0:\n            predictions[case_id] = \"authentic\"\n        else:\n            rle_json = rle_encode(combined_mask_resized)\n            predictions[case_id] = rle_json\n    \n    return predictions\n\npredictions = predict_test_images(model, paths['test_images'], device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:46:52.514314Z","iopub.execute_input":"2025-12-09T23:46:52.514601Z","iopub.status.idle":"2025-12-09T23:46:52.637756Z","shell.execute_reply.started":"2025-12-09T23:46:52.51458Z","shell.execute_reply":"2025-12-09T23:46:52.637148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reading the sample submission for the correct order\nsample_submission = pd.read_csv('/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv')\n    \n# Create DataFrame with predictions\nsubmission_data = []\nfor case_id in sample_submission['case_id']:\n    case_id_str = str(case_id)\n    if case_id_str in predictions:\n        submission_data.append({'case_id': case_id, 'annotation': predictions[case_id_str]})\n    else:\n        # If case_id not in predictions, use authentic as default\n        submission_data.append({'case_id': case_id, 'annotation': 'authentic'})\n    \nsubmission = pd.DataFrame(submission_data)\n    \n# Save submission file\nsubmission.to_csv('submission.csv', index=False)\n    \n# Prediction statistics\nauthentic_count = (submission['annotation'] == 'authentic').sum()\nforged_count = len(submission) - authentic_count\n\nprint(f\"Prediction Statistics:\")\nprint(f\"Authentic: {authentic_count}\")\nprint(f\"Forged: {forged_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:46:52.638459Z","iopub.execute_input":"2025-12-09T23:46:52.63871Z","iopub.status.idle":"2025-12-09T23:46:52.669917Z","shell.execute_reply.started":"2025-12-09T23:46:52.638693Z","shell.execute_reply":"2025-12-09T23:46:52.669383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\n# Take the first file from the test folder\ntest_files = sorted(os.listdir(paths['test_images']))\nfile = test_files[0]\nimg_path = os.path.join(paths['test_images'], file)\n\n# Load image\nimage = Image.open(img_path).convert('RGB')\nimage_np = np.array(image)\n\n# Transformations\ntransform = A.Compose([\n    A.Resize(256, 256),\n    A.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\ntransformed = transform(image=image_np)\nimage_tensor = transformed['image'].unsqueeze(0).to(device)\n\nwith torch.no_grad():\n    outputs = model(image_tensor)   # list of length 1\nout = outputs[0]\n\nmasks = out[\"masks\"]                     # [K, Hm, Wm], uint8 {0,1}\nmask_scores = out[\"mask_scores\"]        # [K]\nforgery_scores = out[\"mask_forgery_scores\"]  # [K]\nimg_forged_prob = out[\"image_authenticity\"]  # scalar in [0,1]\n\n# Thresholds\nforgery_threshold = 0.5   # per-mask forgery score\nauth_threshold = 0.5       # image-level authenticity gate\n\n# Decide valid detections\nif masks.shape[0] == 0 or img_forged_prob < auth_threshold:\n    # Authenticity gate or no masks -> predicted authentic\n    valid_detections = torch.zeros(0, dtype=torch.bool, device=masks.device)\nelse:\n    valid_detections = forgery_scores > forgery_threshold  # [K]\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Original\naxes[0].imshow(image_np)\naxes[0].set_title(f'Original: {file}', fontsize=14, fontweight='bold')\naxes[0].axis('off')\n\n# Mask side\nif valid_detections.numel() == 0 or valid_detections.sum() == 0:\n    combined_mask = np.zeros((256, 256), dtype=np.float32)\n    title = f'Predicted: Authentic (p_forged={img_forged_prob:.2f})'\n    cmap = 'gray'\nelse:\n    # Combine predicted forgery masks, upsample to 256x256 for display\n    combined_mask = np.zeros((256, 256), dtype=np.float32)\n    for idx in torch.nonzero(valid_detections, as_tuple=False).flatten():\n        mk = masks[idx].float().cpu().numpy()  # [Hm, Wm]\n        # Resize to 256x256 for visualization\n        mk_resized = cv2.resize(mk, (256, 256), interpolation=cv2.INTER_NEAREST)\n        combined_mask = np.maximum(combined_mask, mk_resized)\n\n    title = (\n        f'Predicted: Forged ({valid_detections.sum().item()} regions, '\n        f'p_forged={img_forged_prob:.2f})'\n    )\n    cmap = 'hot'\n\nim = axes[1].imshow(combined_mask, cmap=cmap, vmin=0, vmax=1)\naxes[1].set_title(title, fontsize=14, fontweight='bold')\naxes[1].axis('off')\n\nif valid_detections.numel() > 0 and valid_detections.sum() > 0:\n    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:46:52.67055Z","iopub.execute_input":"2025-12-09T23:46:52.670736Z","iopub.status.idle":"2025-12-09T23:46:53.278772Z","shell.execute_reply.started":"2025-12-09T23:46:52.670721Z","shell.execute_reply":"2025-12-09T23:46:53.278037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/working/submission.csv')\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T23:46:53.279569Z","iopub.execute_input":"2025-12-09T23:46:53.279836Z","iopub.status.idle":"2025-12-09T23:46:53.297815Z","shell.execute_reply.started":"2025-12-09T23:46:53.279815Z","shell.execute_reply":"2025-12-09T23:46:53.297001Z"}},"outputs":[],"execution_count":null}]}