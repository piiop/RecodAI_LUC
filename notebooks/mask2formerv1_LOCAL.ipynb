{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:38:48.245210Z",
     "iopub.status.busy": "2025-12-10T17:38:48.245015Z",
     "iopub.status.idle": "2025-12-10T17:39:35.645326Z",
     "shell.execute_reply": "2025-12-10T17:39:35.644536Z",
     "shell.execute_reply.started": "2025-12-10T17:38:48.245193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import sympy\n",
    "import sympy.printing  # ensure submodule is imported\n",
    "sympy.printing = sympy.printing  # attach as attribute explicitly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights\n",
    "from torchvision.ops import FeaturePyramidNetwork\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:39:35.647940Z",
     "iopub.status.busy": "2025-12-10T17:39:35.647290Z",
     "iopub.status.idle": "2025-12-10T17:39:35.739935Z",
     "shell.execute_reply": "2025-12-10T17:39:35.739097Z",
     "shell.execute_reply.started": "2025-12-10T17:39:35.647920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:39:35.743714Z",
     "iopub.status.busy": "2025-12-10T17:39:35.743484Z",
     "iopub.status.idle": "2025-12-10T17:39:36.021330Z",
     "shell.execute_reply": "2025-12-10T17:39:36.020700Z",
     "shell.execute_reply.started": "2025-12-10T17:39:35.743676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentic images: 2377\n",
      "Forged images: 2751\n",
      "Masks: 2751\n",
      "Test images: 1\n",
      "Supp forged images: 48\n",
      "Supp masks: 48\n",
      "Examples of mask files: ['10.npy', '10015.npy', '10017.npy', '10030.npy', '10070.npy']\n",
      "Mask format: (1, 512, 648), dtype: uint8\n",
      "Test images: ['45.png']\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_structure():\n",
    "    base_path = '.'\n",
    "    \n",
    "    # Checking train images\n",
    "    train_authentic_path = os.path.join(base_path, 'train_images/authentic')\n",
    "    train_forged_path = os.path.join(base_path, 'train_images/forged')\n",
    "    train_masks_path = os.path.join(base_path, 'train_masks')\n",
    "    test_images_path = os.path.join(base_path, 'test_images')\n",
    "    supp_forged_path = os.path.join(base_path, 'supplemental_images')\n",
    "    supp_masks_path = os.path.join(base_path, 'supplemental_masks')\n",
    "    \n",
    "    print(f\"Authentic images: {len(os.listdir(train_authentic_path))}\")\n",
    "    print(f\"Forged images: {len(os.listdir(train_forged_path))}\")\n",
    "    print(f\"Masks: {len(os.listdir(train_masks_path))}\")\n",
    "    print(f\"Test images: {len(os.listdir(test_images_path))}\")\n",
    "\n",
    "    print(f\"Supp forged images: {len(os.listdir(supp_forged_path))}\")\n",
    "    print(f\"Supp masks: {len(os.listdir(supp_masks_path))}\")\n",
    "    \n",
    "    # Let's analyze some examples of masks\n",
    "    mask_files = os.listdir(train_masks_path)[:5]\n",
    "    print(f\"Examples of mask files: {mask_files}\")\n",
    "    \n",
    "    # Checking the mask format\n",
    "    sample_mask = np.load(os.path.join(train_masks_path, mask_files[0]))\n",
    "    print(f\"Mask format: {sample_mask.shape}, dtype: {sample_mask.dtype}\")\n",
    "    \n",
    "    test_files = os.listdir(test_images_path)\n",
    "    print(f\"Test images: {test_files}\")\n",
    "    \n",
    "    return {\n",
    "        'train_authentic': train_authentic_path,\n",
    "        'train_forged': train_forged_path,\n",
    "        'train_masks': train_masks_path,\n",
    "        'test_images': test_images_path,\n",
    "        'supp_forged': supp_forged_path,\n",
    "        'supp_masks' : supp_masks_path\n",
    "    }\n",
    "\n",
    "paths = analyze_data_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:39:36.022417Z",
     "iopub.status.busy": "2025-12-10T17:39:36.022124Z",
     "iopub.status.idle": "2025-12-10T17:39:36.037713Z",
     "shell.execute_reply": "2025-12-10T17:39:36.037102Z",
     "shell.execute_reply.started": "2025-12-10T17:39:36.022391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ForgeryDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        authentic_path,\n",
    "        forged_path,\n",
    "        masks_path,\n",
    "        supp_forged_path=None,\n",
    "        supp_masks_path=None,\n",
    "        transform=None,\n",
    "        is_train=True,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Collect all data samples\n",
    "        self.samples = []\n",
    "        \n",
    "        # Authentic images\n",
    "        for file in os.listdir(authentic_path):\n",
    "            img_path = os.path.join(authentic_path, file)\n",
    "            base_name = file.split('.')[0]\n",
    "            mask_path = os.path.join(masks_path, f\"{base_name}.npy\")\n",
    "            \n",
    "            self.samples.append({\n",
    "                'image_path': img_path,\n",
    "                'mask_path': mask_path,\n",
    "                'is_forged': False,\n",
    "                'image_id': base_name\n",
    "            })\n",
    "        \n",
    "        # Forged images (original)\n",
    "        for file in os.listdir(forged_path):\n",
    "            img_path = os.path.join(forged_path, file)\n",
    "            base_name = file.split('.')[0]\n",
    "            mask_path = os.path.join(masks_path, f\"{base_name}.npy\")\n",
    "            \n",
    "            self.samples.append({\n",
    "                'image_path': img_path,\n",
    "                'mask_path': mask_path,\n",
    "                'is_forged': True,\n",
    "                'image_id': base_name\n",
    "            })\n",
    "\n",
    "        # Supplemental forged images (all forged)\n",
    "        if supp_forged_path is not None and supp_masks_path is not None:\n",
    "            for file in os.listdir(supp_forged_path):\n",
    "                img_path = os.path.join(supp_forged_path, file)\n",
    "                base_name = file.split('.')[0]\n",
    "                mask_path = os.path.join(supp_masks_path, f\"{base_name}.npy\")\n",
    "                \n",
    "                self.samples.append({\n",
    "                    'image_path': img_path,\n",
    "                    'mask_path': mask_path,\n",
    "                    'is_forged': True,\n",
    "                    'image_id': base_name\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(sample['image_path']).convert('RGB')\n",
    "        image = np.array(image)  # (H, W, 3)\n",
    "        \n",
    "        # Load and process mask\n",
    "        if os.path.exists(sample['mask_path']):\n",
    "            mask = np.load(sample['mask_path'])\n",
    "            \n",
    "            # Handle multi-channel masks: MUST be channel-first (C, H, W)\n",
    "            if mask.ndim == 3:\n",
    "                # Expect small channel dimension as first dim\n",
    "                if mask.shape[0] <= 15 and mask.shape[1:] == image.shape[:2]:\n",
    "                    mask = np.any(mask, axis=0)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Expected channel-first mask (C, H, W) with small C. Got {mask.shape}\"\n",
    "                    )\n",
    "            \n",
    "            mask = (mask > 0).astype(np.uint8)\n",
    "        else:\n",
    "            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "    \n",
    "        # Shape validation\n",
    "        assert image.shape[:2] == mask.shape, f\"Shape mismatch: img {image.shape}, mask {mask.shape}\"\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        else:\n",
    "            image = F_transforms.to_tensor(image)\n",
    "            mask = torch.tensor(mask, dtype=torch.uint8)\n",
    "        \n",
    "        # Prepare targets for Mask2Former\n",
    "        if sample['is_forged'] and mask.sum() > 0:\n",
    "            boxes, labels, masks = self.mask_to_boxes(mask)\n",
    "            \n",
    "            target = {\n",
    "                'boxes': boxes,\n",
    "                'labels': labels,\n",
    "                'masks': masks,\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "                'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "            }\n",
    "            target['image_label'] = torch.tensor(1.0)   # forged\n",
    "        else:\n",
    "            # For authentic images or images without masks\n",
    "            target = {\n",
    "                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'masks': torch.zeros((0, image.shape[1], image.shape[2]), dtype=torch.uint8),\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': torch.zeros(0, dtype=torch.float32),\n",
    "                'iscrowd': torch.zeros((0,), dtype=torch.int64)\n",
    "            }\n",
    "            target['image_label'] = torch.tensor(0.0)   # authentic\n",
    "        return image, target\n",
    "    \n",
    "    def mask_to_boxes(self, mask):\n",
    "        \"\"\"Convert segmentation mask to bounding boxes for Mask2Former\"\"\"\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask_np = mask.numpy()\n",
    "        else:\n",
    "            mask_np = mask\n",
    "        \n",
    "        # Find contours in the mask\n",
    "        contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        boxes = []\n",
    "        masks = []\n",
    "        \n",
    "        for contour in contours:\n",
    "            if len(contour) > 0:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                # Filter out very small regions\n",
    "                if w > 5 and h > 5:\n",
    "                    boxes.append([x, y, x + w, y + h])\n",
    "                    # Create binary mask for this contour\n",
    "                    contour_mask = np.zeros_like(mask_np)\n",
    "                    cv2.fillPoly(contour_mask, [contour], 1)\n",
    "                    masks.append(contour_mask)\n",
    "        \n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "            masks = torch.tensor(np.array(masks), dtype=torch.uint8)\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "            masks = torch.zeros((0, mask_np.shape[0], mask_np.shape[1]), dtype=torch.uint8)\n",
    "        \n",
    "        return boxes, labels, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:39:36.038529Z",
     "iopub.status.busy": "2025-12-10T17:39:36.038334Z",
     "iopub.status.idle": "2025-12-10T17:39:36.093185Z",
     "shell.execute_reply": "2025-12-10T17:39:36.092561Z",
     "shell.execute_reply.started": "2025-12-10T17:39:36.038514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4140\n",
      "Val samples: 1036\n"
     ]
    }
   ],
   "source": [
    "# Transformations for learning, ImageNet standards\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "full_dataset = ForgeryDataset(\n",
    "    paths['train_authentic'],\n",
    "    paths['train_forged'],\n",
    "    paths['train_masks'],\n",
    "    supp_forged_path=paths['supp_forged'],\n",
    "    supp_masks_path=paths['supp_masks'],\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Use val transforms for validation subset\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:39:36.095557Z",
     "iopub.status.busy": "2025-12-10T17:39:36.095313Z",
     "iopub.status.idle": "2025-12-10T17:39:36.101879Z",
     "shell.execute_reply": "2025-12-10T17:39:36.101315Z",
     "shell.execute_reply.started": "2025-12-10T17:39:36.095539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvNeXtFPNBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNeXt-T backbone with FPN neck.\n",
    "    Uses outputs from indices [1, 3, 5, 7], which have channels\n",
    "    [96, 192, 384, 768]\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels=256, train_backbone=True):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            backbone = convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "        except Exception:\n",
    "            backbone = convnext_tiny(weights=None)\n",
    "\n",
    "        self.body = backbone.features  # modules 0..7\n",
    "\n",
    "        # Use the last block of each resolution stage:\n",
    "        #  1: 96 @ 64×64\n",
    "        #  3: 192 @ 32×32\n",
    "        #  5: 384 @ 16×16\n",
    "        #  7: 768 @  8×8\n",
    "        self.out_indices = (1, 3, 5, 7)\n",
    "\n",
    "        in_channels_list = [96, 192, 384, 768]\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=in_channels_list,\n",
    "            out_channels=out_channels,\n",
    "        )\n",
    "\n",
    "        if not train_backbone:\n",
    "            for p in self.body.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        out = x\n",
    "        for i, layer in enumerate(self.body):\n",
    "            out = layer(out)\n",
    "            if i in self.out_indices:\n",
    "                feats.append(out)\n",
    "\n",
    "        # feats is a list of 4 tensors with channels [96, 192, 384, 768]\n",
    "        feat_dict = {str(i): f for i, f in enumerate(feats)}  # keys \"0\",\"1\",\"2\",\"3\"\n",
    "        fpn_out = self.fpn(feat_dict)  # dict of {level_name: [B, C, H, W]}\n",
    "\n",
    "        # Sort by level name to keep consistent order\n",
    "        levels = [fpn_out[k] for k in sorted(fpn_out.keys(), key=int)]\n",
    "        return levels  # [P2, P3, P4, P5], all with C=out_channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:39:36.102675Z",
     "iopub.status.busy": "2025-12-10T17:39:36.102462Z",
     "iopub.status.idle": "2025-12-10T17:40:09.476291Z",
     "shell.execute_reply": "2025-12-10T17:40:09.475483Z",
     "shell.execute_reply.started": "2025-12-10T17:39:36.102650Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# backbone_fpn = ConvNeXtFPNBackbone(out_channels=256, train_backbone=False).eval()\n",
    "\n",
    "# x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     levels = backbone_fpn(x)\n",
    "#     print(\"=== FPN levels ===\")\n",
    "#     for i, f in enumerate(levels):\n",
    "#         print(f\"Level {i}: {tuple(f.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:40:09.477515Z",
     "iopub.status.busy": "2025-12-10T17:40:09.477216Z",
     "iopub.status.idle": "2025-12-10T17:40:09.486715Z",
     "shell.execute_reply": "2025-12-10T17:40:09.485950Z",
     "shell.execute_reply.started": "2025-12-10T17:40:09.477484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DetrTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
    "\n",
    "        # FFN\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.dropout_ffn = nn.Dropout(dropout)\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = F.gelu\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "    def with_pos(self, x, pos):\n",
    "        if pos is None:\n",
    "            return x\n",
    "        return x + pos\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,                      # [Q, B, C]\n",
    "        memory,                   # [S, B, C]\n",
    "        tgt_pos=None,             # [Q, B, C] or None\n",
    "        memory_pos=None,          # [S, B, C] or None\n",
    "        tgt_mask=None,\n",
    "        memory_mask=None,\n",
    "        tgt_key_padding_mask=None,\n",
    "        memory_key_padding_mask=None,\n",
    "    ):\n",
    "        # ---- Self-attention (queries attend to themselves) ----\n",
    "        q = k = self.with_pos(tgt, tgt_pos)\n",
    "        tgt2, _ = self.self_attn(\n",
    "            q, k, value=tgt,\n",
    "            attn_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # ---- Cross-attention (queries attend to encoder memory) ----\n",
    "        q = self.with_pos(tgt, tgt_pos)\n",
    "        k = self.with_pos(memory, memory_pos)\n",
    "        tgt2, _ = self.multihead_attn(\n",
    "            q, k, value=memory,\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=memory_key_padding_mask,\n",
    "        )\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # ---- Feed-forward ----\n",
    "        tgt2 = self.linear2(self.dropout_ffn(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "\n",
    "        return tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:40:09.487787Z",
     "iopub.status.busy": "2025-12-10T17:40:09.487487Z",
     "iopub.status.idle": "2025-12-10T17:40:09.515695Z",
     "shell.execute_reply": "2025-12-10T17:40:09.515074Z",
     "shell.execute_reply.started": "2025-12-10T17:40:09.487768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DetrTransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,                      # [Q, B, C]\n",
    "        memory,                   # [S, B, C]\n",
    "        tgt_pos=None,             # [Q, B, C] or None\n",
    "        memory_pos=None,          # [S, B, C] or None\n",
    "        tgt_mask=None,\n",
    "        memory_mask=None,\n",
    "        tgt_key_padding_mask=None,\n",
    "        memory_key_padding_mask=None,\n",
    "    ):\n",
    "        output = tgt\n",
    "        intermediate = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(\n",
    "                output,\n",
    "                memory,\n",
    "                tgt_pos=tgt_pos,\n",
    "                memory_pos=memory_pos,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask,\n",
    "            )\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(output)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "            if self.return_intermediate:\n",
    "                intermediate[-1] = output\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            # [num_layers, Q, B, C]\n",
    "            return torch.stack(intermediate)\n",
    "\n",
    "        # [1, Q, B, C] to match DETR’s interface\n",
    "        return output.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:40:09.516611Z",
     "iopub.status.busy": "2025-12-10T17:40:09.516354Z",
     "iopub.status.idle": "2025-12-10T17:40:09.535429Z",
     "shell.execute_reply": "2025-12-10T17:40:09.534833Z",
     "shell.execute_reply.started": "2025-12-10T17:40:09.516589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleTransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=256,\n",
    "        nhead=8,\n",
    "        num_layers=6,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        num_queries=100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "\n",
    "        # learned object queries (like DETR)\n",
    "        self.query_embed = nn.Embedding(num_queries, d_model)\n",
    "\n",
    "        layer = DetrTransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.decoder = DetrTransformerDecoder(\n",
    "            decoder_layer=layer,\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(d_model),\n",
    "            return_intermediate=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, feats, pos_list):\n",
    "        \"\"\"\n",
    "        feats:    list of [B, C, H, W]\n",
    "        pos_list: list of [B, C, H, W] (positional encodings for each feat)\n",
    "        \"\"\"\n",
    "        srcs = []\n",
    "        pos_embs = []\n",
    "        for feat, pos in zip(feats, pos_list):\n",
    "            # [B, C, H, W] -> [HW, B, C]\n",
    "            srcs.append(feat.flatten(2).permute(2, 0, 1))\n",
    "            pos_embs.append(pos.flatten(2).permute(2, 0, 1))\n",
    "\n",
    "        memory = torch.cat(srcs, dim=0)        # [S, B, C]\n",
    "        memory_pos = torch.cat(pos_embs, dim=0)  # [S, B, C]\n",
    "\n",
    "        B = memory.size(1)\n",
    "        # [Q, B, C] query positions\n",
    "        query_pos = self.query_embed.weight.unsqueeze(1).repeat(1, B, 1)\n",
    "        tgt = torch.zeros_like(query_pos)  # initial target is zeros; queries in query_pos\n",
    "\n",
    "        hs = self.decoder(\n",
    "            tgt=tgt,\n",
    "            memory=memory,\n",
    "            tgt_pos=query_pos,\n",
    "            memory_pos=memory_pos,\n",
    "        )\n",
    "        # [num_layers, Q, B, C] -> [num_layers, B, Q, C]\n",
    "        hs = hs.permute(0, 2, 1, 3)\n",
    "        return hs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:40:09.536220Z",
     "iopub.status.busy": "2025-12-10T17:40:09.536016Z",
     "iopub.status.idle": "2025-12-10T17:40:09.562774Z",
     "shell.execute_reply": "2025-12-10T17:40:09.562224Z",
     "shell.execute_reply.started": "2025-12-10T17:40:09.536196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard sine-cosine positional encoding as in DETR.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=128, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        self.scale = 2 * torch.pi if scale is None else scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        B, C, H, W = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        y_embed = torch.arange(H, device=device).unsqueeze(1).repeat(1, W)\n",
    "        x_embed = torch.arange(W, device=device).unsqueeze(0).repeat(H, 1)\n",
    "\n",
    "        y_embed = y_embed.float()\n",
    "        x_embed = x_embed.float()\n",
    "\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (H - 1 + eps) * self.scale\n",
    "            x_embed = x_embed / (W - 1 + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, device=device).float()\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[..., None] / dim_t\n",
    "        pos_y = y_embed[..., None] / dim_t\n",
    "        pos_x = torch.stack([pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()], dim=-1).flatten(-2)\n",
    "        pos_y = torch.stack([pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()], dim=-1).flatten(-2)\n",
    "\n",
    "        pos = torch.cat([pos_y, pos_x], dim=-1)  # [H, W, 2*num_pos_feats]\n",
    "        pos = pos.permute(2, 0, 1).unsqueeze(0).expand(B, -1, -1, -1)  # [B, C, H, W]\n",
    "        return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:40:09.563812Z",
     "iopub.status.busy": "2025-12-10T17:40:09.563411Z",
     "iopub.status.idle": "2025-12-10T17:40:42.553614Z",
     "shell.execute_reply": "2025-12-10T17:40:42.552878Z",
     "shell.execute_reply.started": "2025-12-10T17:40:09.563787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Instantiate ConvNeXt-Tiny exactly as in the Mask2Former model\n",
    "# try:\n",
    "#     backbone = convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "# except Exception:\n",
    "#     backbone = convnext_tiny(weights=None)\n",
    "\n",
    "# backbone.eval()\n",
    "\n",
    "# # Dummy input\n",
    "# x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     out = x\n",
    "#     print(\"Input:\", x.shape)\n",
    "#     for i, layer in enumerate(backbone.features):\n",
    "#         out = layer(out)\n",
    "#         print(f\"features[{i}] output:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:40:42.554741Z",
     "iopub.status.busy": "2025-12-10T17:40:42.554382Z",
     "iopub.status.idle": "2025-12-10T17:41:15.631451Z",
     "shell.execute_reply": "2025-12-10T17:41:15.630663Z",
     "shell.execute_reply.started": "2025-12-10T17:40:42.554718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# backbone_fpn = ConvNeXtFPNBackbone(out_channels=256, train_backbone=False).eval()\n",
    "\n",
    "# x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     feats = []\n",
    "#     out = x\n",
    "#     print(\"=== Debug ConvNeXtFPNBackbone ===\")\n",
    "#     for i, layer in enumerate(backbone_fpn.body):\n",
    "#         out = layer(out)\n",
    "#         print(f\"body[{i}] output:\", out.shape)\n",
    "#     print(\"---- FPN inputs (as dict keys / shapes) ----\")\n",
    "#     # Re-run forward like in the class, but instrumented\n",
    "#     out = x\n",
    "#     feat_list = []\n",
    "#     for i, layer in enumerate(backbone_fpn.body):\n",
    "#         out = layer(out)\n",
    "#         if i in backbone_fpn.out_indices:\n",
    "#             feat_list.append((i, out))\n",
    "#     for idx, feat in feat_list:\n",
    "#         print(f\"Selected feature index {idx} -> {feat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:41:15.632563Z",
     "iopub.status.busy": "2025-12-10T17:41:15.632316Z",
     "iopub.status.idle": "2025-12-10T17:41:15.669527Z",
     "shell.execute_reply": "2025-12-10T17:41:15.668894Z",
     "shell.execute_reply.started": "2025-12-10T17:41:15.632545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     feat_dict = {str(i): f for i, (_, f) in enumerate(feat_list)}\n",
    "#     fpn_out = backbone_fpn.fpn(feat_dict)\n",
    "#     print(\"---- FPN outputs ----\")\n",
    "#     for k in sorted(fpn_out.keys(), key=int):\n",
    "#         print(f\"FPN level {k} -> {fpn_out[k].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-10T17:42:49.292771Z",
     "iopub.status.busy": "2025-12-10T17:42:49.292115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Mask2FormerForgeryModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Instance-Seg Transformer (Mask2Former-style) + Authenticity Gate baseline.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_queries=15,\n",
    "        d_model=256,\n",
    "        nhead=8,\n",
    "        num_decoder_layers=6,\n",
    "        mask_dim=256,\n",
    "        backbone_trainable=True,\n",
    "        authenticity_penalty_weight=5.0,\n",
    "        auth_gate_forged_threshold=0.5,  # new\n",
    "        default_mask_threshold=0.5,      # optional, for masks\n",
    "        default_cls_threshold=0.5,       # optional, for per-query forgery\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "        self.d_model = d_model\n",
    "        self.mask_dim = mask_dim\n",
    "        self.authenticity_penalty_weight = authenticity_penalty_weight\n",
    "\n",
    "        # Backbone + FPN\n",
    "        self.backbone = ConvNeXtFPNBackbone(out_channels=d_model, train_backbone=backbone_trainable)\n",
    "\n",
    "        # Transformer decoder\n",
    "        self.position_encoding = PositionEmbeddingSine(d_model // 2, normalize=True)\n",
    "        self.transformer_decoder = SimpleTransformerDecoder(\n",
    "            d_model=256,\n",
    "            nhead=8,\n",
    "            num_layers=6,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            num_queries=num_queries,\n",
    "        )\n",
    "\n",
    "        # Pixel decoder: project highest-res FPN level to mask feature space\n",
    "        self.mask_feature_proj = nn.Conv2d(d_model, mask_dim, kernel_size=1)\n",
    "\n",
    "        # Instance heads\n",
    "        self.class_head = nn.Linear(d_model, 1)  # forgery vs ignore, per query\n",
    "        self.mask_embed_head = nn.Linear(d_model, mask_dim)\n",
    "\n",
    "        # Image-level authenticity head (global pooled high-level feat)\n",
    "        self.img_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "        # Thresholds\n",
    "        self.auth_gate_forged_threshold = auth_gate_forged_threshold\n",
    "        self.default_mask_threshold = default_mask_threshold\n",
    "        self.default_cls_threshold = default_cls_threshold\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        \"\"\"\n",
    "        images: Tensor [B, 3, H, W]\n",
    "        targets: list[dict], each with:\n",
    "          - 'masks': [N_gt, H, W] binary mask tensor\n",
    "          - 'image_label': scalar tensor 0 (authentic) or 1 (forged)\n",
    "        Returns:\n",
    "          - if training: dict of losses\n",
    "          - if inference: list[dict] with masks, mask_scores, mask_forgery_scores, image_authenticity\n",
    "        \"\"\"\n",
    "        if isinstance(images, list):\n",
    "            images = torch.stack(images, dim=0)\n",
    "\n",
    "        B = images.shape[0]\n",
    "\n",
    "        # Backbone + FPN\n",
    "        fpn_feats = self.backbone(images)  # [P2, P3, P4, P5]\n",
    "        # Use highest-res level (P2) for mask features\n",
    "        mask_feats = self.mask_feature_proj(fpn_feats[0])  # [B, mask_dim, Hm, Wm]\n",
    "        pos_list = [self.position_encoding(x) for x in fpn_feats]\n",
    "\n",
    "        # Transformer on multi-scale features\n",
    "        hs_all = self.transformer_decoder(fpn_feats, pos_list)  # [num_layers, B, Q, C]\n",
    "        hs = hs_all[-1]                                         # last layer: [B, Q, C]\n",
    "\n",
    "        # Heads\n",
    "        class_logits    = self.class_head(hs).squeeze(-1)       # [B, Q]\n",
    "        mask_embeddings = self.mask_embed_head(hs)              # [B, Q, mask_dim]\n",
    "\n",
    "\n",
    "        # Produce mask logits via dot-product\n",
    "        # mask_feats: [B, mask_dim, Hm, Wm]; mask_emb: [B, Q, mask_dim]\n",
    "        mask_logits = torch.einsum(\"bqc, bchw -> bqhw\", mask_embeddings, mask_feats)\n",
    "\n",
    "        # Image-level authenticity from highest-level FPN feature (P5)\n",
    "        high_level_feat = fpn_feats[-1]  # [B, C, Hh, Wh]\n",
    "        img_logits = self.img_head(high_level_feat)  # [B, 1]\n",
    "        img_logits = img_logits.squeeze(-1)          # [B]\n",
    "\n",
    "        if targets is not None:\n",
    "            return self.compute_losses(mask_logits, class_logits, img_logits, targets)\n",
    "        else:\n",
    "            return self.inference(mask_logits, class_logits, img_logits)\n",
    "\n",
    "    # ------------------- Losses & matching -------------------\n",
    "\n",
    "    def compute_losses(self, mask_logits, class_logits, img_logits, targets):\n",
    "        \"\"\"\n",
    "        mask_logits: [B, Q, Hm, Wm]\n",
    "        class_logits: [B, Q]  (forgery vs ignore)\n",
    "        img_logits: [B]\n",
    "        targets: list[dict]\n",
    "        \"\"\"\n",
    "        B, Q, Hm, Wm = mask_logits.shape\n",
    "\n",
    "        # Hungarian matching per image\n",
    "        indices = []\n",
    "        for b in range(B):\n",
    "            tgt_masks = targets[b][\"masks\"]  # [N_gt, H, W] or [0, ...] if authentic\n",
    "            if tgt_masks.numel() == 0:\n",
    "                indices.append((torch.empty(0, dtype=torch.long, device=mask_logits.device),\n",
    "                                torch.empty(0, dtype=torch.long, device=mask_logits.device)))\n",
    "                continue\n",
    "\n",
    "            # Downsample GT masks to mask resolution\n",
    "            tgt_masks_resized = F.interpolate(\n",
    "                tgt_masks.unsqueeze(1).float(),\n",
    "                size=(Hm, Wm),\n",
    "                mode=\"nearest\"\n",
    "            ).squeeze(1)  # [N_gt, Hm, Wm]\n",
    "\n",
    "            pred = mask_logits[b]  # [Q, Hm, Wm]\n",
    "            cost = self.match_cost(pred, tgt_masks_resized)  # [N_gt, Q]\n",
    "\n",
    "            tgt_ind, pred_ind = linear_sum_assignment(cost.detach().cpu().numpy())\n",
    "            tgt_ind = torch.as_tensor(tgt_ind, dtype=torch.long, device=mask_logits.device)\n",
    "            pred_ind = torch.as_tensor(pred_ind, dtype=torch.long, device=mask_logits.device)\n",
    "            indices.append((pred_ind, tgt_ind))\n",
    "\n",
    "        # Mask losses (Dice + BCE) on matched pairs\n",
    "        loss_mask = 0.0\n",
    "        loss_dice = 0.0\n",
    "        num_instances = 0\n",
    "\n",
    "        for b in range(B):\n",
    "            pred_ind, tgt_ind = indices[b]\n",
    "            if len(pred_ind) == 0:\n",
    "                continue\n",
    "\n",
    "            tgt_masks = targets[b][\"masks\"]\n",
    "            tgt_masks_resized = F.interpolate(\n",
    "                tgt_masks.unsqueeze(1).float(),\n",
    "                size=(Hm, Wm),\n",
    "                mode=\"nearest\"\n",
    "            ).squeeze(1)  # [N_gt, Hm, Wm]\n",
    "\n",
    "            pred_masks = mask_logits[b, pred_ind]        # [M, Hm, Wm]\n",
    "            gt_masks = tgt_masks_resized[tgt_ind]        # [M, Hm, Wm]\n",
    "\n",
    "            loss_mask += self.sigmoid_ce_loss(pred_masks, gt_masks).sum()\n",
    "            loss_dice += self.dice_loss(pred_masks, gt_masks).sum()\n",
    "            num_instances += pred_ind.numel()\n",
    "\n",
    "        if num_instances > 0:\n",
    "            loss_mask = loss_mask / num_instances\n",
    "            loss_dice = loss_dice / num_instances\n",
    "        else:\n",
    "            loss_mask = mask_logits.sum() * 0.0\n",
    "            loss_dice = mask_logits.sum() * 0.0\n",
    "\n",
    "        # Mask-level classification BCE\n",
    "        # All GT instances are \"forgery\" (1). Unmatched predictions are ignore (0).\n",
    "        class_targets = torch.zeros_like(class_logits)  # [B, Q]\n",
    "        for b in range(B):\n",
    "            pred_ind, tgt_ind = indices[b]\n",
    "            if len(pred_ind) > 0:\n",
    "                class_targets[b, pred_ind] = 1.0\n",
    "\n",
    "        loss_cls = F.binary_cross_entropy_with_logits(\n",
    "            class_logits,\n",
    "            class_targets,\n",
    "        )\n",
    "\n",
    "        # Image-level authenticity loss\n",
    "        img_targets = torch.stack([t[\"image_label\"].float() for t in targets]).to(img_logits.device)  # [B]\n",
    "        loss_img = F.binary_cross_entropy_with_logits(img_logits, img_targets)\n",
    "\n",
    "        # Authenticity penalty: if authentic image (y=0) has non-empty predicted forgery mask\n",
    "        with torch.no_grad():\n",
    "            mask_probs = torch.sigmoid(mask_logits)        # [B, Q, Hm, Wm]\n",
    "            cls_probs = torch.sigmoid(class_logits)        # [B, Q]\n",
    "            # Take only masks with high forgery prob\n",
    "            forgery_mask = cls_probs > 0.5                 # [B, Q]\n",
    "\n",
    "        penalty = 0.0\n",
    "        for b in range(B):\n",
    "            if img_targets[b] < 0.5:  # authentic\n",
    "                if mask_logits.shape[1] == 0:\n",
    "                    continue\n",
    "                # Select predicted forgery masks\n",
    "                if forgery_mask[b].any():\n",
    "                    m = mask_probs[b, forgery_mask[b]]  # [K, Hm, Wm]\n",
    "                    # L1 norm as \"area\"\n",
    "                    penalty += m.mean()\n",
    "\n",
    "        if isinstance(penalty, float):\n",
    "            penalty = mask_logits.sum() * 0.0\n",
    "        loss_auth_penalty = self.authenticity_penalty_weight * penalty / max(B, 1)\n",
    "\n",
    "        losses = {\n",
    "            \"loss_mask_bce\": loss_mask,\n",
    "            \"loss_mask_dice\": loss_dice,\n",
    "            \"loss_mask_cls\": loss_cls,\n",
    "            \"loss_img_auth\": loss_img,\n",
    "            \"loss_auth_penalty\": loss_auth_penalty,\n",
    "        }\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def match_cost(self, pred_masks, tgt_masks):\n",
    "        \"\"\"\n",
    "        pred_masks: [Q, H, W]\n",
    "        tgt_masks: [N_gt, H, W]\n",
    "        Returns cost matrix [N_gt, Q]\n",
    "        \"\"\"\n",
    "        Q, H, W = pred_masks.shape\n",
    "        N = tgt_masks.shape[0]\n",
    "\n",
    "        pred_flat = pred_masks.flatten(1)  # [Q, HW]\n",
    "        tgt_flat = tgt_masks.flatten(1)    # [N, HW]\n",
    "\n",
    "        # BCE cost\n",
    "        pred_logits = pred_flat.unsqueeze(0)             # [1, Q, HW]\n",
    "        tgt = tgt_flat.unsqueeze(1)                      # [N, 1, HW]\n",
    "        bce = F.binary_cross_entropy_with_logits(\n",
    "            pred_logits.expand(N, -1, -1),\n",
    "            tgt.expand(-1, Q, -1),\n",
    "            reduction=\"none\",\n",
    "        ).mean(-1)  # [N, Q]\n",
    "\n",
    "        # Dice cost\n",
    "        pred_prob = pred_flat.sigmoid()\n",
    "        numerator = 2 * (pred_prob.unsqueeze(0) * tgt_flat.unsqueeze(1)).sum(-1)\n",
    "        denominator = pred_prob.unsqueeze(0).sum(-1) + tgt_flat.unsqueeze(1).sum(-1) + 1e-6\n",
    "        dice = 1.0 - (numerator + 1e-6) / (denominator)\n",
    "\n",
    "        cost = bce + dice\n",
    "        return cost\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_ce_loss(inputs, targets):\n",
    "        \"\"\"\n",
    "        BCE on logits, per-instance mean.\n",
    "        inputs: [M, H, W], targets: [M, H, W]\n",
    "        \"\"\"\n",
    "        return F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\").mean(dim=(1, 2))\n",
    "\n",
    "    @staticmethod\n",
    "    def dice_loss(inputs, targets, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Soft dice loss on logits.\n",
    "        inputs: [M, H, W], targets: [M, H, W]\n",
    "        \"\"\"\n",
    "        inputs = inputs.sigmoid()\n",
    "        inputs = inputs.flatten(1)\n",
    "        targets = targets.flatten(1)\n",
    "\n",
    "        numerator = 2 * (inputs * targets).sum(1)\n",
    "        denominator = inputs.sum(1) + targets.sum(1) + eps\n",
    "        loss = 1 - (numerator + eps) / (denominator)\n",
    "        return loss\n",
    "\n",
    "    # ------------------- Inference -------------------\n",
    "\n",
    "    def inference(\n",
    "        self,\n",
    "        mask_logits,\n",
    "        class_logits,\n",
    "        img_logits,\n",
    "        mask_threshold=None,\n",
    "        cls_threshold=None,\n",
    "        auth_gate_forged_threshold=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns list of dicts per image:\n",
    "          - 'masks': [K, Hm, Wm] uint8\n",
    "          - 'mask_scores': [K]\n",
    "          - 'mask_forgery_scores': [K]\n",
    "          - 'image_authenticity': float in [0,1], prob of \"forged\"\n",
    "        Authenticity gate: if prob(authentic) > 0.5, masks list is empty.\n",
    "        \"\"\"\n",
    "        B, Q, Hm, Wm = mask_logits.shape\n",
    "        mask_probs = torch.sigmoid(mask_logits)\n",
    "        cls_probs = torch.sigmoid(class_logits)\n",
    "        img_probs = torch.sigmoid(img_logits)  # prob \"forged\"\n",
    "\n",
    "            # fall back to model defaults\n",
    "        if mask_threshold is None:\n",
    "            mask_threshold = self.default_mask_threshold\n",
    "        if cls_threshold is None:\n",
    "            cls_threshold = self.default_cls_threshold\n",
    "        if auth_gate_forged_threshold is None:\n",
    "            auth_gate_forged_threshold = self.auth_gate_forged_threshold\n",
    "\n",
    "        outputs = []\n",
    "        for b in range(B):\n",
    "            forged_prob = img_probs[b].item()\n",
    "    \n",
    "            # gate based directly on forged probability\n",
    "            if forged_prob < auth_gate_forged_threshold:\n",
    "                outputs.append({\n",
    "                    \"masks\": torch.zeros((0, Hm, Wm), dtype=torch.uint8, device=mask_logits.device),\n",
    "                    \"mask_scores\": torch.empty(0, device=mask_logits.device),\n",
    "                    \"mask_forgery_scores\": torch.empty(0, device=mask_logits.device),\n",
    "                    \"image_authenticity\": forged_prob,\n",
    "                })\n",
    "                continue\n",
    "    \n",
    "            keep = cls_probs[b] > cls_threshold\n",
    "            if keep.sum() == 0:\n",
    "                outputs.append({\n",
    "                    \"masks\": torch.zeros((0, Hm, Wm), dtype=torch.uint8, device=mask_logits.device),\n",
    "                    \"mask_scores\": torch.empty(0, device=mask_logits.device),\n",
    "                    \"mask_forgery_scores\": torch.empty(0, device=mask_logits.device),\n",
    "                    \"image_authenticity\": forged_prob,\n",
    "                })\n",
    "                continue\n",
    "    \n",
    "            masks_b = (mask_probs[b, keep] > mask_threshold).to(torch.uint8)\n",
    "            scores_b = mask_probs[b, keep].flatten(1).mean(-1)\n",
    "            cls_b = cls_probs[b, keep]\n",
    "    \n",
    "            outputs.append({\n",
    "                \"masks\": masks_b,\n",
    "                \"mask_scores\": scores_b,\n",
    "                \"mask_forgery_scores\": cls_b,\n",
    "                \"image_authenticity\": forged_prob,\n",
    "            })\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "model = Mask2FormerForgeryModel(\n",
    "    num_queries=15,\n",
    "    d_model=256,\n",
    "    auth_gate_forged_threshold=0.8,   # only show masks if forged_prob > 0.8\n",
    "    default_cls_threshold=0.7,        # only keep high-confidence queries\n",
    "    default_mask_threshold=0.5,\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "for images, targets in train_loader:\n",
    "    images = [img.to(device) for img in images]\n",
    "    for t in targets:\n",
    "        t['masks'] = t['masks'].to(device)\n",
    "        t['image_label'] = t['image_label'].to(device)\n",
    "\n",
    "    loss_dict = model(images, targets)\n",
    "    loss = sum(loss_dict.values())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T17:42:42.652518Z",
     "iopub.status.busy": "2025-12-10T17:42:42.651955Z",
     "iopub.status.idle": "2025-12-10T17:42:42.957788Z",
     "shell.execute_reply": "2025-12-10T17:42:42.956737Z",
     "shell.execute_reply.started": "2025-12-10T17:42:42.652492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "[Model] num_queries: 15\n",
      "\n",
      "[Encoder / FPN]\n",
      "  P0: (2, 256, 32, 32)\n",
      "  P1: (2, 256, 16, 16)\n",
      "  P2: (2, 256, 8, 8)\n",
      "  P3: (2, 256, 4, 4)\n",
      "\n",
      "[Decoder]\n",
      "  hs_all shape (num_layers, B, Q, C): (6, 2, 15, 256)\n",
      "  hs last layer shape (B, Q, C): (2, 15, 256)\n",
      "\n",
      "[Forward + losses (train mode)]\n",
      "  loss_mask_bce: 1.4509\n",
      "  loss_mask_dice: 0.9042\n",
      "  loss_mask_cls: 0.6620\n",
      "  loss_img_auth: 0.6905\n",
      "  loss_auth_penalty: 2.1290\n",
      "  total loss: 5.836508750915527\n",
      "\n",
      "[match_cost]\n",
      "  cost shape (N_gt, Q): (3, 4)\n",
      "\n",
      "[Forward (eval mode / inference)]\n",
      "  batch size: 2\n",
      "  sample 0:\n",
      "    masks: (6, 32, 32)\n",
      "    mask_scores: (6,)\n",
      "    mask_forgery_scores: (6,)\n",
      "    image_authenticity: 0.5009550452232361\n",
      "  sample 1:\n",
      "    masks: (5, 32, 32)\n",
      "    mask_scores: (5,)\n",
      "    mask_forgery_scores: (5,)\n",
      "    image_authenticity: 0.501069188117981\n",
      "\n",
      "[Train step]\n",
      "  train_loss: 7.140699863433838\n",
      "\n",
      "[Val step]\n",
      "  val_loss: 8.13501262664795\n",
      "\n",
      "[Done] All main components ran without shape errors (if you see this, shapes are consistent).\n"
     ]
    }
   ],
   "source": [
    "# ==== Sanity / shape checks for encoder/decoder/model/loss/train/val ====\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Small dummy batch\n",
    "B, C, H, W = 2, 3, 128, 128\n",
    "dummy_imgs = torch.randn(B, C, H, W, device=device)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) Instantiate model\n",
    "# -------------------------------------------------------------------------\n",
    "model = Mask2FormerForgeryModel(num_queries=15, d_model=256).to(device)\n",
    "model.eval()\n",
    "print(\"\\n[Model] num_queries:\", model.num_queries)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2) Encoder: ConvNeXt + FPN features\n",
    "# -------------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    fpn_feats = model.backbone(dummy_imgs)  # list of feature maps\n",
    "\n",
    "print(\"\\n[Encoder / FPN]\")\n",
    "for i, f in enumerate(fpn_feats):\n",
    "    print(f\"  P{i}: {tuple(f.shape)}\")\n",
    "assert len(fpn_feats) >= 1, \"Backbone/FPN returned no feature maps\"\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3) Positional encodings + Transformer decoder\n",
    "# -------------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    pos_list = [model.position_encoding(f) for f in fpn_feats]\n",
    "    hs_all = model.transformer_decoder(fpn_feats, pos_list)  # [num_layers, B, Q, C]\n",
    "    hs = hs_all[-1]                                          # [B, Q, C]\n",
    "\n",
    "print(\"\\n[Decoder]\")\n",
    "print(\"  hs_all shape (num_layers, B, Q, C):\", tuple(hs_all.shape))\n",
    "print(\"  hs last layer shape (B, Q, C):\", tuple(hs.shape))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4) Build dummy targets and run full forward (training mode -> losses)\n",
    "# -------------------------------------------------------------------------\n",
    "dummy_targets = []\n",
    "\n",
    "# sample 0: forged image with 2 GT instances\n",
    "masks0 = torch.zeros(2, H, W, device=device)\n",
    "masks0[0, H // 4 : H // 2, W // 4 : W // 2] = 1\n",
    "masks0[1, H // 2 : 3 * H // 4, W // 2 : 3 * W // 4] = 1\n",
    "dummy_targets.append({\n",
    "    \"masks\": masks0,\n",
    "    \"image_label\": torch.tensor(1.0, device=device),\n",
    "})\n",
    "\n",
    "# sample 1: authentic image with no GT masks\n",
    "masks1 = torch.zeros(0, H, W, device=device)\n",
    "dummy_targets.append({\n",
    "    \"masks\": masks1,\n",
    "    \"image_label\": torch.tensor(0.0, device=device),\n",
    "})\n",
    "\n",
    "model.train()\n",
    "loss_dict = model(dummy_imgs, dummy_targets)\n",
    "print(\"\\n[Forward + losses (train mode)]\")\n",
    "for k, v in loss_dict.items():\n",
    "    print(f\"  {k}: {float(v):.4f}\")\n",
    "total_loss = sum(loss_dict.values())\n",
    "print(\"  total loss:\", float(total_loss))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5) Direct loss helper: match_cost sanity\n",
    "# -------------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    pred_masks = torch.randn(4, H, W, device=device)                # Q=4\n",
    "    tgt_masks = (torch.rand(3, H, W, device=device) > 0.5).float()  # N_gt=3\n",
    "    cost = model.match_cost(pred_masks, tgt_masks)\n",
    "\n",
    "print(\"\\n[match_cost]\")\n",
    "print(\"  cost shape (N_gt, Q):\", tuple(cost.shape))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6) Inference forward (no targets) to check outputs\n",
    "# -------------------------------------------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(dummy_imgs, targets=None)\n",
    "\n",
    "print(\"\\n[Forward (eval mode / inference)]\")\n",
    "print(\"  batch size:\", len(outputs))\n",
    "for b, out in enumerate(outputs):\n",
    "    print(f\"  sample {b}:\")\n",
    "    print(\"    masks:\", tuple(out[\"masks\"].shape))\n",
    "    print(\"    mask_scores:\", tuple(out[\"mask_scores\"].shape))\n",
    "    print(\"    mask_forgery_scores:\", tuple(out[\"mask_forgery_scores\"].shape))\n",
    "    print(\"    image_authenticity:\", float(out[\"image_authenticity\"]))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 7) One dummy train step\n",
    "# -------------------------------------------------------------------------\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss_dict = model(dummy_imgs, dummy_targets)\n",
    "train_loss = sum(loss_dict.values())\n",
    "train_loss.backward()\n",
    "optimizer.step()\n",
    "print(\"\\n[Train step]\")\n",
    "print(\"  train_loss:\", float(train_loss))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 8) One dummy val step (no grad)\n",
    "# -------------------------------------------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_loss_dict = model(dummy_imgs, dummy_targets)\n",
    "val_loss = sum(val_loss_dict.values())\n",
    "print(\"\\n[Val step]\")\n",
    "print(\"  val_loss:\", float(val_loss))\n",
    "\n",
    "print(\"\\n[Done] All main components ran without shape errors (if you see this, shapes are consistent).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    \"\"\"\n",
    "    Encode binary mask to RLE in the format required by the competition.\n",
    "    Returns a JSON string like \"[123,4,567,8]\"\n",
    "    \"\"\"\n",
    "    # Ensure mask is 2D and binary\n",
    "    mask = mask.astype(bool)\n",
    "    \n",
    "    # Flatten in Fortran order\n",
    "    flat = mask.T.flatten()\n",
    "    \n",
    "    # Find indices where value is True\n",
    "    dots = np.where(flat)[0]\n",
    "    \n",
    "    if len(dots) == 0:\n",
    "        return json.dumps([])  # or just return 'authentic' upstream\n",
    "    \n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend([b + 1, 0])  # 1-based index\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    \n",
    "    # Convert numpy ints to Python ints for JSON compatibility\n",
    "    run_lengths = [int(x) for x in run_lengths]\n",
    "    return json.dumps(run_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built solution_df with 5176 rows\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Fold 1 Epoch 1/3 - train loss: 2.0240\n",
      "Fold 1 Epoch 2/3 - train loss: 1.8656\n",
      "Fold 1 Epoch 3/3 - train loss: 1.7527\n",
      "Fold 1 metric: 0.459459\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Fold 2 Epoch 1/3 - train loss: 1.9908\n",
      "Fold 2 Epoch 2/3 - train loss: 1.8540\n",
      "Fold 2 Epoch 3/3 - train loss: 1.7649\n",
      "Fold 2 metric: 0.459903\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Fold 3 Epoch 1/3 - train loss: 1.9745\n",
      "Fold 3 Epoch 2/3 - train loss: 1.8513\n",
      "Fold 3 Epoch 3/3 - train loss: 1.8102\n",
      "Fold 3 metric: 0.458937\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Fold 4 Epoch 1/3 - train loss: 2.0283\n",
      "Fold 4 Epoch 2/3 - train loss: 1.8882\n",
      "Fold 4 Epoch 3/3 - train loss: 1.7566\n",
      "Fold 4 metric: 0.458937\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Fold 5 Epoch 1/3 - train loss: 1.9746\n",
      "Fold 5 Epoch 2/3 - train loss: 1.7615\n",
      "Fold 5 Epoch 3/3 - train loss: 1.7212\n",
      "Fold 5 metric: 0.458937\n",
      "\n",
      "Per-fold scores: [0.4594594594594595, 0.4599033816425121, 0.45893719806763283, 0.45893719806763283, 0.45893719806763283]\n",
      "Mean CV: 0.45923488706097404\n",
      "OOF score: 0.45923493044822256\n"
     ]
    }
   ],
   "source": [
    "# ===== CV + OOF PREDICTIONS WITH OFFICIAL METRIC =====\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from kaggle_metric import score as kaggle_score\n",
    "\n",
    "# --- Build meta + GT solution dataframe from the training masks ---\n",
    "\n",
    "n_samples = len(full_dataset)\n",
    "image_ids = [full_dataset.samples[i]['image_id'] for i in range(n_samples)]\n",
    "is_forged = np.array([1 if full_dataset.samples[i]['is_forged'] else 0\n",
    "                      for i in range(n_samples)])\n",
    "\n",
    "gt_annotations = []\n",
    "gt_shapes = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    sample = full_dataset.samples[i]\n",
    "    img_path = sample['image_path']\n",
    "    mask_path = sample['mask_path']\n",
    "\n",
    "    # Default shape from image\n",
    "    with Image.open(img_path) as pil_img:\n",
    "        w, h = pil_img.size\n",
    "\n",
    "    if sample['is_forged'] and os.path.exists(mask_path):\n",
    "        m = np.load(mask_path)\n",
    "\n",
    "        # Same handling as in dataset\n",
    "        if m.ndim == 3 and m.shape[0] <= 10 and m.shape[1:] == (h, w):\n",
    "            m = np.any(m, axis=0)\n",
    "        m = (m > 0).astype(np.uint8)\n",
    "\n",
    "        gt_annotations.append(rle_encode(m))\n",
    "    else:\n",
    "        gt_annotations.append(\"authentic\")\n",
    "\n",
    "    gt_shapes.append(json.dumps([h, w]))  # e.g. \"[720, 960]\"\n",
    "\n",
    "solution_df = pd.DataFrame({\n",
    "    \"row_id\": image_ids,\n",
    "    \"annotation\": gt_annotations,\n",
    "    \"shape\": gt_shapes,\n",
    "})\n",
    "\n",
    "print(\"Built solution_df with\", len(solution_df), \"rows\")\n",
    "\n",
    "# --- K-fold CV with OOF predictions ---\n",
    "\n",
    "N_FOLDS = 5\n",
    "NUM_EPOCHS = 3          # adjust for real training\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_pred = [None] * n_samples\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(n_samples), is_forged)):\n",
    "    print(f\"\\n===== Fold {fold + 1}/{N_FOLDS} =====\")\n",
    "    train_idx = train_idx.tolist()\n",
    "    val_idx = val_idx.tolist()\n",
    "\n",
    "    # Fresh datasets so transforms are independent\n",
    "    ds_train = ForgeryDataset(\n",
    "        paths[\"train_authentic\"],\n",
    "        paths[\"train_forged\"],\n",
    "        paths[\"train_masks\"],\n",
    "        supp_forged_path=paths[\"supp_forged\"],\n",
    "        supp_masks_path=paths[\"supp_masks\"],\n",
    "        transform=train_transform,\n",
    "    )\n",
    "    ds_val = ForgeryDataset(\n",
    "        paths[\"train_authentic\"],\n",
    "        paths[\"train_forged\"],\n",
    "        paths[\"train_masks\"],\n",
    "        supp_forged_path=paths[\"supp_forged\"],\n",
    "        supp_masks_path=paths[\"supp_masks\"],\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        torch.utils.data.Subset(ds_train, train_idx),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: tuple(zip(*x)),\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        torch.utils.data.Subset(ds_val, val_idx),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: tuple(zip(*x)),\n",
    "    )\n",
    "\n",
    "    # Model + optimizer per fold\n",
    "    model = Mask2FormerForgeryModel(\n",
    "        num_queries=15,\n",
    "        d_model=256,\n",
    "        authenticity_penalty_weight=5.0,\n",
    "        auth_gate_forged_threshold=0.5,\n",
    "        default_mask_threshold=0.5,\n",
    "        default_cls_threshold=0.5,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    # ---- Train ----\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, targets in train_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "\n",
    "            for t in targets:\n",
    "                t[\"masks\"] = t[\"masks\"].to(device)\n",
    "                t[\"image_label\"] = t[\"image_label\"].to(device)\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * len(images)\n",
    "\n",
    "        running_loss /= len(train_idx)\n",
    "        print(f\"Fold {fold + 1} Epoch {epoch + 1}/{NUM_EPOCHS} - train loss: {running_loss:.4f}\")\n",
    "\n",
    "    # ---- Inference on validation fold (OOF) ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)  # calls model.inference(...)\n",
    "\n",
    "            for out, t in zip(outputs, targets):\n",
    "                # Global dataset index (same as in full_dataset.samples)\n",
    "                global_idx = int(t[\"image_id\"].item())\n",
    "                sample = full_dataset.samples[global_idx]\n",
    "                img_path = sample[\"image_path\"]\n",
    "                mask_path = sample[\"mask_path\"]\n",
    "\n",
    "                # Original H, W\n",
    "                if os.path.exists(mask_path):\n",
    "                    m = np.load(mask_path)\n",
    "                    if m.ndim == 3:\n",
    "                        h, w = m.shape[1], m.shape[2]\n",
    "                    else:\n",
    "                        h, w = m.shape[0], m.shape[1]\n",
    "                else:\n",
    "                    with Image.open(img_path) as pil_img:\n",
    "                        w, h = pil_img.size\n",
    "\n",
    "                # Convert model outputs to competition-style annotation\n",
    "                if out[\"masks\"].numel() == 0:\n",
    "                    # authenticity gate said authentic\n",
    "                    pred_ann = \"authentic\"\n",
    "                else:\n",
    "                    # union all predicted masks then upsample to original size\n",
    "                    union_small = (out[\"masks\"] > 0).any(dim=0).float()  # [H', W']\n",
    "                    union_small = union_small.unsqueeze(0).unsqueeze(0)  # [1,1,H',W']\n",
    "                    union_up = F.interpolate(\n",
    "                        union_small,\n",
    "                        size=(h, w),\n",
    "                        mode=\"nearest\",\n",
    "                    ).squeeze().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "                    pred_ann = rle_encode(union_up)\n",
    "\n",
    "                oof_pred[global_idx] = pred_ann\n",
    "\n",
    "    # ---- Fold metric using official score ----\n",
    "    fold_solution = solution_df.iloc[val_idx].reset_index(drop=True)\n",
    "    fold_submission = pd.DataFrame({\n",
    "        \"row_id\": fold_solution[\"row_id\"],\n",
    "        \"annotation\": [oof_pred[i] for i in val_idx],\n",
    "    })\n",
    "\n",
    "    fold_score = kaggle_score(\n",
    "        fold_solution.copy(),\n",
    "        fold_submission.copy(),\n",
    "        row_id_column_name=\"row_id\",\n",
    "    )\n",
    "    fold_scores.append(fold_score)\n",
    "    print(f\"Fold {fold + 1} metric: {fold_score:.6f}\")\n",
    "\n",
    "# --- Overall OOF metric ---\n",
    "oof_solution = solution_df.copy()\n",
    "oof_submission = pd.DataFrame({\n",
    "    \"row_id\": oof_solution[\"row_id\"],\n",
    "    \"annotation\": oof_pred,\n",
    "})\n",
    "\n",
    "oof_score = kaggle_score(\n",
    "    oof_solution.copy(),\n",
    "    oof_submission.copy(),\n",
    "    row_id_column_name=\"row_id\",\n",
    ")\n",
    "\n",
    "print(\"\\nPer-fold scores:\", fold_scores)\n",
    "print(\"Mean CV:\", np.mean(fold_scores))\n",
    "print(\"OOF score:\", oof_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on FULL DATASET: 5176 samples\n",
      "Epoch 1/30 - Loss: 1.9821\n",
      "Epoch 2/30 - Loss: 1.8129\n",
      "Epoch 3/30 - Loss: 1.6722\n",
      "Epoch 4/30 - Loss: 1.5795\n",
      "Epoch 5/30 - Loss: 1.5375\n",
      "Epoch 6/30 - Loss: 1.4495\n",
      "Epoch 7/30 - Loss: 1.4203\n",
      "Epoch 8/30 - Loss: 1.3912\n",
      "Epoch 9/30 - Loss: 1.3667\n",
      "Epoch 10/30 - Loss: 1.3414\n",
      "Epoch 11/30 - Loss: 1.3428\n",
      "Epoch 12/30 - Loss: 1.3492\n",
      "Epoch 13/30 - Loss: 1.2749\n",
      "Epoch 14/30 - Loss: 1.2196\n",
      "Epoch 15/30 - Loss: 1.2270\n",
      "Epoch 16/30 - Loss: 1.1983\n",
      "Epoch 17/30 - Loss: 1.2178\n",
      "Epoch 18/30 - Loss: 1.1148\n",
      "Epoch 19/30 - Loss: 1.1074\n",
      "Epoch 20/30 - Loss: 1.0698\n",
      "Epoch 21/30 - Loss: 1.0877\n",
      "Epoch 22/30 - Loss: 1.0461\n",
      "Epoch 23/30 - Loss: 1.0367\n",
      "Epoch 24/30 - Loss: 1.0174\n",
      "Epoch 25/30 - Loss: 0.9867\n",
      "Epoch 26/30 - Loss: 0.9968\n",
      "Epoch 27/30 - Loss: 0.9750\n",
      "Epoch 28/30 - Loss: 0.9560\n",
      "Epoch 29/30 - Loss: 0.9358\n",
      "Epoch 30/30 - Loss: 0.9226\n",
      "Model weights saved to: model_full_data_baseline.pth\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Hyperparameters\n",
    "# -----------------------------\n",
    "NUM_EPOCHS = 30             # adjust as needed\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_PATH = \"model_full_data_baseline.pth\"\n",
    "\n",
    "# -----------------------------\n",
    "# Full dataset (same transforms as CV)\n",
    "# -----------------------------\n",
    "train_dataset = full_dataset     \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Model\n",
    "# -----------------------------\n",
    "model = Mask2FormerForgeryModel(\n",
    "    num_queries=15,\n",
    "    d_model=256,\n",
    "    authenticity_penalty_weight=5.0,\n",
    "    auth_gate_forged_threshold=0.5,\n",
    "    default_mask_threshold=0.5,\n",
    "    default_cls_threshold=0.5,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Training Loop\n",
    "# -----------------------------\n",
    "print(\"Training on FULL DATASET:\", len(train_dataset), \"samples\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        for t in targets:\n",
    "            t[\"masks\"] = t[\"masks\"].to(device)\n",
    "            t[\"image_label\"] = t[\"image_label\"].to(device)\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * len(images)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Save Weights\n",
    "# -----------------------------\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"Model weights saved to: {SAVE_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14456136,
     "sourceId": 113558,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
