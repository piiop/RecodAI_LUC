{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OOF_DIR = \"../experiments/oof_results\"\n",
    "LB_LOG_PATH = \"../experiments/lb_scores.csv\"\n",
    "\n",
    "if not os.path.exists(\"../experiments\"):\n",
    "    os.makedirs(\"../experiments\")\n",
    "\n",
    "# Initialize LB log if missing\n",
    "if not os.path.isfile(LB_LOG_PATH):\n",
    "    pd.DataFrame([\n",
    "        {\"run_name\": None, \"mean_cv\": None, \"oof_score\": None, \"kaggle_lb\": None}\n",
    "    ]).to_csv(LB_LOG_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115aa201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oof_metrics(oof_dir=OOF_DIR):\n",
    "    out = []\n",
    "    for root, dirs, files in os.walk(oof_dir):\n",
    "        if \"oof_metrics.json\" in files:\n",
    "            name = os.path.basename(root)\n",
    "            with open(os.path.join(root, \"oof_metrics.json\"), \"r\") as f:\n",
    "                m = json.load(f)\n",
    "            out.append({\n",
    "                \"run_name\": name,\n",
    "                \"mean_cv\": m.get(\"mean_cv\"),\n",
    "                \"oof_score\": m.get(\"oof_score\")\n",
    "            })\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "cv_df = load_oof_metrics()\n",
    "cv_df.sort_values(\"mean_cv\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18920d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = input(\"Run/experiment name: \").strip()\n",
    "lb_score = float(input(\"Kaggle LB score: \"))\n",
    "\n",
    "lb_df = pd.read_csv(LB_LOG_PATH)\n",
    "\n",
    "# Pull the matching experiment from CV results\n",
    "match = cv_df[cv_df.run_name == run_name]\n",
    "if len(match) == 0:\n",
    "    print(\"⚠️ No local run found. Logging LB only.\")\n",
    "    new_row = {\n",
    "        \"run_name\": run_name,\n",
    "        \"mean_cv\": None,\n",
    "        \"oof_score\": None,\n",
    "        \"kaggle_lb\": lb_score\n",
    "    }\n",
    "else:\n",
    "    m = match.iloc[0]\n",
    "    new_row = {\n",
    "        \"run_name\": run_name,\n",
    "        \"mean_cv\": m.mean_cv,\n",
    "        \"oof_score\": m.oof_score,\n",
    "        \"kaggle_lb\": lb_score\n",
    "    }\n",
    "\n",
    "# Update CSV\n",
    "lb_df = pd.concat([lb_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "lb_df.to_csv(LB_LOG_PATH, index=False)\n",
    "\n",
    "print(\"Logged:\")\n",
    "new_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666db972",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_df = pd.read_csv(LB_LOG_PATH).dropna(subset=[\"run_name\"], how=\"all\")\n",
    "lb_df.sort_values(\"kaggle_lb\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171424ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lb_df.dropna(subset=[\"mean_cv\", \"kaggle_lb\"])\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(df[\"mean_cv\"], df[\"kaggle_lb\"])\n",
    "plt.xlabel(\"Local Mean CV Score\")\n",
    "plt.ylabel(\"Kaggle LB Score\")\n",
    "plt.title(\"Local CV vs Kaggle LB\")\n",
    "for _, r in df.iterrows():\n",
    "    plt.annotate(r.run_name, (r.mean_cv, r.kaggle_lb))\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_df.sort_values(\"kaggle_lb\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lb_df.copy()\n",
    "df[\"delta_lb_cv\"] = df[\"kaggle_lb\"] - df[\"mean_cv\"]\n",
    "df.sort_values(\"delta_lb_cv\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0785f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"baseline\"  # or \"lr\", \"bs\", etc.\n",
    "lb_df[lb_df.run_name.str.contains(prefix, na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kagglelb_analysis.ipynb — setup\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# where we'll store the running log of LB submissions\n",
    "LB_LOG_PATH = Path(\"experiments/kaggle_lb_scores.csv\")\n",
    "\n",
    "# where your gate sweep summary was saved\n",
    "GATE_SWEEP_SUMMARY_PATH = Path(\"experiments/auth_gate_sweep/auth_gate_sweep_summary.json\")\n",
    "\n",
    "# identify the model you’re using for submissions\n",
    "MODEL_ID = \"full_baseline_v1\"\n",
    "WEIGHTS_PATH = \"weights/full_train/model_full_data_baseline.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gate_sweep_summary(summary_path: Path = GATE_SWEEP_SUMMARY_PATH) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load auth_gate_sweep_summary.json into a DataFrame with columns:\n",
    "    ['gate', 'local_score', 'csv_path'].\n",
    "    \"\"\"\n",
    "    if not summary_path.is_file():\n",
    "        raise FileNotFoundError(f\"No gate sweep summary found at {summary_path}\")\n",
    "\n",
    "    with summary_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.rename(columns={\"score\": \"local_score\"})\n",
    "    df = df.sort_values(\"gate\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "gate_sweep_df = load_gate_sweep_summary()\n",
    "gate_sweep_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c15ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lb_log(path: Path = LB_LOG_PATH) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load existing Kaggle LB log or create an empty one with the correct schema.\n",
    "    \"\"\"\n",
    "    if path.is_file():\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "    cols = [\n",
    "        \"timestamp\",         # ISO timestamp when you logged it\n",
    "        \"submission_name\",   # Kaggle submission name you used\n",
    "        \"model_id\",          # e.g. full_baseline_v1\n",
    "        \"weights_path\",      # where the weights came from\n",
    "        \"auth_gate\",         # gate used for this submission\n",
    "        \"local_score\",       # from gate sweep\n",
    "        \"lb_score\",          # Kaggle public LB score\n",
    "        \"notes\",             # free text\n",
    "    ]\n",
    "    return pd.DataFrame(columns=cols)\n",
    "\n",
    "lb_df = load_lb_log()\n",
    "lb_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d924a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_kaggle_submission(\n",
    "    submission_name: str,\n",
    "    auth_gate: float,\n",
    "    lb_score: float,\n",
    "    local_score: float | None = None,\n",
    "    notes: str = \"\",\n",
    "    model_id: str = MODEL_ID,\n",
    "    weights_path: str = WEIGHTS_PATH,\n",
    "    path: Path = LB_LOG_PATH,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Append a new row to the Kaggle LB log and save it.\n",
    "    \"\"\"\n",
    "    df = load_lb_log(path)\n",
    "\n",
    "    # if local_score not provided, try to look it up from gate_sweep_df\n",
    "    if local_score is None:\n",
    "        try:\n",
    "            match = gate_sweep_df.loc[gate_sweep_df[\"gate\"] == auth_gate]\n",
    "            if not match.empty:\n",
    "                local_score = float(match[\"local_score\"].iloc[0])\n",
    "        except Exception:\n",
    "            local_score = None\n",
    "\n",
    "    new_row = {\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"submission_name\": submission_name,\n",
    "        \"model_id\": model_id,\n",
    "        \"weights_path\": weights_path,\n",
    "        \"auth_gate\": auth_gate,\n",
    "        \"local_score\": local_score,\n",
    "        \"lb_score\": lb_score,\n",
    "        \"notes\": notes,\n",
    "    }\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
