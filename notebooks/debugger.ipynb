{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dd353b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\piiop\\Desktop\\Portfolio\\Projects\\RecodAI_LUC\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "\n",
    "# make `src` importable\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.dataloader import (\n",
    "    ForgeryDataset,\n",
    "    detection_collate_fn,\n",
    "    get_val_transform,\n",
    ")\n",
    "from src.models.mask2former_v1 import Mask2FormerForgeryModel\n",
    "from src.utils.config_utils import load_yaml, sanitize_model_kwargs\n",
    "from src.training.train_cv import build_solution_df\n",
    "from src.models.kaggle_metric import score as kaggle_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cadbf0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CFG_PATH = PROJECT_ROOT / \"config\" / \"base.yaml\"\n",
    "WEIGHTS  = PROJECT_ROOT / \"weights\" / \"full_train\" / \"model_full_data_baseline.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46affdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authentic: exists: True\n",
      "forged   : exists: True\n",
      "masks    : exists: True\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "train_authentic = PROJECT_ROOT / \"data\" / \"train_images\" / \"authentic\"   \n",
    "train_forged    = PROJECT_ROOT / \"data\" / \"train_images\" / \"forged\"\n",
    "train_masks     = PROJECT_ROOT / \"data\" / \"train_masks\" \n",
    "\n",
    "print(\"authentic:\", \"exists:\", train_authentic.exists())\n",
    "print(\"forged   :\", \"exists:\", train_forged.exists())\n",
    "print(\"masks    :\", \"exists:\", train_masks.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f7bca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5176\n",
      "First 3 samples:\n",
      "{'image_path': 'C:\\\\Users\\\\piiop\\\\Desktop\\\\Portfolio\\\\Projects\\\\RecodAI_LUC\\\\data\\\\train_images\\\\authentic\\\\10.png', 'mask_path': 'C:\\\\Users\\\\piiop\\\\Desktop\\\\Portfolio\\\\Projects\\\\RecodAI_LUC\\\\data\\\\train_masks\\\\10.npy', 'is_forged': False}\n",
      "{'image_path': 'C:\\\\Users\\\\piiop\\\\Desktop\\\\Portfolio\\\\Projects\\\\RecodAI_LUC\\\\data\\\\train_images\\\\authentic\\\\10015.png', 'mask_path': 'C:\\\\Users\\\\piiop\\\\Desktop\\\\Portfolio\\\\Projects\\\\RecodAI_LUC\\\\data\\\\train_masks\\\\10015.npy', 'is_forged': False}\n",
      "{'image_path': 'C:\\\\Users\\\\piiop\\\\Desktop\\\\Portfolio\\\\Projects\\\\RecodAI_LUC\\\\data\\\\train_images\\\\authentic\\\\10017.png', 'mask_path': 'C:\\\\Users\\\\piiop\\\\Desktop\\\\Portfolio\\\\Projects\\\\RecodAI_LUC\\\\data\\\\train_masks\\\\10017.npy', 'is_forged': False}\n"
     ]
    }
   ],
   "source": [
    "# Build Dataset and inspect\n",
    "ds = ForgeryDataset(\n",
    "    transform=None,\n",
    "    is_train=True,\n",
    ")\n",
    "\n",
    "print(\"Total samples:\", len(ds))\n",
    "\n",
    "# if empty, inspect why\n",
    "if len(ds) == 0:\n",
    "    # Quickly list what files it *would* see\n",
    "    authentic_files = sorted(os.listdir(train_authentic)) if train_authentic.exists() else []\n",
    "    forged_files    = sorted(os.listdir(train_forged))    if train_forged.exists() else []\n",
    "    mask_files      = sorted(os.listdir(train_masks))     if train_masks.exists() else []\n",
    "\n",
    "    print(f\"#authentic files: {len(authentic_files)}\")\n",
    "    print(f\"#forged files   : {len(forged_files)}\")\n",
    "    print(f\"#mask files     : {len(mask_files)}\")\n",
    "\n",
    "    print(\"First few authentic:\", authentic_files[:5])\n",
    "    print(\"First few forged   :\", forged_files[:5])\n",
    "    print(\"First few masks    :\", mask_files[:5])\n",
    "else:\n",
    "    # peek at first few entries\n",
    "    print(\"First 3 samples:\")\n",
    "    for sample in ds.samples[:3]:\n",
    "        print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a0af15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forged samples: 2799\n",
      "forged-with-positive-mask: 2799\n"
     ]
    }
   ],
   "source": [
    "pos = sum(1 for s in ds.samples if s[\"is_forged\"] and os.path.exists(s[\"mask_path\"]) and np.load(s[\"mask_path\"]).sum() > 0)\n",
    "print(\"forged samples:\", sum(s[\"is_forged\"] for s in ds.samples))\n",
    "print(\"forged-with-positive-mask:\", pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d14ef64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary counts ===\n",
      "authentic_with_nonempty_mask_on_disk     : 2377\n",
      "forged_label1_but_zero_instances         : 0\n",
      "forged_with_nonempty_mask_on_disk        : 2799\n",
      "getitem_image_label_0                    : 2377\n",
      "getitem_image_label_1                    : 2799\n",
      "getitem_instances_eq0                    : 2377\n",
      "getitem_instances_gt0                    : 2799\n",
      "is_forged_false                          : 2377\n",
      "is_forged_true                           : 2799\n",
      "mask_load_error                          : 0\n",
      "mask_missing                             : 0\n",
      "mask_ndim_2                              : 0\n",
      "mask_ndim_3                              : 5176\n",
      "mask_ndim_other                          : 0\n",
      "mask_ok                                  : 5176\n",
      "mask_union_sum_gt0                       : 5176\n",
      "shape_mismatch_disk                      : 0\n",
      "total                                    : 5176\n"
     ]
    }
   ],
   "source": [
    "def _safe_load_mask(mask_path: str):\n",
    "    p = Path(mask_path)\n",
    "    if not p.exists():\n",
    "        return None, \"missing\"\n",
    "    try:\n",
    "        m = np.load(p)\n",
    "        return m, \"ok\"\n",
    "    except Exception as e:\n",
    "        return None, f\"load_error: {type(e).__name__}: {e}\"\n",
    "\n",
    "def summarize_dataset_emits(ds, max_examples=5):\n",
    "    samples = ds.samples\n",
    "    n = len(samples)\n",
    "\n",
    "    counts = {\n",
    "        \"total\": n,\n",
    "        \"is_forged_true\": 0,\n",
    "        \"is_forged_false\": 0,\n",
    "        \"mask_missing\": 0,\n",
    "        \"mask_load_error\": 0,\n",
    "        \"mask_ok\": 0,\n",
    "        \"mask_ndim_2\": 0,\n",
    "        \"mask_ndim_3\": 0,\n",
    "        \"mask_ndim_other\": 0,\n",
    "        \"mask_union_sum_gt0\": 0,\n",
    "        \"shape_mismatch_disk\": 0,\n",
    "        \"authentic_with_nonempty_mask_on_disk\": 0,\n",
    "        \"forged_with_nonempty_mask_on_disk\": 0,\n",
    "        # after __getitem__\n",
    "        \"getitem_image_label_1\": 0,\n",
    "        \"getitem_image_label_0\": 0,\n",
    "        \"getitem_instances_gt0\": 0,\n",
    "        \"getitem_instances_eq0\": 0,\n",
    "        \"forged_label1_but_zero_instances\": 0,\n",
    "    }\n",
    "\n",
    "    examples = {k: [] for k in [\n",
    "        \"mask_load_error\",\n",
    "        \"shape_mismatch_disk\",\n",
    "        \"authentic_with_nonempty_mask_on_disk\",\n",
    "        \"forged_with_nonempty_mask_on_disk\",\n",
    "        \"forged_label1_but_zero_instances\",\n",
    "    ]}\n",
    "\n",
    "    for i, s in enumerate(samples):\n",
    "        is_forged = bool(s.get(\"is_forged\", False))\n",
    "        counts[\"is_forged_true\" if is_forged else \"is_forged_false\"] += 1\n",
    "\n",
    "        img_path = s[\"image_path\"]\n",
    "        mask_path = s.get(\"mask_path\", \"\")\n",
    "\n",
    "        # disk-level checks (don’t rely on __getitem__)\n",
    "        m, status = _safe_load_mask(mask_path)\n",
    "        if status == \"missing\":\n",
    "            counts[\"mask_missing\"] += 1\n",
    "        elif status.startswith(\"load_error\"):\n",
    "            counts[\"mask_load_error\"] += 1\n",
    "            if len(examples[\"mask_load_error\"]) < max_examples:\n",
    "                examples[\"mask_load_error\"].append({\"i\": i, \"img\": img_path, \"mask\": mask_path, \"err\": status})\n",
    "        else:\n",
    "            counts[\"mask_ok\"] += 1\n",
    "            if m.ndim == 2:\n",
    "                counts[\"mask_ndim_2\"] += 1\n",
    "                union = (m > 0).astype(np.uint8)\n",
    "            elif m.ndim == 3:\n",
    "                counts[\"mask_ndim_3\"] += 1\n",
    "                # mirror dataloader behavior: accept channel-first (C,H,W) and union over C\n",
    "                union = np.any(m > 0, axis=0).astype(np.uint8)\n",
    "            else:\n",
    "                counts[\"mask_ndim_other\"] += 1\n",
    "                union = None\n",
    "\n",
    "            if union is not None and union.sum() > 0:\n",
    "                counts[\"mask_union_sum_gt0\"] += 1\n",
    "                if is_forged:\n",
    "                    counts[\"forged_with_nonempty_mask_on_disk\"] += 1\n",
    "                    if len(examples[\"forged_with_nonempty_mask_on_disk\"]) < max_examples:\n",
    "                        examples[\"forged_with_nonempty_mask_on_disk\"].append({\"i\": i, \"img\": img_path, \"mask\": mask_path, \"ndim\": int(m.ndim)})\n",
    "                else:\n",
    "                    counts[\"authentic_with_nonempty_mask_on_disk\"] += 1\n",
    "                    if len(examples[\"authentic_with_nonempty_mask_on_disk\"]) < max_examples:\n",
    "                        examples[\"authentic_with_nonempty_mask_on_disk\"].append({\"i\": i, \"img\": img_path, \"mask\": mask_path, \"ndim\": int(m.ndim)})\n",
    "\n",
    "            # shape mismatch vs image (only if union derived)\n",
    "            if union is not None:\n",
    "                with Image.open(img_path) as im:\n",
    "                    w, h = im.size\n",
    "                if union.shape != (h, w):\n",
    "                    counts[\"shape_mismatch_disk\"] += 1\n",
    "                    if len(examples[\"shape_mismatch_disk\"]) < max_examples:\n",
    "                        examples[\"shape_mismatch_disk\"].append({\n",
    "                            \"i\": i, \"img\": img_path, \"mask\": mask_path,\n",
    "                            \"img_hw\": [h, w], \"mask_hw\": list(union.shape),\n",
    "                            \"mask_ndim\": int(m.ndim),\n",
    "                        })\n",
    "\n",
    "        # emitted target checks via __getitem__\n",
    "        try:\n",
    "            img_t, tgt = ds[i]\n",
    "        except Exception as e:\n",
    "            # treat as load_error example\n",
    "            if len(examples[\"mask_load_error\"]) < max_examples:\n",
    "                examples[\"mask_load_error\"].append({\"i\": i, \"img\": img_path, \"mask\": mask_path, \"err\": f\"getitem_error: {type(e).__name__}: {e}\"})\n",
    "            continue\n",
    "\n",
    "        il = float(tgt.get(\"image_label\", torch.tensor(-1.0)).item())\n",
    "        if il == 1.0:\n",
    "            counts[\"getitem_image_label_1\"] += 1\n",
    "        elif il == 0.0:\n",
    "            counts[\"getitem_image_label_0\"] += 1\n",
    "\n",
    "        inst = tgt.get(\"masks\", torch.zeros(0))\n",
    "        k = int(inst.shape[0]) if isinstance(inst, torch.Tensor) and inst.ndim >= 3 else 0\n",
    "        if k > 0:\n",
    "            counts[\"getitem_instances_gt0\"] += 1\n",
    "        else:\n",
    "            counts[\"getitem_instances_eq0\"] += 1\n",
    "\n",
    "        # key contradiction: forged image_label=1 but no instances\n",
    "        if il == 1.0 and k == 0:\n",
    "            counts[\"forged_label1_but_zero_instances\"] += 1\n",
    "            if len(examples[\"forged_label1_but_zero_instances\"]) < max_examples:\n",
    "                examples[\"forged_label1_but_zero_instances\"].append({\n",
    "                    \"i\": i,\n",
    "                    \"img\": img_path,\n",
    "                    \"mask\": mask_path,\n",
    "                    \"boxes_shape\": list(tgt.get(\"boxes\", torch.zeros((0,4))).shape),\n",
    "                    \"masks_shape\": list(inst.shape) if isinstance(inst, torch.Tensor) else None,\n",
    "                })\n",
    "\n",
    "    return counts, examples\n",
    "\n",
    "counts, examples = summarize_dataset_emits(ds, max_examples=5)\n",
    "\n",
    "print(\"=== Summary counts ===\")\n",
    "for k in sorted(counts.keys()):\n",
    "    print(f\"{k:40s} : {counts[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e7a189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 0, 'img_dtype': 'torch.float32', 'img_shape': [3, 512, 648], 'img_min': 0.0, 'img_max': 1.0, 'mask_dtype': 'torch.uint8', 'mask_shape': [0, 512, 648], 'image_label': 0.0, 'num_boxes': 0}\n",
      "{'i': 1, 'img_dtype': 'torch.float32', 'img_shape': [3, 1200, 1600], 'img_min': 0.0, 'img_max': 0.9411764740943909, 'mask_dtype': 'torch.uint8', 'mask_shape': [0, 1200, 1600], 'image_label': 0.0, 'num_boxes': 0}\n",
      "{'i': 2, 'img_dtype': 'torch.float32', 'img_shape': [3, 256, 320], 'img_min': 0.11764705926179886, 'img_max': 0.843137264251709, 'mask_dtype': 'torch.uint8', 'mask_shape': [0, 256, 320], 'image_label': 0.0, 'num_boxes': 0}\n",
      "{'i': 3, 'img_dtype': 'torch.float32', 'img_shape': [3, 666, 1000], 'img_min': 0.0, 'img_max': 1.0, 'mask_dtype': 'torch.uint8', 'mask_shape': [0, 666, 1000], 'image_label': 0.0, 'num_boxes': 0}\n",
      "{'i': 4, 'img_dtype': 'torch.float32', 'img_shape': [3, 712, 414], 'img_min': 0.01568627543747425, 'img_max': 1.0, 'mask_dtype': 'torch.uint8', 'mask_shape': [0, 712, 414], 'image_label': 0.0, 'num_boxes': 0}\n",
      "{'i': 5, 'img_dtype': 'torch.float32', 'img_shape': [3, 158, 1310], 'img_min': 0.26274511218070984, 'img_max': 0.9137254953384399, 'mask_dtype': 'torch.uint8', 'mask_shape': [0, 158, 1310], 'image_label': 0.0, 'num_boxes': 0}\n",
      "{'i': 6, 'img_dtype': 'torch.float32', 'img_shape': [3, 3888, 2592], 'img_min': 0.0, 'img_max': 1.0, 'mask_dtype': 'torch.uint8', 'mask_shape': [0, 3888, 2592], 'image_label': 0.0, 'num_boxes': 0}\n",
      "{'i': 7, 'img_dtype': 'torch.float32', 'img_shape': [3, 846, 759], 'img_min': 0.007843137718737125, 'img_max': 1.0, 'mask_dtype': 'torch.uint8', 'mask_shape': [0, 846, 759], 'image_label': 0.0, 'num_boxes': 0}\n",
      "{'i': 8, 'img_dtype': 'torch.float32', 'img_shape': [3, 1912, 2568], 'img_min': 0.062745101749897, 'img_max': 1.0, 'mask_dtype': 'torch.uint8', 'mask_shape': [0, 1912, 2568], 'image_label': 0.0, 'num_boxes': 0}\n",
      "{'i': 9, 'img_dtype': 'torch.float32', 'img_shape': [3, 152, 1201], 'img_min': 0.2862745225429535, 'img_max': 0.8392156958580017, 'mask_dtype': 'torch.uint8', 'mask_shape': [0, 152, 1201], 'image_label': 0.0, 'num_boxes': 0}\n"
     ]
    }
   ],
   "source": [
    "def spotcheck_tensors(ds, n=10):\n",
    "    out = []\n",
    "    for i in range(min(n, len(ds))):\n",
    "        img_t, tgt = ds[i]\n",
    "        out.append({\n",
    "            \"i\": i,\n",
    "            \"img_dtype\": str(img_t.dtype),\n",
    "            \"img_shape\": list(img_t.shape),\n",
    "            \"img_min\": float(img_t.min().item()),\n",
    "            \"img_max\": float(img_t.max().item()),\n",
    "            \"mask_dtype\": str(tgt[\"masks\"].dtype),\n",
    "            \"mask_shape\": list(tgt[\"masks\"].shape),\n",
    "            \"image_label\": float(tgt[\"image_label\"].item()),\n",
    "            \"num_boxes\": int(tgt[\"boxes\"].shape[0]),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "for row in spotcheck_tensors(ds, n=10):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67395d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy all-authentic score (CV-aligned): 0.45923493044822256\n"
     ]
    }
   ],
   "source": [
    "# Check CV score with dummy submission\n",
    "\n",
    "# Build the exact same ground-truth dataframe CV uses (row_id, annotation, shape)\n",
    "solution_df = build_solution_df(ds)\n",
    "\n",
    "# Dummy submission: predict \"authentic\" for every row_id\n",
    "dummy_submission = pd.DataFrame(\n",
    "    {\"row_id\": solution_df[\"row_id\"].values, \"annotation\": [\"authentic\"] * len(solution_df)}\n",
    ")\n",
    "\n",
    "dummy_score = kaggle_score(\n",
    "    solution_df.copy(),\n",
    "    dummy_submission.copy(),\n",
    "    row_id_column_name=\"row_id\",\n",
    ")\n",
    "\n",
    "print(\"Dummy all-authentic score (CV-aligned):\", float(dummy_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a0409",
   "metadata": {},
   "source": [
    "runs model.forward_logits(images) and records per-image and per-query statistics:\n",
    "\n",
    "img_forged_prob: is the image head always predicting “authentic” or “forged”?\n",
    "\n",
    "cls_max/cls_mean/keep_rate@thr: is the per-query class head collapsed low (so nothing ever survives cls filtering)?\n",
    "\n",
    "mask_max: is the mask head dead (mask probs never rise) or always-hot (spamming FG)?\n",
    "It also attaches the GT image_label from targets so you can compare distributions by class.\n",
    "\n",
    "quick_plots(df_img) + summary prints: visual checks for collapse (single spike distributions, near-zero keep rates, etc.).\n",
    "\n",
    "So overall: it tests (1) you loaded the right weights into the right architecture and (2) whether collapse is happening in the image head, cls filtering, or mask logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b74268f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_yaml(str(CFG_PATH))\n",
    "model_cfg = cfg.get(\"model\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: 432\n",
      "sample keys: ['backbone.body.0.0.weight', 'backbone.body.0.0.bias', 'backbone.body.0.1.weight', 'backbone.body.0.1.bias', 'backbone.body.1.0.layer_scale', 'backbone.body.1.0.block.0.weight', 'backbone.body.1.0.block.0.bias', 'backbone.body.1.0.block.2.weight', 'backbone.body.1.0.block.2.bias', 'backbone.body.1.0.block.3.weight', 'backbone.body.1.0.block.3.bias', 'backbone.body.1.0.block.5.weight', 'backbone.body.1.0.block.5.bias', 'backbone.body.1.1.layer_scale', 'backbone.body.1.1.block.0.weight', 'backbone.body.1.1.block.0.bias', 'backbone.body.1.1.block.2.weight', 'backbone.body.1.1.block.2.bias', 'backbone.body.1.1.block.3.weight', 'backbone.body.1.1.block.3.bias']\n",
      "has pixel_decoder: True\n",
      "has mask_feature_proj: False\n"
     ]
    }
   ],
   "source": [
    "state = torch.load(WEIGHTS, map_location=\"cpu\")\n",
    "\n",
    "# unwrap common checkpoint formats\n",
    "if isinstance(state, dict) and \"state_dict\" in state:\n",
    "    sd = state[\"state_dict\"]\n",
    "elif isinstance(state, dict) and \"model\" in state:\n",
    "    sd = state[\"model\"]\n",
    "else:\n",
    "    sd = state  # assume it's already a state_dict\n",
    "\n",
    "keys = list(sd.keys())\n",
    "print(\"num keys:\", len(keys))\n",
    "print(\"sample keys:\", keys[:20])\n",
    "# Ensuring we're on the correct architecture\n",
    "print(\"has pixel_decoder:\", any(k.startswith(\"pixel_decoder.\") for k in keys))\n",
    "print(\"has mask_feature_proj:\", any(k.startswith(\"mask_feature_proj.\") for k in keys))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_eval_loader(\n",
    "    img_size=256,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "):\n",
    "\n",
    "\n",
    "    ds = ForgeryDataset(\n",
    "        transform=get_val_transform(img_size=img_size),\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,  # shuffle so repeated runs don't stare at same few images\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=detection_collate_fn,  # list[Tensor], list[dict]\n",
    "        persistent_workers=(num_workers > 0),\n",
    "    )\n",
    "    return ds, loader\n",
    "\n",
    "def load_model(weights_path, device):\n",
    "    mk = sanitize_model_kwargs(model_cfg)\n",
    "\n",
    "    # Disable gate for analysis unless you're explicitly studying gating\n",
    "    model = Mask2FormerForgeryModel(\n",
    "        **mk\n",
    "    ).to(device)\n",
    "\n",
    "    state = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_logit_stats(model, loader, device, num_batches=50, cls_thrs=(0.1, 0.2, 0.3)):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      df_img: per-image rows (best for plotting / debugging collapse)\n",
    "      df_query: optional per-query rows (heavier; use only if needed)\n",
    "    \"\"\"\n",
    "    rows_img = []\n",
    "    rows_query = []\n",
    "\n",
    "    it = iter(loader)\n",
    "    for b in range(num_batches):\n",
    "        images, targets = next(it)\n",
    "        images = [x.to(device, non_blocking=True) for x in images]\n",
    "\n",
    "        mask_logits, class_logits, img_logits = model.forward_logits(images)\n",
    "\n",
    "        cls_probs = class_logits.sigmoid()               # [B,Q]\n",
    "        img_probs = img_logits.sigmoid()                 # [B]\n",
    "        mask_probs = mask_logits.sigmoid().flatten(2)    # [B,Q,HW]\n",
    "\n",
    "        # image-level summaries (mirrors train_full debug intent)\n",
    "        cls_max = cls_probs.max(dim=1).values            # [B]\n",
    "        mask_max = mask_probs.max(dim=2).values.max(dim=1).values  # [B]\n",
    "        cls_mean = cls_probs.mean(dim=1)                 # [B]\n",
    "        cls_std  = cls_probs.std(dim=1)                  # [B]\n",
    "\n",
    "        # target info (authentic vs forged)\n",
    "        y = torch.stack([t[\"image_label\"].float() for t in targets]).cpu().numpy()\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            r = {\n",
    "                \"batch\": b,\n",
    "                \"i\": i,\n",
    "                \"image_label\": float(y[i]),                 # 0 authentic / 1 forged\n",
    "                \"img_forged_prob\": float(img_probs[i].item()),\n",
    "                \"cls_max\": float(cls_max[i].item()),\n",
    "                \"cls_mean\": float(cls_mean[i].item()),\n",
    "                \"cls_std\": float(cls_std[i].item()),\n",
    "                \"mask_max\": float(mask_max[i].item()),\n",
    "            }\n",
    "            for thr in cls_thrs:\n",
    "                r[f\"keep_rate@{thr}\"] = float((cls_probs[i] > thr).float().mean().item())\n",
    "                r[f\"num_keep@{thr}\"] = int((cls_probs[i] > thr).sum().item())\n",
    "            rows_img.append(r)\n",
    "\n",
    "        # per-query (optional but useful if collapse is “all queries identical”)\n",
    "        # comment this block out if you want it lighter\n",
    "        B, Q = cls_probs.shape\n",
    "        for bi in range(B):\n",
    "            for q in range(Q):\n",
    "                rows_query.append({\n",
    "                    \"batch\": b,\n",
    "                    \"i\": bi,\n",
    "                    \"q\": q,\n",
    "                    \"image_label\": float(y[bi]),\n",
    "                    \"img_forged_prob\": float(img_probs[bi].item()),\n",
    "                    \"cls_prob\": float(cls_probs[bi, q].item()),\n",
    "                    \"mask_prob_mean\": float(mask_probs[bi, q].mean().item()),\n",
    "                    \"mask_prob_max\": float(mask_probs[bi, q].max().item()),\n",
    "                })\n",
    "\n",
    "    df_img = pd.DataFrame(rows_img)\n",
    "    df_query = pd.DataFrame(rows_query)\n",
    "    return df_img, df_query\n",
    "\n",
    "\n",
    "def quick_plots(df_img):\n",
    "    # 1) forged-prob distribution split by label\n",
    "    for lbl in [0.0, 1.0]:\n",
    "        sub = df_img[df_img[\"image_label\"] == lbl]\n",
    "        plt.figure()\n",
    "        plt.hist(sub[\"img_forged_prob\"].values, bins=40)\n",
    "        plt.title(f\"img_forged_prob (label={int(lbl)})\")\n",
    "        plt.xlabel(\"prob(forged)\")\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.show()\n",
    "\n",
    "    # 2) cls_max and mask_max (collapse detectors)\n",
    "    for col in [\"cls_max\", \"mask_max\"]:\n",
    "        plt.figure()\n",
    "        plt.hist(df_img[col].values, bins=40)\n",
    "        plt.title(col)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.show()\n",
    "\n",
    "    # 3) keep-rate sanity\n",
    "    keep_cols = [c for c in df_img.columns if c.startswith(\"keep_rate@\")]\n",
    "    for col in keep_cols:\n",
    "        plt.figure()\n",
    "        plt.hist(df_img[col].values, bins=40)\n",
    "        plt.title(col)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Run it\n",
    "# ---------------------------\n",
    "\n",
    "ds, loader = build_full_eval_loader(img_size=256, batch_size=4, num_workers=4)\n",
    "model = load_model(WEIGHTS, DEVICE)\n",
    "\n",
    "df_img, df_query = collect_logit_stats(model, loader, DEVICE, num_batches=50)\n",
    "display(df_img.head())\n",
    "display(df_img.describe(percentiles=[0.5, 0.9, 0.95, 0.99]))\n",
    "\n",
    "quick_plots(df_img)\n",
    "\n",
    "# Useful “collapse checks” at a glance:\n",
    "summary = {\n",
    "    \"img_forged_prob_mean\": df_img[\"img_forged_prob\"].mean(),\n",
    "    \"img_forged_prob_p95\": df_img[\"img_forged_prob\"].quantile(0.95),\n",
    "    \"cls_max_mean\": df_img[\"cls_max\"].mean(),\n",
    "    \"cls_max_p95\": df_img[\"cls_max\"].quantile(0.95),\n",
    "    \"mask_max_mean\": df_img[\"mask_max\"].mean(),\n",
    "    \"mask_max_p95\": df_img[\"mask_max\"].quantile(0.95),\n",
    "}\n",
    "print(pd.Series(summary).sort_index())\n",
    "\n",
    "# If you suspect \"all queries are the same\", check per-image variance across queries:\n",
    "per_image_cls_std = df_query.groupby([\"batch\",\"i\"])[\"cls_prob\"].std()\n",
    "print(\"per-image cls_prob std: mean=\", per_image_cls_std.mean(), \" p05=\", per_image_cls_std.quantile(0.05))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
