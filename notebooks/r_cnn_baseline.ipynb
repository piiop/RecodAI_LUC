{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import sympy\n",
    "import sympy.printing  # ensure submodule is imported\n",
    "sympy.printing = sympy.printing  # attach as attribute explicitly\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F_transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Checking GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5da36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_structure():\n",
    "    base_path = '/kaggle/input/recodai-luc-scientific-image-forgery-detection'\n",
    "    \n",
    "    # Checking train images\n",
    "    train_authentic_path = os.path.join(base_path, 'train_images/authentic')\n",
    "    train_forged_path = os.path.join(base_path, 'train_images/forged')\n",
    "    train_masks_path = os.path.join(base_path, 'train_masks')\n",
    "    test_images_path = os.path.join(base_path, 'test_images')\n",
    "    \n",
    "    print(f\"Authentic images: {len(os.listdir(train_authentic_path))}\")\n",
    "    print(f\"Forged images: {len(os.listdir(train_forged_path))}\")\n",
    "    print(f\"Masks: {len(os.listdir(train_masks_path))}\")\n",
    "    print(f\"Test images: {len(os.listdir(test_images_path))}\")\n",
    "    \n",
    "    # Let's analyze some examples of masks\n",
    "    mask_files = os.listdir(train_masks_path)[:5]\n",
    "    print(f\"Examples of mask files: {mask_files}\")\n",
    "    \n",
    "    # Checking the mask format\n",
    "    sample_mask = np.load(os.path.join(train_masks_path, mask_files[0]))\n",
    "    print(f\"Mask format: {sample_mask.shape}, dtype: {sample_mask.dtype}\")\n",
    "    \n",
    "    test_files = os.listdir(test_images_path)\n",
    "    print(f\"Test images: {test_files}\")\n",
    "    \n",
    "    return {\n",
    "        'train_authentic': train_authentic_path,\n",
    "        'train_forged': train_forged_path,\n",
    "        'train_masks': train_masks_path,\n",
    "        'test_images': test_images_path\n",
    "    }\n",
    "\n",
    "paths = analyze_data_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6aa562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForgeryDataset(Dataset):\n",
    "    def __init__(self, authentic_path, forged_path, masks_path, transform=None, is_train=True):\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Collect all data samples\n",
    "        self.samples = []\n",
    "        \n",
    "        # Authentic images\n",
    "        for file in os.listdir(authentic_path):\n",
    "            img_path = os.path.join(authentic_path, file)\n",
    "            base_name = file.split('.')[0]\n",
    "            mask_path = os.path.join(masks_path, f\"{base_name}.npy\")\n",
    "            \n",
    "            self.samples.append({\n",
    "                'image_path': img_path,\n",
    "                'mask_path': mask_path,\n",
    "                'is_forged': False,\n",
    "                'image_id': base_name\n",
    "            })\n",
    "        \n",
    "        # Forged images\n",
    "        for file in os.listdir(forged_path):\n",
    "            img_path = os.path.join(forged_path, file)\n",
    "            base_name = file.split('.')[0]\n",
    "            mask_path = os.path.join(masks_path, f\"{base_name}.npy\")\n",
    "            \n",
    "            self.samples.append({\n",
    "                'image_path': img_path,\n",
    "                'mask_path': mask_path,\n",
    "                'is_forged': True,\n",
    "                'image_id': base_name\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(sample['image_path']).convert('RGB')\n",
    "        image = np.array(image)  # (H, W, 3)\n",
    "        \n",
    "        # Load and process mask\n",
    "        if os.path.exists(sample['mask_path']):\n",
    "            mask = np.load(sample['mask_path'])\n",
    "            \n",
    "            # Handle multi-channel masks\n",
    "            if mask.ndim == 3:\n",
    "                if mask.shape[0] <= 10:  # channels first (C, H, W)\n",
    "                    mask = np.any(mask, axis=0)\n",
    "                elif mask.shape[-1] <= 10:  # channels last (H, W, C)\n",
    "                    mask = np.any(mask, axis=-1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Ambiguous 3D mask shape: {mask.shape}\")\n",
    "            \n",
    "            mask = (mask > 0).astype(np.uint8)\n",
    "        else:\n",
    "            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "    \n",
    "        # Shape validation\n",
    "        assert image.shape[:2] == mask.shape, f\"Shape mismatch: img {image.shape}, mask {mask.shape}\"\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        else:\n",
    "            image = F_transforms.to_tensor(image)\n",
    "            mask = torch.tensor(mask, dtype=torch.uint8)\n",
    "        \n",
    "        # Prepare targets for Mask R-CNN\n",
    "        if sample['is_forged'] and mask.sum() > 0:\n",
    "            boxes, labels, masks = self.mask_to_boxes(mask)\n",
    "            \n",
    "            target = {\n",
    "                'boxes': boxes,\n",
    "                'labels': labels,\n",
    "                'masks': masks,\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "                'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "            }\n",
    "        else:\n",
    "            # For authentic images or images without masks\n",
    "            target = {\n",
    "                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'masks': torch.zeros((0, image.shape[1], image.shape[2]), dtype=torch.uint8),\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': torch.zeros(0, dtype=torch.float32),\n",
    "                'iscrowd': torch.zeros((0,), dtype=torch.int64)\n",
    "            }\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def mask_to_boxes(self, mask):\n",
    "        \"\"\"Convert segmentation mask to bounding boxes for Mask R-CNN\"\"\"\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask_np = mask.numpy()\n",
    "        else:\n",
    "            mask_np = mask\n",
    "        \n",
    "        # Find contours in the mask\n",
    "        contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        boxes = []\n",
    "        masks = []\n",
    "        \n",
    "        for contour in contours:\n",
    "            if len(contour) > 0:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                # Filter out very small regions\n",
    "                if w > 5 and h > 5:\n",
    "                    boxes.append([x, y, x + w, y + h])\n",
    "                    # Create binary mask for this contour\n",
    "                    contour_mask = np.zeros_like(mask_np)\n",
    "                    cv2.fillPoly(contour_mask, [contour], 1)\n",
    "                    masks.append(contour_mask)\n",
    "        \n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "            masks = torch.tensor(np.array(masks), dtype=torch.uint8)\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "            masks = torch.zeros((0, mask_np.shape[0], mask_np.shape[1]), dtype=torch.uint8)\n",
    "        \n",
    "        return boxes, labels, masks\n",
    "\n",
    "# Transformations for learning\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ForgeryDataset(\n",
    "    paths['train_authentic'], \n",
    "    paths['train_forged'], \n",
    "    paths['train_masks'],\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Changing transformations for the val dataset\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "# Creating dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ee83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_light_mask_rcnn(num_classes=2):\n",
    "    backbone = torchvision.models.mobilenet_v3_small(pretrained=False).features\n",
    "    backbone.out_channels = 576\n",
    "    \n",
    "    # extracts characteristics from an image\n",
    "    backbone = nn.Sequential(\n",
    "        backbone,\n",
    "        nn.Conv2d(576, 256, kernel_size=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    backbone.out_channels = 256\n",
    "    \n",
    "    # Anchor generator\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((16, 32, 64, 128),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "    \n",
    "    # ROI pools\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=5,\n",
    "        sampling_ratio=1\n",
    "    )\n",
    "    \n",
    "    mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=10,\n",
    "        sampling_ratio=1\n",
    "    )\n",
    "    \n",
    "    model = MaskRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        mask_roi_pool=mask_roi_pooler,\n",
    "        min_size=224,\n",
    "        max_size=224,\n",
    "        rpn_pre_nms_top_n_train=1000,\n",
    "        rpn_pre_nms_top_n_test=1000,\n",
    "        rpn_post_nms_top_n_train=200,\n",
    "        rpn_post_nms_top_n_test=200,\n",
    "        box_detections_per_img=100\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_light_mask_rcnn()\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5492b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, device):\n",
    "    model.train()  # For validation, we use train mode because of the features of Mask R-CNN\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(tqdm(dataloader, desc=\"Validation\")):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += losses.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133260a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch_samples(dataloader, model=None, device=device):\n",
    "    images, targets = next(iter(dataloader))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    for i in range(min(4, len(images))):\n",
    "        # Original image\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # denormalize\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].set_title(f'Image {i}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Mask\n",
    "        mask = torch.zeros_like(images[i][0])\n",
    "        for target_mask in targets[i]['masks']:\n",
    "            mask = torch.max(mask, target_mask.cpu())\n",
    "        \n",
    "        axes[1, i].imshow(mask, cmap='hot')\n",
    "        axes[1, i].set_title(f'Mask {i}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_batch_samples(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287dd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 3\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = validate_epoch(model, val_loader, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # We save the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        torch.save(model.state_dict(), f'mask_rcnn_epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d016b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    \"\"\"\n",
    "    Encode binary mask to RLE in the format required by the competition.\n",
    "    Returns a JSON string like \"[123,4,567,8]\"\n",
    "    \"\"\"\n",
    "    # Ensure mask is 2D and binary\n",
    "    mask = mask.astype(bool)\n",
    "    \n",
    "    # Flatten in Fortran order\n",
    "    flat = mask.T.flatten()\n",
    "    \n",
    "    # Find indices where value is True\n",
    "    dots = np.where(flat)[0]\n",
    "    \n",
    "    if len(dots) == 0:\n",
    "        return json.dumps([])  # or just return 'authentic' upstream\n",
    "    \n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend([b + 1, 0])  # 1-based index\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    \n",
    "    # Convert numpy ints to Python ints for JSON compatibility\n",
    "    run_lengths = [int(x) for x in run_lengths]\n",
    "    return json.dumps(run_lengths)\n",
    "\n",
    "def predict_test_images(model, test_path, device):\n",
    "    model.eval()\n",
    "    predictions = {}\n",
    "    \n",
    "    test_files = sorted(os.listdir(test_path))\n",
    "    \n",
    "    transform = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    for file in tqdm(test_files, desc=\"Processing test images\"):\n",
    "        case_id = file.split('.')[0]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img_path = os.path.join(test_path, file)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "        \n",
    "        original_size = image_np.shape[:2]\n",
    "        \n",
    "        # Apply transformations\n",
    "        transformed = transform(image=image_np)\n",
    "        image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Model prediction\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image_tensor)\n",
    "        \n",
    "        # Process predictions\n",
    "        masks = prediction[0]['masks']\n",
    "        scores = prediction[0]['scores']\n",
    "        \n",
    "        # Filter by confidence threshold\n",
    "        confidence_threshold = 0.4     # CHANGE THIS TO SEE RESULTS(changes)\n",
    "        valid_detections = scores > confidence_threshold\n",
    "        \n",
    "        if valid_detections.sum() == 0:\n",
    "            # No detections -> authentic image\n",
    "            predictions[case_id] = \"authentic\"\n",
    "        else:\n",
    "            # Combine all detected masks\n",
    "            combined_mask = torch.zeros((256, 256), device=device)\n",
    "            for i in range(len(masks)):\n",
    "                if valid_detections[i]:\n",
    "                    mask = masks[i, 0] > 0.5  # Binarize mask\n",
    "                    combined_mask = torch.logical_or(combined_mask, mask)\n",
    "            \n",
    "            # Convert to numpy and resize to original dimensions\n",
    "            combined_mask_np = combined_mask.cpu().numpy().astype(np.uint8)\n",
    "            combined_mask_resized = cv2.resize(combined_mask_np, \n",
    "                                             (original_size[1], original_size[0]),\n",
    "                                             interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # RLE encoding\n",
    "            if combined_mask_resized.sum() == 0:\n",
    "                predictions[case_id] = \"authentic\"\n",
    "            else:\n",
    "                rle_json = rle_encode_competition(combined_mask_resized)\n",
    "                predictions[case_id] = rle_json\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "predictions = predict_test_images(model, paths['test_images'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15887ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the sample submission for the correct order\n",
    "sample_submission = pd.read_csv('/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv')\n",
    "    \n",
    "# Create DataFrame with predictions\n",
    "submission_data = []\n",
    "for case_id in sample_submission['case_id']:\n",
    "    case_id_str = str(case_id)\n",
    "    if case_id_str in predictions:\n",
    "        submission_data.append({'case_id': case_id, 'annotation': predictions[case_id_str]})\n",
    "    else:\n",
    "        # If case_id not in predictions, use authentic as default\n",
    "        submission_data.append({'case_id': case_id, 'annotation': 'authentic'})\n",
    "    \n",
    "submission = pd.DataFrame(submission_data)\n",
    "    \n",
    "# Save submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "# Prediction statistics\n",
    "authentic_count = (submission['annotation'] == 'authentic').sum()\n",
    "forged_count = len(submission) - authentic_count\n",
    "\n",
    "print(f\"Prediction Statistics:\")\n",
    "print(f\"Authentic: {authentic_count}\")\n",
    "print(f\"Forged: {forged_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# We take the first file from the test folder\n",
    "test_files = sorted(os.listdir(paths['test_images']))\n",
    "file = test_files[0]\n",
    "img_path = os.path.join(paths['test_images'], file)\n",
    "\n",
    "# Uploading an image\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image_np = np.array(image)\n",
    "\n",
    "# Transformations\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Apply transformations and make predictions\n",
    "transformed = transform(image=image_np)\n",
    "image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(image_tensor)\n",
    "\n",
    "masks = prediction[0]['masks']\n",
    "scores = prediction[0]['scores']\n",
    "confidence_threshold = 0.5\n",
    "valid_detections = scores > confidence_threshold\n",
    "\n",
    "# Creating a shape: original on the left, mask on the right\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(image_np)\n",
    "axes[0].set_title(f'Original: {file}', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Mask\n",
    "if valid_detections.sum() == 0:\n",
    "    combined_mask = np.zeros((256, 256))\n",
    "    title = 'Predicted: Authentic'\n",
    "    cmap = 'gray'\n",
    "else:\n",
    "    combined_mask = np.zeros((256, 256), dtype=np.float32)\n",
    "    for idx in range(len(masks)):\n",
    "        if valid_detections[idx]:\n",
    "            mask = masks[idx, 0] > 0.5\n",
    "            combined_mask = np.maximum(combined_mask, mask.cpu().numpy().astype(np.float32))\n",
    "    \n",
    "    title = f'Predicted: Forged ({valid_detections.sum()} regions)'\n",
    "    cmap = 'hot'\n",
    "\n",
    "im = axes[1].imshow(combined_mask, cmap=cmap, vmin=0, vmax=1)\n",
    "axes[1].set_title(title, fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "if valid_detections.sum() > 0:\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46393544",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/kaggle/working/submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d720dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final_mask_rcnn_model.pth')\n",
    "print(\"The final model is saved as: 'final_mask_rcnn_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
