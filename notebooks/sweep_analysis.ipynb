{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bdfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_analysis.ipynb\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from ipywidgets import Dropdown, HBox, IntSlider, VBox, interact\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.dataloader import (\n",
    "    ForgeryDataset,\n",
    "    detection_collate_fn,\n",
    "    get_val_transform,\n",
    ")\n",
    "from src.models.mask2former_v1 import Mask2FormerForgeryModel\n",
    "from src.utils.config_utils import load_yaml, sanitize_model_kwargs\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Constants / Paths\n",
    "# -----------------\n",
    "\n",
    "OOF_ROOT = PROJECT_ROOT / \"experiments\" / \"oof_results\"\n",
    "FULL_TRAIN_ROOT = PROJECT_ROOT / \"experiments\" / \"full_train_results\"\n",
    "\n",
    "CLS_THRESHOLD_PATH = (\n",
    "    PROJECT_ROOT / \"experiments\" / \"cls_threshold_sweep\" / \"cls_threshold_sweep.csv\"\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CFG_PATH = PROJECT_ROOT / \"config\" / \"base.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdd80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Load cls_threshold\n",
    "# -----------------\n",
    "cls_threshold = pd.read_csv(CLS_THRESHOLD_PATH)\n",
    "\n",
    "# Quick sanity check\n",
    "display(cls_threshold.head())\n",
    "print(f\"Loaded {len(cls_threshold)} rows\")\n",
    "print(cls_threshold.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_threshold['cls_threshold'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate per cls_threshold across all images\n",
    "summary = (\n",
    "    cls_threshold.groupby(\"cls_threshold\")\n",
    "      .agg(\n",
    "          gate_pass_rate=(\"gate_pass\", \"mean\"),\n",
    "          avg_num_keep=(\"num_keep\", \"mean\"),\n",
    "          any_fg_pre_rate=(\"any_fg_pre_keep\", \"mean\"),\n",
    "          any_fg_post_rate=(\"any_fg_post_keep\", \"mean\"),\n",
    "          avg_max_cls_prob=(\"max_cls_prob\", \"mean\"),\n",
    "          avg_max_mask_prob=(\"max_mask_prob\", \"mean\"),\n",
    "          avg_image_forged_prob=(\"image_forged_prob\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"cls_threshold\")\n",
    ")\n",
    "\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0cb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f1c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171186f6",
   "metadata": {},
   "source": [
    "cls weight sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ff587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/analyze_cls_weight_sweep.ipynb (run from repo root)\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logs are written by ClsCollapseLogger to:\n",
    "# experiments/cls_collapse/<run_name>/{meta.json, optimizer.json, step_losses.csv, epoch_summary.csv, debug.jsonl}\n",
    "# :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "BASE = Path(\"../experiments/cls_collapse\")\n",
    "\n",
    "def _safe_read_json(p: Path):\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    return json.loads(p.read_text())\n",
    "\n",
    "def _safe_read_csv(p: Path):\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "def _safe_read_jsonl(p: Path):\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    rows = []\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return pd.DataFrame(rows) if rows else pd.DataFrame()\n",
    "\n",
    "def load_run(run_dir: Path):\n",
    "    meta = _safe_read_json(run_dir / \"meta.json\") or {}\n",
    "    opt  = _safe_read_json(run_dir / \"optimizer.json\") or {}\n",
    "    steps = _safe_read_csv(run_dir / \"step_losses.csv\")\n",
    "    epochs = _safe_read_csv(run_dir / \"epoch_summary.csv\")\n",
    "    dbg = _safe_read_jsonl(run_dir / \"debug.jsonl\")\n",
    "\n",
    "    # Prefer weight from logged CSV snapshot (most robust)\n",
    "    w = None\n",
    "    if steps is not None and \"w_mask_cls\" in steps.columns and len(steps):\n",
    "        w = float(pd.to_numeric(steps[\"w_mask_cls\"], errors=\"coerce\").dropna().iloc[-1])\n",
    "    elif epochs is not None and \"w_mask_cls\" in epochs.columns and len(epochs):\n",
    "        w = float(pd.to_numeric(epochs[\"w_mask_cls\"], errors=\"coerce\").dropna().iloc[-1])\n",
    "\n",
    "    out = {\n",
    "        \"run_dir\": run_dir,\n",
    "        \"run_name\": run_dir.name,\n",
    "        \"w_mask_cls\": w,\n",
    "        \"meta\": meta,\n",
    "        \"optimizer\": opt,\n",
    "        \"step_losses\": steps,\n",
    "        \"epoch_summary\": epochs,\n",
    "        \"debug\": dbg,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def load_sweep_runs(base=BASE, name_prefix=\"full_full_cls_w\"):\n",
    "    run_dirs = sorted([p for p in base.glob(f\"{name_prefix}*\") if p.is_dir()])\n",
    "    runs = [load_run(rd) for rd in run_dirs]\n",
    "    # Keep only runs that look like our sweep (have at least epoch_summary or step_losses)\n",
    "    runs = [r for r in runs if (r[\"epoch_summary\"] is not None or r[\"step_losses\"] is not None)]\n",
    "    return runs\n",
    "\n",
    "runs = load_sweep_runs()\n",
    "print(\"Found runs:\", len(runs))\n",
    "for r in runs:\n",
    "    print(r[\"run_name\"], \"w_mask_cls=\", r[\"w_mask_cls\"])\n",
    "\n",
    "# ----------------------------\n",
    "# Build a compact sweep table\n",
    "# ----------------------------\n",
    "rows = []\n",
    "for r in runs:\n",
    "    epochs = r[\"epoch_summary\"]\n",
    "    steps = r[\"step_losses\"]\n",
    "\n",
    "    last_epoch = None\n",
    "    if epochs is not None and len(epochs):\n",
    "        last_epoch = epochs.sort_values(\"epoch\").iloc[-1].to_dict()\n",
    "\n",
    "    last_step = None\n",
    "    if steps is not None and len(steps):\n",
    "        last_step = steps.sort_values(\"global_step\").iloc[-1].to_dict()\n",
    "\n",
    "    rows.append({\n",
    "        \"run_name\": r[\"run_name\"],\n",
    "        \"run_dir\": str(r[\"run_dir\"]),\n",
    "        \"w_mask_cls\": r[\"w_mask_cls\"],\n",
    "        \"epochs_logged\": int(epochs[\"epoch\"].max()) if epochs is not None and len(epochs) else np.nan,\n",
    "        \"steps_logged\": int(steps[\"global_step\"].max()) + 1 if steps is not None and len(steps) else np.nan,\n",
    "\n",
    "        # epoch-end collapse detectors\n",
    "        \"cls_max_mean_last\": (last_epoch or {}).get(\"cls_max_mean\", np.nan),\n",
    "        \"cls_max_p95_last\":  (last_epoch or {}).get(\"cls_max_p95\", np.nan),\n",
    "        \"mask_max_mean_last\":(last_epoch or {}).get(\"mask_max_mean\", np.nan),\n",
    "        \"img_forged_mean_last\": (last_epoch or {}).get(\"img_forged_mean\", np.nan),\n",
    "\n",
    "        # loss snapshot\n",
    "        \"loss_total_last\": (last_step or {}).get(\"loss_total\", np.nan),\n",
    "        \"loss_mask_cls_last\": (last_step or {}).get(\"loss_mask_cls\", np.nan),\n",
    "        \"loss_auth_penalty_last\": (last_step or {}).get(\"loss_auth_penalty\", np.nan),\n",
    "    })\n",
    "\n",
    "sweep_df = pd.DataFrame(rows).sort_values(\"w_mask_cls\", na_position=\"last\")\n",
    "display(sweep_df)\n",
    "\n",
    "# ----------------------------\n",
    "# Plots: collapse vs w\n",
    "# ----------------------------\n",
    "def plot_vs_w(df, x=\"w_mask_cls\", ys=()):\n",
    "    dfp = df.dropna(subset=[x]).sort_values(x)\n",
    "    for y in ys:\n",
    "        plt.figure()\n",
    "        plt.plot(dfp[x].values, dfp[y].values, marker=\"o\")\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(y)\n",
    "        plt.title(f\"{y} vs {x}\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "plot_vs_w(\n",
    "    sweep_df,\n",
    "    ys=[\"cls_max_mean_last\", \"cls_max_p95_last\", \"mask_max_mean_last\", \"img_forged_mean_last\", \"loss_total_last\"]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Per-run learning curves\n",
    "# ----------------------------\n",
    "def plot_run_curves(runs, metric=\"loss_total\"):\n",
    "    for r in runs:\n",
    "        steps = r[\"step_losses\"]\n",
    "        if steps is None or metric not in steps.columns:\n",
    "            continue\n",
    "        steps = steps.sort_values(\"global_step\")\n",
    "        plt.figure()\n",
    "        plt.plot(steps[\"global_step\"], steps[metric])\n",
    "        plt.xlabel(\"global_step\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f\"{r['run_name']} (w={r['w_mask_cls']})\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "plot_run_curves(runs, metric=\"loss_total\")\n",
    "plot_run_curves(runs, metric=\"loss_mask_cls\")\n",
    "plot_run_curves(runs, metric=\"loss_auth_penalty\")\n",
    "\n",
    "# ----------------------------\n",
    "# Debug JSONL: auth penalty + cls target density over time\n",
    "# ----------------------------\n",
    "def extract_debug_timeseries(runs):\n",
    "    all_rows = []\n",
    "    for r in runs:\n",
    "        dbg = r[\"debug\"]\n",
    "        if dbg is None or dbg.empty:\n",
    "            continue\n",
    "        # keep only structured events we care about\n",
    "        keep = dbg[dbg[\"tag\"].isin([\"loss_auth_penalty_stats\", \"loss_cls_targets\"])].copy()\n",
    "        if keep.empty:\n",
    "            continue\n",
    "        keep[\"run_name\"] = r[\"run_name\"]\n",
    "        keep[\"w_mask_cls\"] = r[\"w_mask_cls\"]\n",
    "        # normalize common x-axis\n",
    "        if \"global_step\" not in keep.columns:\n",
    "            keep[\"global_step\"] = np.nan\n",
    "        all_rows.append(keep)\n",
    "    return pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame()\n",
    "\n",
    "dbg_df = extract_debug_timeseries(runs)\n",
    "display(dbg_df.head() if not dbg_df.empty else dbg_df)\n",
    "\n",
    "def plot_debug_series(dbg_df, tag, y, title=None):\n",
    "    if dbg_df.empty:\n",
    "        print(\"No debug events found.\")\n",
    "        return\n",
    "    sub = dbg_df[dbg_df[\"tag\"] == tag].copy()\n",
    "    if sub.empty or y not in sub.columns:\n",
    "        print(f\"No rows for tag={tag} with field {y}\")\n",
    "        return\n",
    "    sub = sub.dropna(subset=[\"w_mask_cls\"]).sort_values([\"w_mask_cls\", \"global_step\"])\n",
    "\n",
    "    for w, g in sub.groupby(\"w_mask_cls\"):\n",
    "        g = g.sort_values(\"global_step\")\n",
    "        plt.figure()\n",
    "        plt.plot(g[\"global_step\"].values, g[y].values)\n",
    "        plt.xlabel(\"global_step\")\n",
    "        plt.ylabel(y)\n",
    "        plt.title(title or f\"{tag}:{y} (w={w})\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "plot_debug_series(dbg_df, tag=\"loss_cls_targets\", y=\"pos_frac\", title=\"Matched-query positive fraction over time\")\n",
    "plot_debug_series(dbg_df, tag=\"loss_auth_penalty_stats\", y=\"per_image_penalty_mean\", title=\"Per-image auth penalty mean over time\")\n",
    "plot_debug_series(dbg_df, tag=\"loss_auth_penalty_stats\", y=\"loss_auth_penalty\", title=\"Auth penalty loss over time\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Quick \"collapse flags\" summary\n",
    "# ----------------------------\n",
    "flags = sweep_df.copy()\n",
    "flags[\"cls_collapsed@~0.125\"] = np.isfinite(flags[\"cls_max_mean_last\"]) & (np.abs(flags[\"cls_max_mean_last\"] - 0.125) < 1e-3)\n",
    "flags[\"mask_saturated@~1.0\"] = np.isfinite(flags[\"mask_max_mean_last\"]) & (flags[\"mask_max_mean_last\"] > 0.99)\n",
    "display(flags[[\"run_name\",\"w_mask_cls\",\"cls_max_mean_last\",\"mask_max_mean_last\",\"img_forged_mean_last\",\"cls_collapsed@~0.125\",\"mask_saturated@~1.0\"]]\n",
    "        .sort_values(\"w_mask_cls\", na_position=\"last\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
